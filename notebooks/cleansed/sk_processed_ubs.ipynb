{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/cleansed/sk_processed_ubs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2C9SSNHREALD",
        "outputId": "3cfd0394-c394-428d-9790-74b5dd8e9e2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.0\\n\\nDescription:\\n    This notebook is dedicated to the data engineering functions for the Bank of England Employer Project.\\n    It includes code for mounting Google Drive, reading raw PDF files (e.g., earnings call transcripts),\\n    and performing text extraction and cleaning using pdfplumber and regular expressions.\\n\\n===================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.0\n",
        "\n",
        "Description:\n",
        "    This notebook is dedicated to the data engineering functions for the Bank of England Employer Project.\n",
        "    It includes code for mounting Google Drive, reading raw PDF files (e.g., earnings call transcripts),\n",
        "    and performing text extraction and cleaning using pdfplumber and regular expressions.\n",
        "\n",
        "===================================================\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XmhbIl8CKIy"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7InvOZLj8TIy"
      },
      "outputs": [],
      "source": [
        "!pip -q install pdfplumber\n",
        "# Make sure spaCy and the English model are installed:\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLiDSPv08QOd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VD8ipZFGbLk",
        "outputId": "787b64eb-8e73-4181-dffa-b85484e691d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['raw', 'processed', 'cleansed', 'data with topic modelling', 'preprocessed_data']\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# 1. Mount Google Drive and define folder paths\n",
        "# -------------------------------\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Assuming 'BOE' folder is in 'MyDrive' and already shared\n",
        "BOE_path = '/content/drive/MyDrive/BOE/bank_of_england/data'\n",
        "\n",
        "# Now you (and others with access) can work with files in this directory\n",
        "# For example, you can list the contents:\n",
        "print(os.listdir(BOE_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgoBPbis7gBV",
        "outputId": "9ea2db1d-6d62-41d5-b0f1-97415baa19e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: 1q23-earnings-call-remarks.pdf\n",
            "Processing file: 1q24-earnings-call-remarks.pdf\n",
            "Processing file: 2q23-earnings-call-remarks.pdf\n",
            "Processing file: 2q24-earnings-call-remarks.pdf\n",
            "Processing file: 3q23-earnings-call-remarks.pdf\n",
            "Processing file: 3q24-earnings-call-remarks.pdf\n",
            "Processing file: 4q23-earnings-call-remarks.pdf\n",
            "Processing file: 4q24-earnings-call-remarks.pdf\n"
          ]
        }
      ],
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Define paths for raw and processed data\n",
        "# -------------------------------\n",
        "BOE_path = \"/content/drive/MyDrive/BOE/bank_of_england/data\"  # adjust as needed\n",
        "raw_dir = os.path.join(BOE_path, \"raw\", \"ubs\")\n",
        "processed_dir = os.path.join(BOE_path, \"cleansed\")\n",
        "os.makedirs(raw_dir, exist_ok=True)\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define helper functions for cleaning and splitting the transcript\n",
        "# -------------------------------\n",
        "def clean_transcript(text):\n",
        "    \"\"\"Cleans the raw transcript text by removing excessive whitespace, page markers, page numbers, and disclaimers.\"\"\"\n",
        "    # Remove long sequences of dots (page breaks)\n",
        "    text = re.sub(r'\\n\\s*\\.{10,}\\s*\\n', '\\n', text)\n",
        "    # Remove isolated numbers on a line\n",
        "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
        "    # Remove phrases like \"On page X\" and \"Starting on page X\"\n",
        "    text = re.sub(r'On page \\d+', '', text)\n",
        "    text = re.sub(r'Starting on page \\d+', '', text)\n",
        "    # Remove page number lines like \"Page 24 of 35\" (case-insensitive, whole-line match)\n",
        "    text = re.sub(r'(?im)^Page\\s+\\d+\\s+of\\s+\\d+\\s*$', '', text)\n",
        "    # Fix punctuation issues\n",
        "    text = re.sub(r'\\.\\s*,', '.', text)\n",
        "    text = text.replace('%. ,', '%.')\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "    # Remove disclaimer text if present\n",
        "    if \"Disclaimer\" in text:\n",
        "        text = text.split(\"Disclaimer\")[0].strip()\n",
        "    return text\n",
        "\n",
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    Extracts the financial quarter and call date from the transcript text.\n",
        "    Expects a header like \"Fourth quarter 2024 results\" and a date like \"4 February 2025\".\n",
        "    Returns a tuple (financial_quarter, call_date) with the quarter standardized (e.g., \"4Q24\").\n",
        "    \"\"\"\n",
        "    quarter_match = re.search(r'(?i)(First|Second|Third|Fourth)\\s+quarter\\s+(\\d{4})', text)\n",
        "    if quarter_match:\n",
        "        quarter_map = {\"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\"}\n",
        "        quarter_num = quarter_map.get(quarter_match.group(1).lower(), \"\")\n",
        "        year_full = quarter_match.group(2)\n",
        "        financial_quarter = f\"{quarter_num}Q{year_full[-2:]}\"\n",
        "    else:\n",
        "        financial_quarter = None\n",
        "\n",
        "    # Capture a date in the form \"4 February 2025\" (assumes day-first)\n",
        "    date_match = re.search(r'^\\s*(\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4})\\s*$', text, re.MULTILINE)\n",
        "    call_date = date_match.group(1) if date_match else None\n",
        "\n",
        "    return financial_quarter, call_date\n",
        "\n",
        "def split_sections(transcript):\n",
        "    \"\"\"\n",
        "    Splits the transcript into Management Announcements and Q&A sections.\n",
        "    Uses \"Analyst Q&A (CEO and CFO)\" as the marker (case-insensitive).\n",
        "    Returns a tuple: (management_announcements, qa_section)\n",
        "    \"\"\"\n",
        "    marker_pattern = r'(?i)Analyst\\s+Q&A\\s*\\(CEO\\s+and\\s+CFO\\)'\n",
        "    marker_match = re.search(marker_pattern, transcript)\n",
        "    if marker_match:\n",
        "        management_announcements = transcript[:marker_match.start()].strip()\n",
        "        qa_section = transcript[marker_match.start():].strip()\n",
        "    else:\n",
        "        management_announcements = transcript\n",
        "        qa_section = \"\"\n",
        "    return management_announcements, qa_section\n",
        "\n",
        "# ----- Management Section Parsing (working code) -----\n",
        "def parse_management_section(management_text, speaker_threshold=4):\n",
        "    \"\"\"\n",
        "    Parses the Management Announcements section into a list of dictionaries.\n",
        "    Each dictionary contains:\n",
        "      - 'speaker'\n",
        "      - 'utterance'\n",
        "\n",
        "    This function skips header lines (e.g., those containing \"results\", \"speeches\", \"transcript\",\n",
        "    \"numbers for slides\", \"available\", \"www.ubs.com\"), any line that contains the word \"slide\",\n",
        "    and lines that match a date.\n",
        "    A line is considered a speaker header if it is short (≤ speaker_threshold words) and in title case.\n",
        "    \"\"\"\n",
        "    header_keywords = [\"results\", \"speeches\", \"transcript\", \"numbers for slides\", \"available\", \"www.ubs.com\"]\n",
        "    entries = []\n",
        "    current_entry = None\n",
        "    lines = management_text.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if not stripped:\n",
        "            continue\n",
        "        # Skip any line that contains the word \"slide\" (case-insensitive)\n",
        "        if \"slide\" in stripped.lower():\n",
        "            continue\n",
        "        if any(kw in stripped.lower() for kw in header_keywords):\n",
        "            continue\n",
        "        if re.match(r'^\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4}$', stripped):\n",
        "            continue\n",
        "        words = stripped.split()\n",
        "        if len(words) <= speaker_threshold and stripped.istitle():\n",
        "            if current_entry is not None:\n",
        "                entries.append(current_entry)\n",
        "            current_entry = {'speaker': stripped, 'utterance': \"\"}\n",
        "        else:\n",
        "            if current_entry is not None:\n",
        "                if current_entry['utterance']:\n",
        "                    current_entry['utterance'] += \" \" + stripped\n",
        "                else:\n",
        "                    current_entry['utterance'] = stripped\n",
        "            else:\n",
        "                current_entry = {'speaker': 'Unknown', 'utterance': stripped}\n",
        "    if current_entry is not None:\n",
        "        entries.append(current_entry)\n",
        "    return entries\n",
        "\n",
        "# ----- Q&A Section Parsing using a Simple Regex Approach (with spaCy check) -----\n",
        "def parse_qa_section_simple(qa_text, header_word_threshold=6):\n",
        "    \"\"\"\n",
        "    Parses the Q&A section using a simple regex approach.\n",
        "\n",
        "    Assumes that speaker headers are lines that either:\n",
        "      - Match the pattern \"Name, Company\" or \"Name:\" (i.e., they contain a comma, colon, or dash),\n",
        "      - Or are short lines (≤ header_word_threshold words) in title case.\n",
        "\n",
        "    IMPORTANT: If a line starts with a conjunction such as \"And\", \"But\", or \"Or\" (optionally followed by a comma),\n",
        "    it is not treated as a new header; instead, it is appended to the previous speaker's utterance.\n",
        "\n",
        "    Additionally, if a candidate header (obtained via regex) does not contain a PERSON entity (via spaCy),\n",
        "    then it is treated as a continuation.\n",
        "\n",
        "    Lines starting with \"Slide\" or empty lines are skipped.\n",
        "\n",
        "    Returns a list of dictionaries with keys: 'speaker', 'job_title', 'utterance'.\n",
        "    \"\"\"\n",
        "    # Remove the marker line if it exists\n",
        "    qa_text = re.sub(r'(?i)^Analyst\\s+Q&A\\s*\\(CEO\\s+and\\s+CFO\\)', '', qa_text).strip()\n",
        "\n",
        "    entries = []\n",
        "    current_entry = None\n",
        "    lines = qa_text.split('\\n')\n",
        "    # A header regex that looks for a name optionally followed by punctuation and a job title\n",
        "    header_regex = re.compile(r'^(?P<speaker>[A-Z][A-Za-z\\s\\.\\-]+)(?:[,:\\-]\\s*(?P<job_title>.+))?$')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line.lower().startswith(\"slide\"):\n",
        "            continue\n",
        "        # Check if the line starts with a conjunction (with optional comma)\n",
        "        if re.match(r'^(And|But|Or)[,]?\\s', line, re.IGNORECASE):\n",
        "            if current_entry is not None:\n",
        "                current_entry['utterance'] += \" \" + line\n",
        "            else:\n",
        "                current_entry = {'speaker': 'Unknown', 'job_title': \"\", 'utterance': line}\n",
        "            continue\n",
        "\n",
        "        m = header_regex.match(line)\n",
        "        if m:\n",
        "            candidate = m.group('speaker').strip()\n",
        "            # Only treat as header if candidate has between 2 and header_word_threshold words and is in title case.\n",
        "            if 2 <= len(candidate.split()) <= header_word_threshold and candidate.istitle():\n",
        "                # Use spaCy to check if candidate contains a PERSON entity.\n",
        "                doc_candidate = nlp(candidate)\n",
        "                first_word = candidate.split()[0].lower().rstrip(\".,\")\n",
        "                greetings = {\"hi\", \"hello\", \"hey\"}\n",
        "                if first_word in greetings:\n",
        "                    # Treat as continuation.\n",
        "                    if current_entry is not None:\n",
        "                        current_entry['utterance'] += \" \" + line\n",
        "                    else:\n",
        "                        current_entry = {'speaker': 'Unknown', 'job_title': \"\", 'utterance': line}\n",
        "                    continue\n",
        "                if not any(ent.label_ == \"PERSON\" for ent in doc_candidate.ents):\n",
        "                    # Not a valid header; treat as continuation.\n",
        "                    if current_entry is not None:\n",
        "                        current_entry['utterance'] += \" \" + line\n",
        "                    else:\n",
        "                        current_entry = {'speaker': 'Unknown', 'job_title': \"\", 'utterance': line}\n",
        "                    continue\n",
        "                # Otherwise, treat it as a header.\n",
        "                if current_entry is not None:\n",
        "                    entries.append(current_entry)\n",
        "                job_title = m.group('job_title').strip() if m.group('job_title') else \"\"\n",
        "                current_entry = {'speaker': candidate, 'job_title': job_title, 'utterance': \"\"}\n",
        "                continue\n",
        "\n",
        "        # If the line did not match the header pattern, append as continuation.\n",
        "        if current_entry is not None:\n",
        "            if current_entry['utterance']:\n",
        "                current_entry['utterance'] += \" \" + line\n",
        "            else:\n",
        "                current_entry['utterance'] = line\n",
        "        else:\n",
        "            current_entry = {'speaker': 'Unknown', 'job_title': \"\", 'utterance': line}\n",
        "    if current_entry is not None:\n",
        "        entries.append(current_entry)\n",
        "    return entries\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Process all PDFs in the raw/ubs directory and accumulate results\n",
        "# -------------------------------\n",
        "management_entries_all = []\n",
        "qa_entries_all = []\n",
        "def remove_colons_from_dict(entry):\n",
        "    \"\"\"Removes colons from all string fields in the dictionary.\"\"\"\n",
        "    for key, value in entry.items():\n",
        "        if isinstance(value, str):\n",
        "            entry[key] = value.replace(\":\", \"\")\n",
        "    return entry\n",
        "\n",
        "for filename in os.listdir(raw_dir):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(raw_dir, filename)\n",
        "        print(f\"Processing file: {filename}\")\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            transcript_text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    transcript_text += page_text + \"\\n\"\n",
        "        cleaned_text = clean_transcript(transcript_text)\n",
        "        financial_quarter, call_date = extract_metadata(cleaned_text)\n",
        "        management_text, qa_text = split_sections(cleaned_text)\n",
        "\n",
        "        # Parse the sections\n",
        "        management_entries = parse_management_section(management_text)\n",
        "        qa_entries = parse_qa_section_simple(qa_text)\n",
        "\n",
        "        # Add metadata and filename info to each parsed entry\n",
        "        for entry in management_entries:\n",
        "            entry['call_date'] = call_date\n",
        "            entry['financial_quarter'] = financial_quarter\n",
        "            entry['source_file'] = filename\n",
        "            management_entries_all.append(entry)\n",
        "        for entry in qa_entries:\n",
        "            entry['call_date'] = call_date\n",
        "            entry['financial_quarter'] = financial_quarter\n",
        "            entry['source_file'] = filename\n",
        "            entry = remove_colons_from_dict(entry)\n",
        "            qa_entries_all.append(entry)\n",
        "\n",
        "# Convert the accumulated lists to DataFrames\n",
        "df_management = pd.DataFrame(management_entries_all)\n",
        "df_qa = pd.DataFrame(qa_entries_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOg_-N-LqdA7",
        "outputId": "f9a1485a-c6cd-4193-9120-3c7bf3c0400a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Management announcements saved to: /content/drive/MyDrive/BOE/bank_of_england/data/processed/ubs_management_discussion.csv\n",
            "Q&A section saved to: /content/drive/MyDrive/BOE/bank_of_england/data/processed/ubs_qna_section.csv\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# 7. Save the results to CSV files\n",
        "# -------------------------------\n",
        "management_csv_path = os.path.join(processed_dir, \"ubs_management_discussion.csv\")\n",
        "qa_csv_path = os.path.join(processed_dir, \"ubs_qna_section.csv\")\n",
        "\n",
        "df_management.to_csv(management_csv_path, index=False)\n",
        "df_qa.to_csv(qa_csv_path, index=False)\n",
        "\n",
        "print(\"Management announcements saved to:\", management_csv_path)\n",
        "print(\"Q&A section saved to:\", qa_csv_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}