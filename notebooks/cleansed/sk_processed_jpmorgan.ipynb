{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/cleansed/sk_processed_jpmorgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2C9SSNHREALD",
        "outputId": "bbdc9664-2cd8-4d26-85b7-080da17ce859"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.1\\n\\nDescription:\\n    This notebook contains data engineering functions to process earnings call transcripts (in PDF format)\\n    from JPMorgan and similar sources stored on Google Drive. The code performs the following tasks:\\n\\n    1. Reads raw PDF files from a specified directory using pdfplumber.\\n    2. Cleans and preprocesses the transcript text via regular expressions.\\n    3. Extracts metadata such as the financial quarter and call date from the transcript.\\n    4. Splits the transcript into two sections:\\n         - Management Discussion: Contains the prepared remarks and discussion from management.\\n         - Q&A Section: Contains the questions and answers from the call.\\n    5. Parses the Q&A section:\\n         - Extracts individual Q&A entries (each row in qa_section_v2.csv contains a single speaker’s entry,\\n           along with optional markers, job titles, and utterances).\\n         - Pairs question entries with their corresponding answer entries to form coherent Q&A pairs\\n           (stored in paired_qa_section_v2.csv).\\n    6. Aggregates the management discussion and Q&A data (both individual entries and paired Q&A) into Pandas DataFrames.\\n    7. Formats and sorts the DataFrames (e.g., converts call dates to datetime objects).\\n    8. Saves the processed results as CSV files to a specified directory on Google Drive.\\n\\n===================================================\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.1\n",
        "\n",
        "Description:\n",
        "    This notebook contains data engineering functions to process earnings call transcripts (in PDF format)\n",
        "    from JPMorgan and similar sources stored on Google Drive. The code performs the following tasks:\n",
        "\n",
        "    1. Reads raw PDF files from a specified directory using pdfplumber.\n",
        "    2. Cleans and preprocesses the transcript text via regular expressions.\n",
        "    3. Extracts metadata such as the financial quarter and call date from the transcript.\n",
        "    4. Splits the transcript into two sections:\n",
        "         - Management Discussion: Contains the prepared remarks and discussion from management.\n",
        "         - Q&A Section: Contains the questions and answers from the call.\n",
        "    5. Parses the Q&A section:\n",
        "         - Extracts individual Q&A entries (each row in qa_section_v2.csv contains a single speaker’s entry,\n",
        "           along with optional markers, job titles, and utterances).\n",
        "         - Pairs question entries with their corresponding answer entries to form coherent Q&A pairs\n",
        "           (stored in paired_qa_section_v2.csv).\n",
        "    6. Aggregates the management discussion and Q&A data (both individual entries and paired Q&A) into Pandas DataFrames.\n",
        "    7. Formats and sorts the DataFrames (e.g., converts call dates to datetime objects).\n",
        "    8. Saves the processed results as CSV files to a specified directory on Google Drive.\n",
        "\n",
        "===================================================\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XmhbIl8CKIy"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z3XRr1FJDIX5"
      },
      "outputs": [],
      "source": [
        "!pip install -q pymupdf > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LChGfv3tCKIz"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\n",
        "import pymupdf\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VD8ipZFGbLk",
        "outputId": "81ad8764-5df1-4014-e6ca-79f4884cbb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alfo7sI13TXa",
        "outputId": "6cd0d2b8-a641-408c-f983-a79dac0910eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/1q23-earnings-transcript.pdf\n",
            "Processed file: 1q23-earnings-transcript.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/2q23-earnings-transcript.pdf\n",
            "Processed file: 2q23-earnings-transcript.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/4q24-earnings-transcript.pdf\n",
            "Processed file: 4q24-earnings-transcript.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf\n",
            "Processed file: jpm-1q24-earnings-call-transcript.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/jpm-2q24-earnings-call-transcript-final.pdf\n",
            "Processed file: jpm-2q24-earnings-call-transcript-final.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/jpm-3q23-earnings-call-transcript.pdf\n",
            "Processed file: jpm-3q23-earnings-call-transcript.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/jpm-4q23-earnings-call-transcript.pdf\n",
            "Processed file: jpm-4q23-earnings-call-transcript.pdf\n",
            "Processing file: /content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan/jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf\n",
            "Processed file: jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf\n",
            "Management Discussion Segments DataFrame saved to: /content/drive/My Drive/BOE/bank_of_england/data/cleansed/jpmorgan_management_discussion.csv\n",
            "Paired Q&A DataFrame saved to: /content/drive/My Drive/BOE/bank_of_england/data/cleansed/jpmorgan_qa_section.csv\n",
            "\n",
            "Total PDF files in directory: 8\n",
            "Unique filenames in parsed data: {'jpm-4q23-earnings-call-transcript.pdf', 'jpm-2q24-earnings-call-transcript-final.pdf', '4q24-earnings-transcript.pdf', 'jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf', 'jpm-1q24-earnings-call-transcript.pdf', 'jpm-3q23-earnings-call-transcript.pdf', '2q23-earnings-transcript.pdf', '1q23-earnings-transcript.pdf'}\n",
            "All PDF files have been processed.\n",
            "\n",
            "For Q&A entries:\n",
            "  Missing filename: 0\n",
            "  Missing call_date: 0\n",
            "  Missing Quarter: 0\n",
            "\n",
            "For Management Discussion segments:\n",
            "  Missing filename: 0\n",
            "  Missing call_date: 0\n",
            "  Missing Quarter: 0\n",
            "\n",
            "For Paired Q&A:\n",
            "  Missing filename: 0\n",
            "  Missing call_date: 0\n",
            "  Missing Quarter: 0\n",
            "\n",
            "No 'Operator' entries found in management discussion segments.\n",
            "\n",
            "Total Q&A entries: 583\n",
            "Total management discussion segments: 11\n",
            "Total paired Q&A entries: 143\n",
            "\n",
            "Sample Paired Q&A entries:\n",
            "         Analyst                 Analyst Role            Question  \\\n",
            "0  Steven Chubak  Analyst, Wolfe Research LLC  Hey, good morning.   \n",
            "\n",
            "       Executive                      Executive Role Type  \\\n",
            "0  Jeremy Barnum  Chief Financial Officer, JPMorgan Chase   \n",
            "\n",
            "               Response                      filename Quarter  call_date  \n",
            "0  Good morning, Steve.  1q23-earnings-transcript.pdf    1Q23 2023-04-14  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Define the path to your raw folder on Google Drive\n",
        "# -------------------------------\n",
        "raw_dir = \"/content/drive/My Drive/BOE/bank_of_england/data/raw/jpmorgan\"\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Define speaker corrections for known names\n",
        "# -------------------------------\n",
        "SPEAKER_CORRECTIONS = {\n",
        "    \"Operator\": \"Operator\",  # We'll filter these out later.\n",
        "    \"Mike Mayo\": \"Mike Mayo, Analyst, Wells Fargo Securities LLC\",\n",
        "    \"Jim Mitchell\": \"Jim Mitchell, Analyst, Seaport Global Securities LLC\",\n",
        "    \"Jeremy Barnum\": \"Jeremy Barnum, Chief Financial Officer, JPMorgan Chase\",\n",
        "    \"Jamie Dimon\": \"Jamie Dimon, Chairman & Chief Executive Officer, JPMorgan Chase\",\n",
        "    \"Erika Najarian\": \"Erika Najarian, Analyst, UBS Securities LLC\",\n",
        "    \"John McDonald\": \"John McDonald, Analyst, Autonomous Research\",\n",
        "    \"Ken Usdin\": \"Ken Usdin, Analyst, Jefferies LLC\",\n",
        "    \"Gerard Cassidy\": \"Gerard Cassidy, Analyst, RBC Capital Markets LLC\",\n",
        "    \"Steven Chubak\": \"Steven Chubak, Analyst, Wolfe Research LLC\",\n",
        "    \"Matt O’Connor\": \"Matt O’Connor, Analyst, Deutsche Bank Securities, Inc.\",\n",
        "    \"Betsy L. Graseck\": \"Betsy L. Graseck, Analyst, Morgan Stanley & Co. LLC\",\n",
        "    \"Saul Martinez\": \"Saul Martinez, Analyst, HSBC Securities (USA), Inc.\",\n",
        "    \"Ebrahim H. Poonawala\": \"Ebrahim H. Poonawala, Analyst, Bank of America Merrill Lynch\",\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define helper functions for processing\n",
        "# -------------------------------\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Extracts text from a PDF file using PyMuPDF.\"\"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_transcript(text):\n",
        "    \"\"\"Cleans the raw transcript text.\"\"\"\n",
        "    # Normalize quotes.\n",
        "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "    text = re.sub(r'\\n\\s*\\.{10,}\\s*\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
        "    text = re.sub(r'On page \\d+', '', text)\n",
        "    text = re.sub(r'Starting on page \\d+', '', text)\n",
        "    text = re.sub(r'\\.\\s*,', '.', text)\n",
        "    text = text.replace('%. ,', '%.')\n",
        "    text = re.sub(r'\\s+\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "    if \"Disclaimer\" in text:\n",
        "        text = text.split(\"Disclaimer\")[0].strip()\n",
        "    return text\n",
        "\n",
        "def extract_metadata(text):\n",
        "    \"\"\"Extracts the financial quarter and call date from the transcript text.\"\"\"\n",
        "    quarter_match = re.search(r'(\\dQ\\s*\\d{2})', text)\n",
        "    financial_quarter = quarter_match.group(1).replace(\" \", \"\") if quarter_match else None\n",
        "    date_match = re.search(r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})', text)\n",
        "    call_date = date_match.group(1) if date_match else None\n",
        "    return financial_quarter, call_date\n",
        "\n",
        "def split_sections(transcript):\n",
        "    \"\"\"\n",
        "    Splits the transcript into Management Discussion and Q&A sections.\n",
        "    Assumes that the Q&A section is introduced by a marker like \"QUESTION AND ANSWER\" (case-insensitive).\n",
        "    Returns a tuple: (management_discussion, qa_section)\n",
        "    \"\"\"\n",
        "    qa_marker = re.search(r'(?i)(QUESTION\\s+AND\\s+ANSWER)', transcript)\n",
        "    if qa_marker:\n",
        "        management_discussion = transcript[:qa_marker.start()].strip()\n",
        "        qa_section = transcript[qa_marker.start():].strip()\n",
        "    else:\n",
        "        management_discussion = transcript\n",
        "        qa_section = \"\"\n",
        "    return management_discussion, qa_section\n",
        "\n",
        "def parse_management_discussion_section(md_text, speaker_corrections):\n",
        "    \"\"\"\n",
        "    Parses the Management Discussion section into segments.\n",
        "    Each segment is a dictionary with keys 'speaker' and 'utterance'.\n",
        "    It detects lines that begin with one of the known speaker names and starts a new segment.\n",
        "    \"\"\"\n",
        "    segments = []\n",
        "    current_speaker = None\n",
        "    current_utterance = []\n",
        "    lines = md_text.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        found = None\n",
        "        for short_name, full_name in speaker_corrections.items():\n",
        "            pattern = r'^' + re.escape(short_name) + r'([\\s,:-].*)?$'\n",
        "            if re.match(pattern, line, re.IGNORECASE):\n",
        "                found = full_name\n",
        "                break\n",
        "        if found:\n",
        "            if current_speaker or current_utterance:\n",
        "                segments.append({\n",
        "                    'speaker': current_speaker,\n",
        "                    'utterance': ' '.join(current_utterance).strip()\n",
        "                })\n",
        "            current_speaker = found\n",
        "            if ':' in line:\n",
        "                parts = line.split(':', 1)\n",
        "                current_utterance = [parts[1].strip()]\n",
        "            else:\n",
        "                current_utterance = []\n",
        "        else:\n",
        "            current_utterance.append(line)\n",
        "    if current_speaker or current_utterance:\n",
        "        segments.append({\n",
        "            'speaker': current_speaker,\n",
        "            'utterance': ' '.join(current_utterance).strip()\n",
        "        })\n",
        "    return segments\n",
        "\n",
        "def parse_qa_section(qa_text):\n",
        "    \"\"\"\n",
        "    Parses the Q&A section from the transcript.\n",
        "    Expects a layout where each speaker block has:\n",
        "      - Line 1: Speaker name (exact match in SPEAKER_CORRECTIONS)\n",
        "      - Line 2: Job title (free text)\n",
        "      - Line 3: Marker ('Q' or 'A')\n",
        "      - Line 4+: Utterance (until the next speaker is detected)\n",
        "    Returns a list of dictionaries with keys: 'speaker', 'job_title', 'marker', 'utterance'\n",
        "    \"\"\"\n",
        "    lines = qa_text.split('\\n')\n",
        "    entries = []\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        line = lines[i].strip()\n",
        "        if line in SPEAKER_CORRECTIONS:\n",
        "            speaker = SPEAKER_CORRECTIONS[line]\n",
        "            job_title = lines[i+1].strip() if i+1 < len(lines) else \"\"\n",
        "            marker = lines[i+2].strip() if i+2 < len(lines) else \"\"\n",
        "            utterance_lines = []\n",
        "            i += 3\n",
        "            while i < len(lines) and (lines[i].strip() not in SPEAKER_CORRECTIONS):\n",
        "                utterance_lines.append(lines[i].strip())\n",
        "                i += 1\n",
        "            utterance = \" \".join(utterance_lines)\n",
        "            entries.append({\n",
        "                'speaker': speaker,\n",
        "                'job_title': job_title,\n",
        "                'marker': marker,\n",
        "                'utterance': utterance\n",
        "            })\n",
        "        else:\n",
        "            i += 1\n",
        "    return entries\n",
        "\n",
        "def add_metadata_to_entries(entries, filename, financial_quarter, call_date):\n",
        "    \"\"\"Adds filename, financial_quarter, and call_date metadata to each entry.\"\"\"\n",
        "    for entry in entries:\n",
        "        entry['filename'] = filename\n",
        "        entry['financial_quarter'] = financial_quarter\n",
        "        entry['call_date'] = call_date\n",
        "    return entries\n",
        "\n",
        "def pair_qa_entries(entries):\n",
        "    \"\"\"\n",
        "    Pairs Q&A entries assuming they alternate (Q then A).\n",
        "    Returns a list of dictionaries with keys:\n",
        "      'question_speaker', 'question_job_title', 'question',\n",
        "      'answer_speaker', 'answer_job_title', 'answer',\n",
        "      'filename', 'financial_quarter', and 'call_date'\n",
        "    \"\"\"\n",
        "    paired = []\n",
        "    i = 0\n",
        "    while i < len(entries):\n",
        "        if entries[i].get('marker', '') == 'Q':\n",
        "            question = entries[i]\n",
        "            j = i + 1\n",
        "            answer = None\n",
        "            while j < len(entries):\n",
        "                if entries[j].get('marker', '') == 'A' or (not entries[j].get('marker') and j == i+1):\n",
        "                    answer = entries[j]\n",
        "                    break\n",
        "                j += 1\n",
        "            paired_entry = {\n",
        "                'question_speaker': question.get('speaker', ''),\n",
        "                'question_job_title': question.get('job_title', ''),\n",
        "                'question': question.get('utterance', ''),\n",
        "                'answer_speaker': answer.get('speaker', '') if answer else \"\",\n",
        "                'answer_job_title': answer.get('job_title', '') if answer else \"\",\n",
        "                'answer': answer.get('utterance', '') if answer else \"\"\n",
        "            }\n",
        "            for key in ['filename', 'financial_quarter', 'call_date']:\n",
        "                paired_entry[key] = question.get(key, \"\")\n",
        "            paired.append(paired_entry)\n",
        "            i = j + 1 if answer else i + 1\n",
        "        else:\n",
        "            paired_entry = {\n",
        "                'question_speaker': entries[i].get('speaker', ''),\n",
        "                'question_job_title': entries[i].get('job_title', ''),\n",
        "                'question': entries[i].get('utterance', ''),\n",
        "                'answer_speaker': \"\",\n",
        "                'answer_job_title': \"\",\n",
        "                'answer': \"\"\n",
        "            }\n",
        "            for key in ['filename', 'financial_quarter', 'call_date']:\n",
        "                paired_entry[key] = entries[i].get(key, \"\")\n",
        "            paired.append(paired_entry)\n",
        "            i += 1\n",
        "    return paired\n",
        "\n",
        "def filter_operator_qa_pairs(df):\n",
        "    \"\"\"\n",
        "    Removes rows from the paired Q&A DataFrame where the text \"Operator:\" (case-insensitive)\n",
        "    appears in either the 'question' or 'answer' field.\n",
        "    \"\"\"\n",
        "    if 'question' not in df.columns or 'answer' not in df.columns:\n",
        "        return df\n",
        "    mask = ~(df['question'].str.contains(r'(?i)Operator:', na=False) |\n",
        "             df['answer'].str.contains(r'(?i)Operator:', na=False))\n",
        "    return df[mask]\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Process each PDF in the raw folder and aggregate results\n",
        "# -------------------------------\n",
        "all_qa_entries = []      # To store parsed Q&A entries from all transcripts.\n",
        "all_md_segments = []     # To store management discussion segments from all transcripts.\n",
        "\n",
        "for filename in os.listdir(raw_dir):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(raw_dir, filename)\n",
        "        print(\"Processing file:\", file_path)\n",
        "\n",
        "        # Extract text using PyMuPDF.\n",
        "        transcript_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "        # Clean transcript.\n",
        "        transcript_clean = clean_transcript(transcript_text)\n",
        "\n",
        "        # Extract metadata.\n",
        "        financial_quarter, call_date = extract_metadata(transcript_clean)\n",
        "\n",
        "        # Split transcript into sections.\n",
        "        management_discussion, qa_section = split_sections(transcript_clean)\n",
        "        qa_section = re.sub(r'(?i)^QUESTION\\s+AND\\s+ANSWER\\s+SECTION\\s*', '', qa_section, count=1).strip()\n",
        "\n",
        "        # Parse Management Discussion.\n",
        "        md_segments = parse_management_discussion_section(management_discussion, SPEAKER_CORRECTIONS)\n",
        "        # Filter out segments with missing speaker or where speaker is \"Operator\"\n",
        "        md_segments = [seg for seg in md_segments if seg.get('speaker') and seg.get('speaker').strip().lower() != \"operator\"]\n",
        "        md_segments = add_metadata_to_entries(md_segments, filename, financial_quarter, call_date)\n",
        "        all_md_segments.extend(md_segments)\n",
        "\n",
        "        # Parse Q&A section.\n",
        "        qa_entries = parse_qa_section(qa_section)\n",
        "        # Filter out Q&A entries with missing speaker or where speaker is \"Operator\"\n",
        "        qa_entries = [entry for entry in qa_entries if entry.get('speaker') and entry.get('speaker').strip().lower() != \"operator\"]\n",
        "        qa_entries = add_metadata_to_entries(qa_entries, filename, financial_quarter, call_date)\n",
        "        all_qa_entries.extend(qa_entries)\n",
        "        print(\"Processed file:\", filename)\n",
        "\n",
        "# Pair Q&A entries.\n",
        "qa_pairs = pair_qa_entries(all_qa_entries)\n",
        "\n",
        "# --- New Step: Remove Q&A pairs with an empty question field ---\n",
        "qa_pairs = [pair for pair in qa_pairs if str(pair.get('question', '')).strip() != \"\"]\n",
        "\n",
        "# --- New Step: Filter out Q&A pairs where \"Operator:\" appears in question or answer ---\n",
        "df_qa_pairs = pd.DataFrame(qa_pairs)\n",
        "df_qa_pairs = filter_operator_qa_pairs(df_qa_pairs)\n",
        "\n",
        "# --- New Step: Remove Q&A pairs where the answer_speaker is null or empty ---\n",
        "df_qa_pairs = df_qa_pairs[df_qa_pairs['answer_speaker'].astype(str).str.strip() != \"\"]\n",
        "\n",
        "# Convert aggregated lists to DataFrames.\n",
        "df_qa_all = pd.DataFrame(all_qa_entries)\n",
        "df_md_segments = pd.DataFrame(all_md_segments)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Format 'call_date' as datetime\n",
        "# -------------------------------\n",
        "for df in [df_qa_all, df_md_segments, df_qa_pairs]:\n",
        "    if 'call_date' not in df.columns:\n",
        "        df['call_date'] = pd.NaT\n",
        "    else:\n",
        "        df['call_date'] = pd.to_datetime(df['call_date'], format='%B %d, %Y', errors='coerce')\n",
        "\n",
        "# --- New Step: Split speaker names into separate name and role fields ---\n",
        "def split_speaker(speaker_str):\n",
        "    if isinstance(speaker_str, str) and ',' in speaker_str:\n",
        "        parts = speaker_str.split(',', 1)\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "    return speaker_str, \"\"\n",
        "\n",
        "# For question speakers:\n",
        "df_qa_pairs['Analyst'], df_qa_pairs['Analyst Role'] = zip(*df_qa_pairs['question_speaker'].apply(split_speaker))\n",
        "# For answer speakers:\n",
        "df_qa_pairs['Executive'], df_qa_pairs['Executive Role Type'] = zip(*df_qa_pairs['answer_speaker'].apply(split_speaker))\n",
        "\n",
        "# --- New Step: Rename remaining columns to match final mapping ---\n",
        "df_qa_pairs.rename(columns={\n",
        "    'financial_quarter': 'Quarter',\n",
        "    'question': 'Question',\n",
        "    'answer': 'Response'\n",
        "}, inplace=True)\n",
        "# (The filename and call_date columns remain unchanged.)\n",
        "\n",
        "# Reorder columns to match the desired final output:\n",
        "final_columns = ['Analyst', 'Analyst Role', 'Question', 'Executive', 'Executive Role Type', 'Response', 'filename', 'Quarter', 'call_date']\n",
        "df_qa_pairs = df_qa_pairs[final_columns]\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Save the DataFrames as CSV Files\n",
        "# -------------------------------\n",
        "md_csv_path = \"/content/drive/My Drive/BOE/bank_of_england/data/cleansed/jpmorgan_management_discussion.csv\"\n",
        "qa_pairs_csv_path = \"/content/drive/My Drive/BOE/bank_of_england/data/cleansed/jpmorgan_qa_section.csv\"\n",
        "\n",
        "df_md_segments.to_csv(md_csv_path, index=False)\n",
        "print(\"Management Discussion Segments DataFrame saved to:\", md_csv_path)\n",
        "df_qa_pairs.to_csv(qa_pairs_csv_path, index=False)\n",
        "print(\"Paired Q&A DataFrame saved to:\", qa_pairs_csv_path)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Data Validation\n",
        "# -------------------------------\n",
        "\n",
        "# 7.1. Check that all PDF files have been processed.\n",
        "pdf_files = [f for f in os.listdir(raw_dir) if f.lower().endswith(\".pdf\")]\n",
        "processed_files = set()\n",
        "if 'filename' in df_qa_all.columns:\n",
        "    processed_files = processed_files.union(set(df_qa_all['filename'].unique()))\n",
        "if 'filename' in df_md_segments.columns:\n",
        "    processed_files = processed_files.union(set(df_md_segments['filename'].unique()))\n",
        "print(f\"\\nTotal PDF files in directory: {len(pdf_files)}\")\n",
        "print(f\"Unique filenames in parsed data: {processed_files}\")\n",
        "missing_files = set(pdf_files) - processed_files\n",
        "if missing_files:\n",
        "    print(\"Warning: The following PDF files were not processed:\", missing_files)\n",
        "else:\n",
        "    print(\"All PDF files have been processed.\")\n",
        "\n",
        "# 7.2. Check for missing metadata in key columns.\n",
        "for name, df in [(\"Q&A entries\", df_qa_all), (\"Management Discussion segments\", df_md_segments), (\"Paired Q&A\", df_qa_pairs)]:\n",
        "    missing_filename = df['filename'].isnull().sum() if 'filename' in df.columns else 0\n",
        "    missing_call_date = df['call_date'].isnull().sum() if 'call_date' in df.columns else 0\n",
        "    # Check for missing Quarter: if already renamed, use 'Quarter'\n",
        "    missing_quarter = df['Quarter'].isnull().sum() if 'Quarter' in df.columns else df['financial_quarter'].isnull().sum()\n",
        "    print(f\"\\nFor {name}:\")\n",
        "    print(f\"  Missing filename: {missing_filename}\")\n",
        "    print(f\"  Missing call_date: {missing_call_date}\")\n",
        "    print(f\"  Missing Quarter: {missing_quarter}\")\n",
        "\n",
        "# 7.3. Verify that no management discussion segments have \"Operator\" as the speaker.\n",
        "if (df_md_segments['speaker'].str.lower() == \"operator\").any():\n",
        "    print(\"\\nError: 'Operator' entries found in management discussion segments!\")\n",
        "else:\n",
        "    print(\"\\nNo 'Operator' entries found in management discussion segments.\")\n",
        "\n",
        "# 7.4. Display summary statistics.\n",
        "print(\"\\nTotal Q&A entries:\", len(df_qa_all))\n",
        "print(\"Total management discussion segments:\", len(df_md_segments))\n",
        "print(\"Total paired Q&A entries:\", len(df_qa_pairs))\n",
        "\n",
        "# 7.5. Display sample data for manual review.\n",
        "print(\"\\nSample Paired Q&A entries:\")\n",
        "print(df_qa_pairs.head(1))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}