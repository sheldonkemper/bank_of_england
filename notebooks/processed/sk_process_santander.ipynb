{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/processed/sk_process_santander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjcWzjxiUs3i"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.1\n",
        "\n",
        "Description:\n",
        "    This notebook implements a system for processing and converting video transcripts into a single CSV file\n",
        "    for the Bank of England project. The workflow processes MP4 files stored in the raw data directory on Google Drive\n",
        "    by using a machine learning-based speech-to-text model (e.g., OpenAI’s Whisper) to transcribe the audio content into text.\n",
        "    Each transcript is appended as a record in the CSV file along with metadata—such as the year, quarter, and a duplicate indicator—\n",
        "    which are inferred from the video file name. This pipeline supports the ongoing integration of transcripts across multiple\n",
        "    quarters and years, facilitating further analysis and reporting within our data engineering infrastructure.\n",
        "\n",
        "===================================================\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "vmX59-ASY-gE",
        "outputId": "b3cfa86a-96e9-4411-f21e-87eadf30c71f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.1\\n\\nDescription:\\n    This notebook implements a system for processing and converting video transcripts into a single CSV file\\n    for the Bank of England project. The workflow processes MP4 files stored in the raw data directory on Google Drive\\n    by using a machine learning-based speech-to-text model (e.g., OpenAI’s Whisper) to transcribe the audio content into text.\\n    Each transcript is appended as a record in the CSV file along with metadata—such as the year, quarter, and a duplicate indicator—\\n    which are inferred from the video file name. This pipeline supports the ongoing integration of transcripts across multiple\\n    quarters and years, facilitating further analysis and reporting within our data engineering infrastructure.\\n\\n===================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install whisper (if not already installed)\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyCigYPknYBS",
        "outputId": "4f3cd959-9d85-4088-faec-51e7589a5d83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-t6nt6q0m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-t6nt6q0m\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.61.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.44.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803669 sha256=9230f14a2e0cf17281d02ec8cec7514347a9886f4be89bd7703d80dbc6ce443e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2uajy1hy/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IyrdK8-KUs3l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import whisper\n",
        "import re\n",
        "import csv\n",
        "import whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6VD8ipZFGbLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858d0955-34ed-4cbb-96b4-b51509dce5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BOE Directory Contents: ['model', 'raw', 'processed']\n",
            "Raw Data Directory Contents: ['video_2024_Q1_1.mp4', 'video_2024_Q2_2.mp4', 'video_2024_Q3_3.mp4', 'video_2024_Q4_4.mp4', 'video_2023_Q1_5.mp4', 'video_2023_Q2_6.mp4', 'video_2023_Q3_7.mp4', 'video_2023_Q4_8.mp4']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Assuming 'BOE' folder is in 'MyDrive' and already shared\n",
        "BOE_path = '/content/drive/MyDrive/BOE/bank_of_england/data'\n",
        "\n",
        "# List the contents of the BOE directory\n",
        "print(\"BOE Directory Contents:\", os.listdir(BOE_path))\n",
        "\n",
        "# Define the raw data path (assuming your audio files are under raw/santander)\n",
        "raw_data_path = os.path.join(BOE_path, 'raw', 'santander')\n",
        "print(\"Raw Data Directory Contents:\", os.listdir(raw_data_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process All Downloaded MP4 Files"
      ],
      "metadata": {
        "id": "XtSShNPv1DDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import whisper\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_call_dates():\n",
        "    \"\"\"\n",
        "    Scrapes the Santander Financial and Economic Information page to build a mapping\n",
        "    of financial quarter to call date. This function assumes that the page contains quarterly\n",
        "    result sections within <div class=\"documents-wrapper\"> elements. Within each wrapper:\n",
        "      - A <div class=\"title-document\"> contains a <span class=\"text-title\"> with text like \"Q4 2024\".\n",
        "      - The first <div class=\"documents-block__date\"> element within the wrapper holds the call date (e.g., \"05-02-2025\").\n",
        "    Returns a dictionary mapping keys like \"2024 Q4\" to the call date.\n",
        "    \"\"\"\n",
        "    url = \"https://www.santander.com/en/shareholders-and-investors/financial-and-economic-information\"\n",
        "    call_date_mapping = {}\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching call dates:\", e)\n",
        "        return call_date_mapping\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    wrappers = soup.find_all(\"div\", class_=\"documents-wrapper\")\n",
        "    for wrapper in wrappers:\n",
        "        title_document = wrapper.find(\"div\", class_=\"title-document\")\n",
        "        if title_document:\n",
        "            span_title = title_document.find(\"span\", class_=\"text-title\")\n",
        "            if span_title:\n",
        "                title_text = span_title.get_text(strip=True)\n",
        "                # Expect title text like \"Q4 2024\"; extract quarter and year.\n",
        "                match = re.search(r'(Q[1-4])\\s+(\\d{4})', title_text)\n",
        "                if match:\n",
        "                    quarter = match.group(1)\n",
        "                    year = match.group(2)\n",
        "                    key = f\"{year} {quarter}\"\n",
        "                    # Look for the call date in the first <div class=\"documents-block__date\">\n",
        "                    date_elem = wrapper.find(\"div\", class_=\"documents-block__date\")\n",
        "                    if date_elem:\n",
        "                        call_date = date_elem.get_text(strip=True)\n",
        "                        if call_date:\n",
        "                            call_date_mapping[key] = call_date\n",
        "                        else:\n",
        "                            call_date_mapping[key] = \"Unknown\"\n",
        "    return call_date_mapping\n",
        "\n",
        "def parse_financial_quarter(filename):\n",
        "    \"\"\"\n",
        "    Given a filename (e.g., \"video_2023_Q3_1\"), extract and return a string like \"2023 Q3\".\n",
        "    If the pattern is not found, return \"Unknown\".\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d{4})_(Q[1-4])', filename)\n",
        "    if match:\n",
        "        year = match.group(1)\n",
        "        quarter = match.group(2)\n",
        "        return f\"{year} {quarter}\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Define directories – adjust these paths as needed.\n",
        "raw_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/raw/santander'\n",
        "processed_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/processed'\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Load the Whisper transcription model.\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Define the CSV file where all transcripts will be appended.\n",
        "all_transcripts_csv = os.path.join(processed_dir, \"santander_management_discussion.csv\")\n",
        "\n",
        "# Prepare a set to store already processed file names for duplicate checking.\n",
        "existing_files = set()\n",
        "if os.path.exists(all_transcripts_csv):\n",
        "    with open(all_transcripts_csv, \"r\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            if \"filename\" in row:\n",
        "                existing_files.add(row[\"filename\"])\n",
        "\n",
        "# If the CSV file doesn't exist, create it with the desired header.\n",
        "if not os.path.exists(all_transcripts_csv):\n",
        "    with open(all_transcripts_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"filename\", \"management_discussion\", \"financial_quarter\", \"call_date\"])\n",
        "\n",
        "# Fetch the mapping of financial quarter to call date from Santander's page.\n",
        "call_date_mapping = get_call_dates()\n",
        "print(\"Call Date Mapping:\", call_date_mapping)\n",
        "\n",
        "# Process each MP4 file in the raw directory.\n",
        "mp4_files = glob.glob(os.path.join(raw_dir, \"*.mp4\"))\n",
        "\n",
        "for mp4_file in mp4_files:\n",
        "    print(f\"\\nProcessing MP4 file: {mp4_file}\")\n",
        "    # Transcribe the video using Whisper.\n",
        "    result = model.transcribe(mp4_file)\n",
        "    transcript_text = result[\"text\"]\n",
        "\n",
        "    # Use the file's base name as an identifier.\n",
        "    base_name = os.path.splitext(os.path.basename(mp4_file))[0]\n",
        "\n",
        "    # Extract the financial quarter from the filename.\n",
        "    financial_quarter = parse_financial_quarter(base_name)\n",
        "    # Look up the call date from our mapping; default to \"Unknown\" if not found.\n",
        "    call_date = call_date_mapping.get(financial_quarter, \"Unknown\")\n",
        "\n",
        "    # Check for duplicates.\n",
        "    duplicate_flag = \"Yes\" if base_name in existing_files else \"No\"\n",
        "    existing_files.add(base_name)\n",
        "    if duplicate_flag == \"Yes\":\n",
        "        print(f\"Duplicate found for {base_name}.\")\n",
        "\n",
        "    # Append the new record to the CSV with headers: filename, management_discussion, financial_quarter, call_date.\n",
        "    with open(all_transcripts_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([base_name, transcript_text, financial_quarter, call_date])\n",
        "\n",
        "    print(f\"Transcript for '{base_name}' appended (financial_quarter: {financial_quarter}, call_date: {call_date}).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cTUjjFd0y7u",
        "outputId": "1cbc608f-b5ca-4c9a-fea0-d98acbf3a482"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call Date Mapping: {}\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2024_Q1_1.mp4\n",
            "Transcript for 'video_2024_Q1_1' appended (financial_quarter: 2024 Q1, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2024_Q2_2.mp4\n",
            "Transcript for 'video_2024_Q2_2' appended (financial_quarter: 2024 Q2, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2024_Q3_3.mp4\n",
            "Transcript for 'video_2024_Q3_3' appended (financial_quarter: 2024 Q3, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2024_Q4_4.mp4\n",
            "Transcript for 'video_2024_Q4_4' appended (financial_quarter: 2024 Q4, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2023_Q1_5.mp4\n",
            "Transcript for 'video_2023_Q1_5' appended (financial_quarter: 2023 Q1, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2023_Q2_6.mp4\n",
            "Transcript for 'video_2023_Q2_6' appended (financial_quarter: 2023 Q2, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2023_Q3_7.mp4\n",
            "Transcript for 'video_2023_Q3_7' appended (financial_quarter: 2023 Q3, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2023_Q4_8.mp4\n",
            "Transcript for 'video_2023_Q4_8' appended (financial_quarter: 2023 Q4, call_date: Unknown).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Ensure the 'punkt_tab' resource is downloaded\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def chunk_text(text, max_chunk_size=500):\n",
        "    \"\"\"\n",
        "    Splits the input text into chunks that do not exceed max_chunk_size characters.\n",
        "    The splitting is based on sentence boundaries.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The full text to be chunked.\n",
        "        max_chunk_size (int): Maximum number of characters per chunk.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into sentences.\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # If adding the next sentence would exceed the maximum size, save the current chunk.\n",
        "        if len(current_chunk) + len(sentence) + 1 > max_chunk_size:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence + \" \"\n",
        "            else:\n",
        "                # If a single sentence exceeds max_chunk_size, add it as its own chunk.\n",
        "                chunks.append(sentence.strip())\n",
        "                current_chunk = \"\"\n",
        "        else:\n",
        "            current_chunk += sentence + \" \"\n",
        "\n",
        "    if current_chunk.strip():\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage:\n",
        "transcript_text = (\n",
        "    \"I'm going to start with the presentation. Please everyone and welcome to our third quarter earnings presentation. \"\n",
        "    \"Like always, the presentation will start with our CEO's comments, followed by my detailed explanation of the P&L. \"\n",
        "    \"He will then offer his concluding remarks and we will open it up for questions. \"\n",
        "    \"This quarter has been a record quarter for Santander, with profit up 12% compared to last quarter. \"\n",
        "    \"On the back of a strong customer base of 171 million, we continue to demonstrate resilience in our business model. \"\n",
        "    \"Our efficiency has improved, and our balance sheets remain solid. \"\n",
        "    \"We are on track to exceed our targets for the year. \"\n",
        "    \"Now, let's move into a more detailed discussion on our financial performance, starting with revenue growth and cost management. \"\n",
        "    \"We are confident that the strategies implemented will continue to deliver strong results in the upcoming quarters.\"\n",
        ")\n",
        "\n",
        "# Split the transcript into chunks (max 500 characters per chunk).\n",
        "chunks = chunk_text(transcript_text, max_chunk_size=500)\n",
        "for i, chunk in enumerate(chunks, start=1):\n",
        "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
      ],
      "metadata": {
        "id": "bxWYeuco5Uxw",
        "outputId": "ea18f0db-8849-4494-b80d-7e682c0d9bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "I'm going to start with the presentation. Please everyone and welcome to our third quarter earnings presentation. Like always, the presentation will start with our CEO's comments, followed by my detailed explanation of the P&L. He will then offer his concluding remarks and we will open it up for questions. This quarter has been a record quarter for Santander, with profit up 12% compared to last quarter.\n",
            "\n",
            "Chunk 2:\n",
            "On the back of a strong customer base of 171 million, we continue to demonstrate resilience in our business model. Our efficiency has improved, and our balance sheets remain solid. We are on track to exceed our targets for the year. Now, let's move into a more detailed discussion on our financial performance, starting with revenue growth and cost management. We are confident that the strategies implemented will continue to deliver strong results in the upcoming quarters.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}