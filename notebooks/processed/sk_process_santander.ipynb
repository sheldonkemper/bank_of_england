{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/processed/sk_process_santander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjcWzjxiUs3i"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.1\n",
        "\n",
        "Description:\n",
        "    This notebook implements a system for processing and converting video transcripts into a single CSV file\n",
        "    for the Bank of England project. The workflow processes MP4 files stored in the raw data directory on Google Drive\n",
        "    by using a machine learning-based speech-to-text model (e.g., OpenAI’s Whisper) to transcribe the audio content into text.\n",
        "    Each transcript is appended as a record in the CSV file along with metadata—such as the year, quarter, and a duplicate indicator—\n",
        "    which are inferred from the video file name. This pipeline supports the ongoing integration of transcripts across multiple\n",
        "    quarters and years, facilitating further analysis and reporting within our data engineering infrastructure.\n",
        "\n",
        "===================================================\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "vmX59-ASY-gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install whisper (if not already installed)\n",
        "# !pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "pyCigYPknYBS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IyrdK8-KUs3l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import whisper\n",
        "import re\n",
        "import csv\n",
        "import whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6VD8ipZFGbLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb4342d-3279-44ad-d8e4-0865d53e6f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BOE Directory Contents: ['model', 'processed', 'raw']\n",
            "Raw Data Directory Contents: ['text']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Assuming 'BOE' folder is in 'MyDrive' and already shared\n",
        "BOE_path = '/content/drive/MyDrive/BOE/bank_of_england/data'\n",
        "\n",
        "# List the contents of the BOE directory\n",
        "print(\"BOE Directory Contents:\", os.listdir(BOE_path))\n",
        "\n",
        "# Define the raw data path (assuming your audio files are under raw/santander)\n",
        "raw_data_path = os.path.join(BOE_path, 'raw', 'santander')\n",
        "print(\"Raw Data Directory Contents:\", os.listdir(raw_data_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process All Downloaded MP4 Files"
      ],
      "metadata": {
        "id": "XtSShNPv1DDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import whisper\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_call_dates():\n",
        "    \"\"\"\n",
        "    Scrapes the Santander Financial and Economic Information page to build a mapping\n",
        "    of financial quarter to call date. The function assumes that each quarterly result\n",
        "    is contained in an element with class \"quarterly-result\" that has:\n",
        "        - an <h2> tag containing text like \"Q3 2023 Results\"\n",
        "        - a <span> tag with class \"call-date\" containing text such as \"Call Date: 25 October 2023\"\n",
        "    Adjust the selectors if the page structure is different.\n",
        "    Returns a dictionary mapping keys like \"2023 Q3\" to the call date.\n",
        "    \"\"\"\n",
        "    url = \"https://www.santander.com/en/shareholders-and-investors/financial-and-economic-information\"\n",
        "    call_date_mapping = {}\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching call dates:\", e)\n",
        "        return call_date_mapping\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Look for quarterly result blocks. (Adjust the tag/class as needed.)\n",
        "    results = soup.find_all(\"div\", class_=\"quarterly-result\")\n",
        "    for result in results:\n",
        "        header_elem = result.find(\"h2\")\n",
        "        if header_elem:\n",
        "            header_text = header_elem.get_text(strip=True)\n",
        "            # Expect header text in the format \"Q3 2023 Results\" (or similar)\n",
        "            match = re.search(r'(Q[1-4])\\s+(\\d{4})', header_text)\n",
        "            if match:\n",
        "                quarter = match.group(1)\n",
        "                year = match.group(2)\n",
        "                key = f\"{year} {quarter}\"\n",
        "                call_date_elem = result.find(\"span\", class_=\"call-date\")\n",
        "                if call_date_elem:\n",
        "                    call_date_text = call_date_elem.get_text(strip=True)\n",
        "                    # Remove any label (e.g., \"Call Date: \")\n",
        "                    call_date = call_date_text.replace(\"Call Date: \", \"\")\n",
        "                    call_date_mapping[key] = call_date\n",
        "    return call_date_mapping\n",
        "\n",
        "def parse_financial_quarter(filename):\n",
        "    \"\"\"\n",
        "    Given a filename (e.g., \"video_2023_Q3_1\"), extract and return a string like \"2023 Q3\".\n",
        "    If the pattern is not found, return \"Unknown\".\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d{4})_(Q[1-4])', filename)\n",
        "    if match:\n",
        "        year = match.group(1)\n",
        "        quarter = match.group(2)\n",
        "        return f\"{year} {quarter}\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Define directories – adjust these paths as needed.\n",
        "raw_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/raw/santander'\n",
        "processed_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/processed'\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Load the Whisper transcription model.\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Define the CSV file where all transcripts will be appended.\n",
        "all_transcripts_csv = os.path.join(processed_dir, \"all_transcripts.csv\")\n",
        "\n",
        "# Prepare a set to store already processed file names for duplicate checking.\n",
        "existing_files = set()\n",
        "if os.path.exists(all_transcripts_csv):\n",
        "    with open(all_transcripts_csv, \"r\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            existing_files.add(row[\"filename\"])\n",
        "\n",
        "# If the CSV file doesn't exist, create it with the desired header.\n",
        "if not os.path.exists(all_transcripts_csv):\n",
        "    with open(all_transcripts_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"filename\", \"management_discussion\", \"financial_quarter\", \"call_date\"])\n",
        "\n",
        "# Fetch the mapping of financial quarter to call date from Santander's page.\n",
        "call_date_mapping = get_call_dates()\n",
        "\n",
        "# Process each MP4 file in the raw directory.\n",
        "mp4_files = glob.glob(os.path.join(raw_dir, \"*.mp4\"))\n",
        "\n",
        "for mp4_file in mp4_files:\n",
        "    print(f\"\\nProcessing MP4 file: {mp4_file}\")\n",
        "    # Transcribe the video using Whisper.\n",
        "    result = model.transcribe(mp4_file)\n",
        "    transcript_text = result[\"text\"]\n",
        "\n",
        "    # Use the file's base name as an identifier.\n",
        "    base_name = os.path.splitext(os.path.basename(mp4_file))[0]\n",
        "\n",
        "    # Extract the financial quarter from the filename.\n",
        "    financial_quarter = parse_financial_quarter(base_name)\n",
        "    # Look up the call date from our mapping; default to \"Unknown\" if not found.\n",
        "    call_date = call_date_mapping.get(financial_quarter, \"Unknown\")\n",
        "\n",
        "    # Check for duplicates.\n",
        "    duplicate_flag = \"Yes\" if base_name in existing_files else \"No\"\n",
        "    existing_files.add(base_name)\n",
        "    if duplicate_flag == \"Yes\":\n",
        "        print(f\"Duplicate found for {base_name}.\")\n",
        "\n",
        "    # Append the new record to the CSV with headers: filename, management_discussion, financial_quarter, call_date.\n",
        "    with open(all_transcripts_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([base_name, transcript_text, financial_quarter, call_date])\n",
        "\n",
        "    print(f\"Transcript for '{base_name}' appended (financial_quarter: {financial_quarter}, call_date: {call_date}).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cTUjjFd0y7u",
        "outputId": "89185e3c-ccf5-46a5-a4a6-f6226a5fa630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2023_Q4_8.mp4\n",
            "Transcript for 'video_2023_Q4_8' appended (financial_quarter: 2023 Q4, call_date: Unknown).\n",
            "\n",
            "Processing MP4 file: /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2024_Q3_3.mp4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}