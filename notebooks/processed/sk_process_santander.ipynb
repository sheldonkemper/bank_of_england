{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/processed/sk_process_santander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjcWzjxiUs3i"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Op8v9iEUnRKF",
        "outputId": "58211399-953e-4fea-c554-814feccd8ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.1\\n\\nDescription:\\n    This notebook implements a system for transcribing and processing audio transcripts for the Bank of England project.\\n    The workflow downloads an audio file from a specified URL, applies a machine learning-based speech-to-text model\\n    (e.g., OpenAI’s Whisper) to convert the audio into text, and segments the resulting transcript into two sections:\\n    the Manager Presentation and the Question & Answer (Q&A) sections. Each section is subsequently exported into its\\n    own CSV file using Python libraries such as requests, regex, and CSV (or pandas). This pipeline builds on our existing\\n    data engineering infrastructure to facilitate efficient extraction, segmentation, and analysis of key project content.\\n\\n===================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.1\n",
        "\n",
        "Description:\n",
        "    This notebook implements a system for transcribing and processing audio transcripts for the Bank of England project.\n",
        "    The workflow downloads an audio file from a specified URL, applies a machine learning-based speech-to-text model\n",
        "    (e.g., OpenAI’s Whisper) to convert the audio into text, and segments the resulting transcript into two sections:\n",
        "    the Manager Presentation and the Question & Answer (Q&A) sections. Each section is subsequently exported into its\n",
        "    own CSV file using Python libraries such as requests, regex, and CSV (or pandas). This pipeline builds on our existing\n",
        "    data engineering infrastructure to facilitate efficient extraction, segmentation, and analysis of key project content.\n",
        "\n",
        "===================================================\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install whisper (if not already installed)\n",
        "# !pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "pyCigYPknYBS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IyrdK8-KUs3l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import whisper\n",
        "import re\n",
        "import csv\n",
        "import whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6VD8ipZFGbLk",
        "outputId": "8eb4342d-3279-44ad-d8e4-0865d53e6f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BOE Directory Contents: ['model', 'processed', 'raw']\n",
            "Raw Data Directory Contents: ['text']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Assuming 'BOE' folder is in 'MyDrive' and already shared\n",
        "BOE_path = '/content/drive/MyDrive/BOE/bank_of_england/data'\n",
        "\n",
        "# List the contents of the BOE directory\n",
        "print(\"BOE Directory Contents:\", os.listdir(BOE_path))\n",
        "\n",
        "# Define the raw data path (assuming your audio files are under raw/santander)\n",
        "raw_data_path = os.path.join(BOE_path, 'raw', 'santander')\n",
        "print(\"Raw Data Directory Contents:\", os.listdir(raw_data_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXYjweZnRKH"
      },
      "source": [
        "## Download Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_video_url(page_url):\n",
        "    \"\"\"\n",
        "    Given a webpage URL, this function scrapes the HTML to find the <video> tag,\n",
        "    then extracts the src attribute from the <source type=\"video/mp4\"> tag.\n",
        "    If the URL is protocol-relative (starting with \"//\"), it prepends \"https:\".\n",
        "    Returns the extracted video URL or None if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(page_url)\n",
        "        response.raise_for_status()  # raise exception if the request failed\n",
        "    except Exception as e:\n",
        "        print(f\"Error requesting page {page_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    video_tag = soup.find('video')\n",
        "    if not video_tag:\n",
        "        print(f\"No video tag found on {page_url}\")\n",
        "        return None\n",
        "\n",
        "    source_tag = video_tag.find('source', {'type': 'video/mp4'})\n",
        "    if not source_tag:\n",
        "        print(f\"MP4 source not found on {page_url}\")\n",
        "        return None\n",
        "\n",
        "    video_url = source_tag.get('src')\n",
        "    if video_url.startswith('//'):\n",
        "        video_url = 'https:' + video_url\n",
        "    return video_url\n",
        "\n",
        "def download_video(video_url, output_filename):\n",
        "    \"\"\"\n",
        "    Given a direct video URL (including any token/signature parameters),\n",
        "    this function uses ffmpeg to download the video without re-encoding.\n",
        "    \"\"\"\n",
        "    if not video_url:\n",
        "        print(\"No video URL provided for download.\")\n",
        "        return\n",
        "\n",
        "    # Build the ffmpeg command\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", video_url,\n",
        "        \"-c\", \"copy\",\n",
        "        output_filename\n",
        "    ]\n",
        "\n",
        "    print(f\"Running command: {' '.join(command)}\")\n",
        "    try:\n",
        "        subprocess.run(command, check=True)\n",
        "        print(f\"Download complete: {output_filename}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error during download: {e}\")\n",
        "\n",
        "def process_video_page(page_url, output_filename):\n",
        "    \"\"\"\n",
        "    Combines the steps:\n",
        "      1. Scrapes the provided page URL to extract the video URL.\n",
        "      2. Downloads the video using ffmpeg.\n",
        "    \"\"\"\n",
        "    print(f\"Processing page: {page_url}\")\n",
        "    video_url = extract_video_url(page_url)\n",
        "    if video_url:\n",
        "        print(f\"Extracted video URL: {video_url}\")\n",
        "        download_video(video_url, output_filename)\n",
        "    else:\n",
        "        print(f\"Could not extract video URL from {page_url}\")\n",
        "\n",
        "# Define a list of video pages along with their associated year and quarter.\n",
        "video_pages = [\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/5SRc1sBhrbQShIGv\", \"year\": \"2024\", \"quarter\": \"Q1\"},\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/mWmPFSlzp8bbFaY6\", \"year\": \"2024\", \"quarter\": \"Q2\"},\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/FDze0qyNaPlaLMGd\", \"year\": \"2024\", \"quarter\": \"Q3\"},\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/CZXmizS4fhXXBe1Y\", \"year\": \"2024\", \"quarter\": \"Q4\"},\n",
        "\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/yVQDX2NYLvKCfejI\", \"year\": \"2023\", \"quarter\": \"Q1\"},\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/GSHpNVJBNGIlmjoC\", \"year\": \"2023\", \"quarter\": \"Q2\"},\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/fmjK8h3yPneBlW6H\", \"year\": \"2023\", \"quarter\": \"Q3\"},\n",
        "    {\"url\": \"https://wavedw01.santandergroup.net/content/aon6bYIQV5etHkKj\", \"year\": \"2023\", \"quarter\": \"Q4\"},\n",
        "\n",
        "]\n",
        "\n",
        "# Set the output directory to your raw/santander directory on Google Drive.\n",
        "output_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/raw/santander'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each video page and include year and quarter in the output filename.\n",
        "for idx, video_info in enumerate(video_pages, start=1):\n",
        "    page_url = video_info[\"url\"]\n",
        "    year = video_info[\"year\"]\n",
        "    quarter = video_info[\"quarter\"]\n",
        "    # Construct the output filename using the metadata.\n",
        "    output_file = os.path.join(output_dir, f\"video_{year}_{quarter}_{idx}.mp4\")\n",
        "    process_video_page(page_url, output_file)\n"
      ],
      "metadata": {
        "id": "1FYVCy6UtbwN",
        "outputId": "14247d26-ed2c-42c7-f97d-79e8a06f8735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page: https://wavedw01.santandergroup.net/content/5SRc1sBhrbQShIGv\n",
            "Extracted video URL: https://waves3.santandergroup.net/waves3/EXTERNOS/sWEB/media/5SRc1sBhrbQShIGv/st3_270_EN01_900p.mp4?AWSAccessKeyId=Z3NuZXR3YXZl&Expires=1739196850&Signature=%2F12zEH3XYyHzhdFuALcSNABYuH4%3D\n",
            "Running command: ffmpeg -i https://waves3.santandergroup.net/waves3/EXTERNOS/sWEB/media/5SRc1sBhrbQShIGv/st3_270_EN01_900p.mp4?AWSAccessKeyId=Z3NuZXR3YXZl&Expires=1739196850&Signature=%2F12zEH3XYyHzhdFuALcSNABYuH4%3D -c copy /content/drive/MyDrive/BOE/bank_of_england/data/raw/santander/video_2024_Q1_1.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process All Downloaded MP4 Files"
      ],
      "metadata": {
        "id": "XtSShNPv1DDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories – adjust these paths as needed.\n",
        "raw_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/raw/santander'\n",
        "processed_dir = '/content/drive/MyDrive/BOE/bank_of_england/data/processed'\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Load the Whisper transcription model (choose an appropriate size)\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Define the CSV file where all transcripts will be appended.\n",
        "all_transcripts_csv = os.path.join(processed_dir, \"santander_all_transcripts.csv\")\n",
        "\n",
        "# Prepare a set to store already processed file names to check for duplicates.\n",
        "existing_files = set()\n",
        "if os.path.exists(all_transcripts_csv):\n",
        "    with open(all_transcripts_csv, \"r\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            existing_files.add(row[\"file_name\"])\n",
        "\n",
        "# If the CSV file doesn't exist, create it and write a header row with the new fields.\n",
        "if not os.path.exists(all_transcripts_csv):\n",
        "    with open(all_transcripts_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"file_name\", \"year\", \"quarter\", \"transcript\", \"duplicate\"])\n",
        "\n",
        "# Process each MP4 file in the raw directory.\n",
        "mp4_files = glob.glob(os.path.join(raw_dir, \"*.mp4\"))\n",
        "\n",
        "for mp4_file in mp4_files:\n",
        "    print(f\"Processing MP4 file: {mp4_file}\")\n",
        "    # Transcribe the video file using Whisper.\n",
        "    result = model.transcribe(mp4_file)\n",
        "    transcript_text = result[\"text\"]\n",
        "\n",
        "    # Use the file's base name as an identifier.\n",
        "    base_name = os.path.splitext(os.path.basename(mp4_file))[0]\n",
        "\n",
        "    # Attempt to extract the year and quarter from the base_name.\n",
        "    # Expecting a pattern like \"video_2023_Q3_1\" in the file name.\n",
        "    match = re.search(r'(\\d{4})_(Q[1-4])', base_name)\n",
        "    if match:\n",
        "        year = match.group(1)\n",
        "        quarter = match.group(2)\n",
        "    else:\n",
        "        year = \"Unknown\"\n",
        "        quarter = \"Unknown\"\n",
        "\n",
        "    # Check if this file has already been processed.\n",
        "    if base_name in existing_files:\n",
        "        duplicate_flag = \"Yes\"\n",
        "    else:\n",
        "        duplicate_flag = \"No\"\n",
        "        existing_files.add(base_name)\n",
        "\n",
        "    # Append the new transcript record to the CSV with the additional fields.\n",
        "    with open(all_transcripts_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([base_name, year, quarter, transcript_text, duplicate_flag])\n",
        "\n",
        "    print(f\"Transcript for '{base_name}' appended to {all_transcripts_csv} (year: {year}, quarter: {quarter}, duplicate: {duplicate_flag})\")\n"
      ],
      "metadata": {
        "id": "1cTUjjFd0y7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}