{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2C9SSNHREALD",
        "outputId": "0154b08d-5804-4312-9d58-3d412444f5cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.0\\n\\nDescription:\\n    This notebook is dedicated to the data engineering functions for the Bank of England Employer Project.\\n    It includes code for mounting Google Drive, reading raw PDF files (e.g., earnings call transcripts),\\n    and performing text extraction and cleaning using pdfplumber and regular expressions.\\n\\nDependencies:\\n    - pdfplumber\\n    - re\\n    - google.colab (for mounting Google Drive)\\n    - os\\n\\n===================================================\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.0\n",
        "\n",
        "Description:\n",
        "    This notebook is dedicated to the data engineering functions for the Bank of England Employer Project.\n",
        "    It includes code for mounting Google Drive, reading raw PDF files (e.g., earnings call transcripts),\n",
        "    and performing text extraction and cleaning using pdfplumber and regular expressions.\n",
        "\n",
        "Dependencies:\n",
        "    - pdfplumber\n",
        "    - re\n",
        "    - google.colab (for mounting Google Drive)\n",
        "    - os\n",
        "\n",
        "===================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XmhbIl8CKIy"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VD8ipZFGbLk",
        "outputId": "095f4f81-98cb-4af4-ebf7-407830094314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "['model', 'cleansed', 'processed', 'raw']\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Assuming 'BOE' folder is in 'MyDrive' and already shared\n",
        "BOE_path = '/content/drive/MyDrive/BOE/bank_of_england/data'\n",
        "\n",
        "# Now you (and others with access) can work with files in this directory\n",
        "# For example, you can list the contents:\n",
        "print(os.listdir(BOE_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# 4. Define helper functions for processing\n",
        "# -------------------------------\n",
        "def clean_transcript(text):\n",
        "    \"\"\"Cleans the raw transcript text by removing excessive whitespace, page markers, and disclaimers.\"\"\"\n",
        "    text = re.sub(r'\\n\\s*\\.{10,}\\s*\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
        "    text = re.sub(r'On page \\d+', '', text)\n",
        "    text = re.sub(r'Starting on page \\d+', '', text)\n",
        "    text = re.sub(r'\\.\\s*,', '.', text)\n",
        "    text = text.replace('%. ,', '%.')\n",
        "    text = re.sub(r'\\s+\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "    if \"Disclaimer\" in text:\n",
        "        text = text.split(\"Disclaimer\")[0].strip()\n",
        "    return text\n",
        "\n",
        "def extract_metadata(text):\n",
        "    \"\"\"Extracts the financial quarter (e.g., '1Q24') and call date (e.g., 'April 12, 2024') from the transcript text.\"\"\"\n",
        "    quarter_match = re.search(r'(\\dQ\\s*\\d{2})', text)\n",
        "    financial_quarter = quarter_match.group(1).replace(\" \", \"\") if quarter_match else None\n",
        "    date_match = re.search(r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})', text)\n",
        "    call_date = date_match.group(1) if date_match else None\n",
        "    return financial_quarter, call_date\n",
        "\n",
        "def split_sections(transcript):\n",
        "    \"\"\"\n",
        "    Splits the transcript into Management Discussion and Q&A sections.\n",
        "    It looks for markers such as \"QUESTION AND ANSWER\" or \"ANALYST Q&A\" (case-insensitive).\n",
        "    Returns a tuple: (management_discussion, qa_section)\n",
        "    \"\"\"\n",
        "    qa_marker = re.search(r'(?i)(QUESTION\\s+AND\\s+ANSWER|ANALYST\\s+Q&A)', transcript)\n",
        "    if qa_marker:\n",
        "        management_discussion = transcript[:qa_marker.start()].strip()\n",
        "        qa_section = transcript[qa_marker.start():].strip()\n",
        "    else:\n",
        "        management_discussion = transcript\n",
        "        qa_section = \"\"\n",
        "    return management_discussion, qa_section\n",
        "\n",
        "def parse_qa_section(qa_text, job_role_word_threshold=10):\n",
        "    \"\"\"\n",
        "    Parses the Q&A section into a list of dictionaries.\n",
        "    Each dictionary contains:\n",
        "      - 'speaker'\n",
        "      - 'marker' (e.g., 'Q' or 'A' if applicable)\n",
        "      - 'job_title'\n",
        "      - 'utterance'\n",
        "    \"\"\"\n",
        "    entries = []\n",
        "    current_entry = None\n",
        "    lines = qa_text.split('\\n')\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        # Match headers like \"Name Q\" or \"Name A\"\n",
        "        m1 = re.match(r'^(?P<speaker>.+?)\\s+(?P<marker>[QA])$', line)\n",
        "        if m1:\n",
        "            if current_entry is not None:\n",
        "                entries.append(current_entry)\n",
        "            current_entry = {\n",
        "                'speaker': m1.group('speaker').strip(),\n",
        "                'marker': m1.group('marker'),\n",
        "                'job_title': \"\",\n",
        "                'utterance': \"\"\n",
        "            }\n",
        "        else:\n",
        "            # Match headers in \"Speaker: Utterance\" format\n",
        "            m2 = re.match(r'^(?P<speaker>[^:]+):\\s*(?P<utterance>.*)$', line)\n",
        "            if m2:\n",
        "                if current_entry is not None:\n",
        "                    entries.append(current_entry)\n",
        "                current_entry = {\n",
        "                    'speaker': m2.group('speaker').strip(),\n",
        "                    'marker': None,\n",
        "                    'job_title': \"\",\n",
        "                    'utterance': m2.group('utterance').strip()\n",
        "                }\n",
        "            else:\n",
        "                # For continuation lines, check if the first line might be a job title\n",
        "                if current_entry is not None:\n",
        "                    if not current_entry['job_title'] and not current_entry['utterance']:\n",
        "                        words = line.split()\n",
        "                        if len(words) < job_role_word_threshold and ',' in line:\n",
        "                            current_entry['job_title'] = line\n",
        "                            continue\n",
        "                    # Append the line to the current entry's utterance\n",
        "                    if current_entry['utterance']:\n",
        "                        current_entry['utterance'] += \" \" + line\n",
        "                    else:\n",
        "                        current_entry['utterance'] = line\n",
        "                else:\n",
        "                    current_entry = {'speaker': 'Unknown', 'marker': None, 'job_title': \"\", 'utterance': line}\n",
        "    if current_entry is not None:\n",
        "        entries.append(current_entry)\n",
        "    return entries\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Process each PDF in the raw folder and aggregate results\n",
        "# -------------------------------\n",
        "all_qa_entries = []  # To store parsed Q&A entries\n",
        "all_md_entries = []  # To store Management Discussion entries\n",
        "\n",
        "for filename in os.listdir(raw_dir):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(raw_dir, filename)\n",
        "        print(\"Processing file:\", file_path)\n",
        "        \n",
        "        # Extract text from PDF\n",
        "        transcript_text = \"\"\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    transcript_text += page_text + \"\\n\"\n",
        "        print(\"Extracted text preview for\", filename, \":\", transcript_text[:1000])\n",
        "        \n",
        "        # Clean the transcript text\n",
        "        transcript_clean = clean_transcript(transcript_text)\n",
        "        print(\"Cleaned text preview for\", filename, \":\", transcript_clean[:1000])\n",
        "        \n",
        "        # Extract metadata: financial quarter and call date\n",
        "        financial_quarter, call_date = extract_metadata(transcript_clean)\n",
        "        print(\"Extracted Financial Quarter:\", financial_quarter)\n",
        "        print(\"Extracted Call Date:\", call_date)\n",
        "        \n",
        "        # Split into Management Discussion and Q&A sections\n",
        "        management_discussion, qa_section = split_sections(transcript_clean)\n",
        "        # Remove any header text from the Q&A section\n",
        "        qa_section = re.sub(r'(?i)^(QUESTION\\s+AND\\s+ANSWER|ANALYST\\s+Q&A)\\s*', '', qa_section, count=1).strip()\n",
        "        \n",
        "        # Append Management Discussion entry (with metadata)\n",
        "        md_entry = {\n",
        "            'filename': filename,\n",
        "            'management_discussion': management_discussion,\n",
        "            'financial_quarter': financial_quarter,\n",
        "            'call_date': call_date\n",
        "        }\n",
        "        all_md_entries.append(md_entry)\n",
        "        \n",
        "        # Parse the Q&A section and add metadata to each entry\n",
        "        qa_entries = parse_qa_section(qa_section)\n",
        "        for entry in qa_entries:\n",
        "            entry['filename'] = filename\n",
        "            entry['financial_quarter'] = financial_quarter\n",
        "            entry['call_date'] = call_date\n",
        "        all_qa_entries.extend(qa_entries)\n",
        "        \n",
        "        print(\"Processed file:\", filename)\n",
        "\n",
        "# Convert aggregated lists to DataFrames\n",
        "df_qa_all = pd.DataFrame(all_qa_entries)\n",
        "df_md_all = pd.DataFrame(all_md_entries)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Format 'call_date' as datetime and sort descending by call_date\n",
        "# -------------------------------\n",
        "df_qa_all['call_date'] = pd.to_datetime(df_qa_all['call_date'], format='%B %d, %Y', errors='coerce')\n",
        "df_md_all['call_date'] = pd.to_datetime(df_md_all['call_date'], format='%B %d, %Y', errors='coerce')\n",
        "\n",
        "df_qa_all = df_qa_all.sort_values(by='call_date', ascending=False)\n",
        "df_md_all = df_md_all.sort_values(by='call_date', ascending=False)\n",
        "\n",
        "print(\"\\nCombined Parsed Q&A Section Preview (Sorted):\")\n",
        "print(df_qa_all.head(10))\n",
        "\n",
        "print(\"\\nCombined Management Discussion DataFrame Preview (Sorted):\")\n",
        "print(df_md_all.head())\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Save the DataFrames as CSV files\n",
        "# -------------------------------\n",
        "qa_csv_path = os.path.join(processed_dir, \"ubs_qa_section.csv\")\n",
        "md_csv_path = os.path.join(processed_dir, \"ubs_management_discussion.csv\")\n",
        "\n",
        "df_qa_all.to_csv(qa_csv_path, index=False)\n",
        "print(\"\\nQ&A DataFrame saved to:\", qa_csv_path)\n",
        "\n",
        "df_md_all.to_csv(md_csv_path, index=False)\n",
        "print(\"Management Discussion DataFrame saved to:\", md_csv_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
