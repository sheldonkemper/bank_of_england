{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "li72cWGiO768"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/processed/am_data_extraction_JPM_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries**"
      ],
      "metadata": {
        "id": "li72cWGiO768"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyPDF2 > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTWezdyKO_LR",
        "outputId": "12f95e34-2c6d-407e-e9b2-0bbc43105ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai==0.28 > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raLjASy4UCVj",
        "outputId": "95c7413c-5e1b-4f73-8163-2a767c3d8db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRE38UUptyhc",
        "outputId": "bd6987df-03ea-4dc5-9b6d-ab567defbb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import PyPDF2\n",
        "import openai\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "rZ5Ciw8CPNp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "openai_api_key = os.getenv(\"Openai_key\")"
      ],
      "metadata": {
        "id": "mpgDSRLXt6l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Data extraction**"
      ],
      "metadata": {
        "id": "n4-5udplO1fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCB3Tpqo46ef"
      },
      "outputs": [],
      "source": [
        "def extract_sections_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Reads a PDF file and extracts text from all pages.\n",
        "    Then segregates the text into two sections based on markers:\n",
        "    - 'Management Discussion' section (md_section)\n",
        "    - 'Question and Answer' section (QNA_section)\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The file path to the PDF.\n",
        "\n",
        "    Returns:\n",
        "        md_section (str): Extracted text for the Management Discussion section.\n",
        "        QNA_section (str): Extracted text for the Question and Answer section.\n",
        "    \"\"\"\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        # Create a PDF reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "\n",
        "        # Extract text from all pages\n",
        "        full_text = \"\"\n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            full_text += page_text + \"\\n\"\n",
        "\n",
        "    # Optionally, clean up extra whitespace/newlines\n",
        "    full_text = re.sub(r'\\n+', '\\n', full_text)\n",
        "\n",
        "    # Convert text to lowercase for marker searching (you can retain original text for extraction)\n",
        "    text_lower = full_text.lower()\n",
        "\n",
        "    # Define markers for splitting sections\n",
        "    # Adjust these markers if your PDF uses slightly different headings\n",
        "    md_marker = \"management discussion\"\n",
        "    qna_marker = \"question and answer\"\n",
        "\n",
        "    # Find the starting index of each section in the text\n",
        "    md_start = text_lower.find(md_marker)\n",
        "    qna_start = text_lower.find(qna_marker)\n",
        "\n",
        "    if md_start == -1:\n",
        "        raise ValueError(\"Management Discussion section marker not found in the PDF.\")\n",
        "    if qna_start == -1:\n",
        "        raise ValueError(\"Question and Answer section marker not found in the PDF.\")\n",
        "\n",
        "    # Extract the sections based on the identified markers\n",
        "    # We assume that the Management Discussion section comes first\n",
        "    md_section = full_text[md_start:qna_start].strip()\n",
        "    QNA_section = full_text[qna_start:].strip()\n",
        "\n",
        "    return md_section, QNA_section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Specify the path to your PDF file\n",
        "    pdf_path = \"/content/4q24-earnings-transcript.pdf\"\n",
        "\n",
        "    # Extract the sections\n",
        "    md_section, QNA_section = extract_sections_from_pdf(pdf_path)\n",
        "\n",
        "    # Optionally, convert the extracted text into datasets (e.g., as a list of lines)\n",
        "    md_dataset = md_section.split(\"\\n\")\n",
        "    qna_dataset = QNA_section.split(\"\\n\")"
      ],
      "metadata": {
        "id": "IXqiSFQyPSsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "a9efa43b-f79c-4a6c-ee1f-475a32c02e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/4q24-earnings-transcript.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f6d419ca65df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract the sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmd_section\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQNA_section\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sections_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Optionally, convert the extracted text into datasets (e.g., as a list of lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-40df352f578b>\u001b[0m in \u001b[0;36mextract_sections_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Open the PDF file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpdf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Create a PDF reader object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpdf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/4q24-earnings-transcript.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Display a sample from each dataset\n",
        "    print(\"=== Management Discussion Section Sample ===\")\n",
        "    for line in md_dataset[:5]:\n",
        "        print(line)"
      ],
      "metadata": {
        "id": "_NwKiq6WO1Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"\\n=== Question and Answer Section Sample ===\")\n",
        "    for line in qna_dataset[:5]:\n",
        "        print(line)"
      ],
      "metadata": {
        "id": "_RB2aQtUO-WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert md_dataset to DataFrame before saving\n",
        "md_df = pd.DataFrame(md_dataset, columns=['Text'])\n",
        "md_df.to_csv(\"MD_1Q23.csv\", index=False)"
      ],
      "metadata": {
        "id": "YGjsbQ_PPCY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_dataset"
      ],
      "metadata": {
        "id": "eSQqJBjHeOq_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "YxCakJVuniyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = userdata.get('Openai_key')"
      ],
      "metadata": {
        "id": "9aeN-1z7u6Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_qna_dataset_to_text(qna_dataset):\n",
        "    \"\"\"\n",
        "    Converts a list of Q&A pair strings into a single text string.\n",
        "\n",
        "    Parameters:\n",
        "        qna_dataset (list): List of strings, where each string represents a Q&A pair.\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all Q&A pairs separated by newlines.\n",
        "    \"\"\"\n",
        "    # Join the list elements using a newline separator\n",
        "    text = \"\\n\".join(qna_dataset)\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Convert the qna_dataset to a single text string\n",
        "    qna_text = convert_qna_dataset_to_text(qna_dataset)\n",
        "    print(\"Converted Q&A Text:\")\n",
        "    print(qna_text)"
      ],
      "metadata": {
        "id": "ToFAb8GBwbII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_operator_segments(qna_dataset):\n",
        "    \"\"\"\n",
        "    Processes the qna_dataset to extract text between consecutive occurrences of the word Operator.\n",
        "\n",
        "    Steps:\n",
        "      1. Check if the word Operator exists in the text.\n",
        "      2. Find all occurrences of Operator.\n",
        "      3. Extract text between each consecutive pair of Operator occurrences.\n",
        "      4. Create a DataFrame with two columns:\n",
        "         - 'Question_Number': a count starting at 1.\n",
        "         - 'Text': the text between consecutive occurrences.\n",
        "\n",
        "    Parameters:\n",
        "        qna_dataset (str): The input text containing multiple occurrences of Operator.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the extracted segments.\n",
        "    \"\"\"\n",
        "    # Step 1: Check if 'Operator' exists in the dataset.\n",
        "    if \"Operator\" not in qna_dataset:\n",
        "        print(\"The word 'Operator' is not found in the dataset.\")\n",
        "        return pd.DataFrame(columns=[\"Question_Number\", \"Text\"])\n",
        "\n",
        "    # Step 2: Find all occurrences of 'Operator'\n",
        "    matches = list(re.finditer(r\"Operator\", qna_dataset))\n",
        "\n",
        "    # Check if there are at least two occurrences to form a segment.\n",
        "    if len(matches) < 2:\n",
        "        print(\"Not enough occurrences of 'Operator' to extract segments.\")\n",
        "        return pd.DataFrame(columns=[\"Question_Number\", \"Text\"])\n",
        "\n",
        "    segments = []\n",
        "    # Step 3: Extract text between consecutive occurrences.\n",
        "    for i in range(len(matches) - 1):\n",
        "        # Get the end index of the current occurrence and start index of the next occurrence.\n",
        "        start = matches[i].end()\n",
        "        end = matches[i+1].start()\n",
        "        segment_text = qna_dataset[start:end].strip()\n",
        "        segments.append(segment_text)\n",
        "\n",
        "    # Step 4: Create the DataFrame.\n",
        "    df = pd.DataFrame({\n",
        "        \"Question_Number\": list(range(1, len(segments) + 1)),\n",
        "        \"Text\": segments\n",
        "    })\n",
        "    return df"
      ],
      "metadata": {
        "id": "OJA_PMHnvL9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    result_df = extract_operator_segments(qna_text)\n",
        "    print(result_df)"
      ],
      "metadata": {
        "id": "UKwZVnbLvcqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "metadata": {
        "id": "ImaIvRsxw3sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.to_csv(\"QNA_output.csv\", index=False)"
      ],
      "metadata": {
        "id": "gvArc4R1W7Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info(text):\n",
        "    \"\"\"\n",
        "    This function sends a prompt to the GPT-4 Turbo model asking it to extract\n",
        "    specific fields from the provided text. The model is expected to return a JSON\n",
        "    with the following keys:\n",
        "    - Name of the first person\n",
        "    - Role of the first person\n",
        "    - All text that the first person said\n",
        "    - Name of the second person\n",
        "    - Role of the second person\n",
        "    - All text that the second person said\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    The text is conversation between two people. Please Extract the following information from the text below:\n",
        "\n",
        "    - Name of the first person\n",
        "    - Role of the first person\n",
        "    - All text that the first person said\n",
        "    - Name of the second person\n",
        "    - Role of the second person\n",
        "    - All text that the second person said\n",
        "\n",
        "\n",
        "    The output should have all text both the persons said in the text.\n",
        "\n",
        "    Provide the response in JSON format with keys exactly as:\n",
        "    \"Name of the first person\", \"Role of the first person\", \"All text that the first person said\", \"Name of the second person\", \"Role of the second person\", \"All text that the second person said\".\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts structured information from text.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"}, # Set output to JSON format\n",
        "            max_tokens=4000,  # Adjust tokens based on your text size\n",
        "            temperature=0  # Keep it deterministic\n",
        "        )\n",
        "        content = response['choices'][0]['message']['content']\n",
        "        # Attempt to parse the JSON response\n",
        "        result = json.loads(content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {e}\")\n",
        "        # Return a dictionary with None values in case of error\n",
        "        result = {\n",
        "            \"Name of the first person\": None,\n",
        "            \"Role of the first person\": None,\n",
        "            \"All text that the first person said\": None,\n",
        "            \"Name of the second person\": None,\n",
        "            \"Role of the second person\": None,\n",
        "            \"All text that the second person said\": None\n",
        "        }\n",
        "    return result"
      ],
      "metadata": {
        "id": "pWUHt-h4UmuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store processed results\n",
        "processed_results = []\n",
        "\n",
        "# Loop through each row in result_df\n",
        "for idx, row in result_df.iterrows():\n",
        "    text = row['Text']\n",
        "    info = extract_info(text)\n",
        "    processed_results.append(info)\n",
        "    # Optional: sleep to respect rate limits (adjust the delay as needed)\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "processed_df = pd.DataFrame(processed_results)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "processed_df.head()"
      ],
      "metadata": {
        "id": "dvOHvrCrU32m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_df.info()"
      ],
      "metadata": {
        "id": "byieCzzbWS4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_df.to_csv(\"QNA_4Q24.csv\", index=False)"
      ],
      "metadata": {
        "id": "RYGVNfznWpQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}