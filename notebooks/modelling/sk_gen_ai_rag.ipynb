{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This module implements a Retrieval Augmented Generation (RAG) engine for analyzing bank\n",
        "    quarterly earnings call transcripts (PDF format) stored on Google Drive. It leverages\n",
        "    Langchain and Hugging Face Transformers for document loading, intelligent document chunking,\n",
        "    embedding, and question answering.\n",
        "\n",
        "    Key features:\n",
        "    - Configures an LLM pipeline using GPT-3.5-turbo for interactive Q&A.\n",
        "    - Utilizes sentence-transformer embeddings for semantic vectorization of documents.\n",
        "    - Loads and processes PDF documents from specified Google Drive directories.\n",
        "    - Intelligently chunks documents using RecursiveCharacterTextSplitter with an adjustable\n",
        "      token threshold to avoid splitting when documents already fit within a larger context window.\n",
        "    - Builds and persists two Chroma vector store indexes (raw and summary) in a dedicated folder\n",
        "      (\"gpt_chatbot\"), which also stores the master CSV log file.\n",
        "    - Supports reusing existing persisted indexes to avoid unnecessary re-indexing.\n",
        "    - Implements context retrieval with token truncation to ensure LLM input limits are maintained.\n",
        "    - Manages conversation history using ConversationBufferWindowMemory for interactive sessions.\n",
        "    - Provides both batch and interactive modes for prompt-based querying and logging of Q&A sessions.\n",
        "    - Integrates a ROUGE evaluation method to quantitatively measure summary quality.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "5b726a05-26ab-42e3-89a5-059807a56ef6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This module implements a Retrieval Augmented Generation (RAG) engine for analyzing bank\\n    quarterly earnings call transcripts (PDF format) stored on Google Drive. It leverages\\n    Langchain and Hugging Face Transformers for document loading, intelligent document chunking,\\n    embedding, and question answering.\\n\\n    Key features:\\n    - Configures an LLM pipeline using GPT-3.5-turbo for interactive Q&A.\\n    - Utilizes sentence-transformer embeddings for semantic vectorization of documents.\\n    - Loads and processes PDF documents from specified Google Drive directories.\\n    - Intelligently chunks documents using RecursiveCharacterTextSplitter with an adjustable\\n      token threshold to avoid splitting when documents already fit within a larger context window.\\n    - Builds and persists two Chroma vector store indexes (raw and summary) in a dedicated folder\\n      (\"gpt_chatbot\"), which also stores the master CSV log file.\\n    - Supports reusing existing persisted indexes to avoid unnecessary re-indexing.\\n    - Implements context retrieval with token truncation to ensure LLM input limits are maintained.\\n    - Manages conversation history using ConversationBufferWindowMemory for interactive sessions.\\n    - Provides both batch and interactive modes for prompt-based querying and logging of Q&A sessions.\\n    - Integrates a ROUGE evaluation method to quantitatively measure summary quality.\\n===================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai chromadb sentence-transformers pypdf datasets rouge-score  > /dev/null 2>&1\n",
        "!pip install --upgrade langchain_community   > /dev/null 2>&1\n",
        "!pip install -U langchain-huggingface  > /dev/null 2>&1\n",
        "!pip install --upgrade openai > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from rouge_score import rouge_scorer\n",
        "import shutil\n",
        "from transformers import AutoTokenizer\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')# Replace with your actual token"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa190f5-518b-490d-dd34-bcbebebd5a70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings in production mode if not in DEV_MODE\n",
        "DEV_MODE = False\n",
        "if not DEV_MODE:\n",
        "    warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SI-Sa_fcKhpt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    \"pdf_folders\": [\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"gpt-3.5-turbo\",  # Using GPT-3.5-turbo\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 3000,  # Increased to avoid splitting when not needed\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "# Use a dedicated folder \"gpt_chatbot\" under the persist directory for saving vector stores and CSV logs.\n",
        "GPT_FOLDER = os.path.join(CONFIG[\"persist_directory\"], \"gpt_chatbot\")\n",
        "os.makedirs(GPT_FOLDER, exist_ok=True)\n",
        "\n",
        "MASTER_AGENT_PROMPT = (\n",
        "    \"You are a highly accurate and detail-oriented assistant specialized in analyzing bank earnings call transcripts. \"\n",
        "    \"Only rely on the specific transcript context provided. Your final answer must be a bullet-point list.\\n\\n\"\n",
        "    \"Please follow this exact format:\\n\"\n",
        "    \"1. Thought: Briefly explain which parts of the transcript are most relevant.\\n\"\n",
        "    \"2. Action: Specify which tool to use (e.g., JP_Morgan_RAG or UBS_RAG) by writing: JP_Morgan_RAG(\\\"<your query>\\\").\\n\"\n",
        "    \"3. Observation: Summarize the key information from the retrieved context in a few words.\\n\"\n",
        "    \"4. Final Answer: Present a concise bullet-point list of the key sentiments and takeaways.\\n\\n\"\n",
        "    \"Now, answer the following question:\\n\"\n",
        "    \"{input}\\n\"\n",
        "    \"Begin!\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- RAGChatbot Class ---\n",
        "class RAGChatbot:\n",
        "    \"\"\"\n",
        "    A RAG chatbot that ingests PDF earnings call transcripts, builds vector stores,\n",
        "    and uses a ConversationalRetrievalChain for Q&A.\n",
        "\n",
        "    Two Chroma vector stores (\"raw\" and \"summary\") are created and stored under the\n",
        "    dedicated folder \"gpt_chatbot\". Additionally, methods are provided to generate\n",
        "    summaries for evaluation and compare these summaries against manually curated\n",
        "    ground truth using ROUGE metrics.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, bank: str):\n",
        "        self.config = config\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        self.pdf_folders = config[\"pdf_folders\"]\n",
        "        self.persist_directory = config[\"persist_directory\"]\n",
        "        self.max_length = config[\"max_length\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.chunk_size = config[\"chunk_size\"]\n",
        "        self.chunk_overlap = config[\"chunk_overlap\"]\n",
        "        self.chunk_threshold = config[\"chunk_threshold\"]\n",
        "        self.memory_window_k = config[\"memory_window_k\"]\n",
        "        self.retriever_search_k = config[\"retriever_search_k\"]\n",
        "        self.bank = bank\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_embeddings()\n",
        "        from transformers import AutoTokenizer\n",
        "        self.split_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        self._load_documents()\n",
        "        self._build_vector_store()\n",
        "        self._build_summary_index()\n",
        "        self._setup_retrieval_chain()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=self.config[\"llm_model_name\"],\n",
        "            temperature=self.config[\"temperature\"],\n",
        "            max_tokens=self.max_length,\n",
        "            openai_api_key=self.api_key,\n",
        "            model_kwargs={\"top_p\": self.config[\"top_p\"]}\n",
        "        )\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        emb_model = self.config[\"embedding_model_name\"]\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=emb_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank = os.path.basename(folder).lower()\n",
        "            files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            for file in files:\n",
        "                path = os.path.join(folder, file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank\n",
        "                        doc.metadata[\"source_pdf\"] = file\n",
        "                        doc.metadata[\"year_quarter\"] = \"Unknown\"\n",
        "                    self.documents.extend(docs)\n",
        "                    if DEV_MODE:\n",
        "                        print(f\"Loaded: {file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    if DEV_MODE:\n",
        "                        print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    def _chunk_document(self, doc: Document) -> list[Document]:\n",
        "        tokens = self.split_tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self._remove_duplicates(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    @staticmethod\n",
        "    def _remove_duplicates(chunks: list[Document]) -> list[Document]:\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def _build_vector_store(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings, persist_directory=GPT_FOLDER)\n",
        "        if DEV_MODE:\n",
        "            print(f\"Built raw vector store with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _build_summary_index(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings,\n",
        "            persist_directory=os.path.join(GPT_FOLDER, \"summaries\"))\n",
        "        if DEV_MODE:\n",
        "            print(f\"Built summary vector index with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _setup_retrieval_chain(self):\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=self.memory_window_k, memory_key=\"chat_history\", return_messages=True)\n",
        "        self.retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_db.as_retriever(\n",
        "                search_kwargs={\"k\": self.retriever_search_k, \"filter\": {\"bank\": self.bank}}\n",
        "            ),\n",
        "            memory=memory,\n",
        "            verbose=DEV_MODE\n",
        "        )\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        response = self.retrieval_chain({\"question\": query})\n",
        "        return response.get(\"answer\", \"\").strip()\n",
        "\n",
        "    def summarize_documents(self) -> dict:\n",
        "        \"\"\"\n",
        "        Generates summaries for each document/chunk using the LLM.\n",
        "        Returns a dictionary with document identifiers as keys and generated summaries as values.\n",
        "        \"\"\"\n",
        "        generated = {}\n",
        "        from langchain.schema import HumanMessage\n",
        "        for doc in self.documents:\n",
        "            prompt = (\n",
        "                \"Summarize the following transcript in bullet points, highlighting key financial metrics, sentiment, and strategic insights:\\n\\n\"\n",
        "                + doc.page_content\n",
        "            )\n",
        "            message = HumanMessage(content=prompt)\n",
        "            summary_text = self.llm([message]).content\n",
        "            doc_id = doc.metadata.get(\"source_pdf\", \"unknown\") + \"_\" + str(hash(doc.page_content[:50]))\n",
        "            generated[doc_id] = {\"summary\": summary_text}\n",
        "        return generated\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_summaries(generated_summaries: dict, ground_truth: dict, verbose: bool = False) -> dict:\n",
        "        \"\"\"\n",
        "        Compares generated summaries against the ground truth using ROUGE metrics.\n",
        "        Both dictionaries should have matching keys.\n",
        "        Returns a dictionary mapping keys to their ROUGE scores.\n",
        "        \"\"\"\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        eval_scores = {}\n",
        "        for key, gen_dict in generated_summaries.items():\n",
        "            gen_summary = gen_dict.get(\"summary\", \"\")\n",
        "            if key in ground_truth:\n",
        "                ref_summary = ground_truth[key]\n",
        "                scores = scorer.score(ref_summary, gen_summary)\n",
        "                eval_scores[key] = scores\n",
        "                if verbose:\n",
        "                    print(f\"Evaluation for {key}: {scores}\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(f\"No ground truth provided for {key}.\")\n",
        "        return eval_scores\n",
        "\n",
        "# --- Multi-Agent Instances ---\n",
        "jpm_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"jpmorgan\" in folder.lower()]\n",
        "ubs_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"ubs\" in folder.lower()]\n",
        "\n",
        "CONFIG_JPM = CONFIG.copy()\n",
        "CONFIG_JPM[\"pdf_folders\"] = jpm_folders\n",
        "\n",
        "CONFIG_UBS = CONFIG.copy()\n",
        "CONFIG_UBS[\"pdf_folders\"] = ubs_folders\n",
        "\n",
        "jpm_chatbot = RAGChatbot(CONFIG_JPM, bank=\"jpmorgan\")\n",
        "ubs_chatbot = RAGChatbot(CONFIG_UBS, bank=\"ubs\")\n",
        "\n",
        "def jpm_tool(query: str) -> str:\n",
        "    return jpm_chatbot.answer_query(query)\n",
        "\n",
        "def ubs_tool(query: str) -> str:\n",
        "    return ubs_chatbot.answer_query(query)\n",
        "\n",
        "jpm_tool_instance = Tool(\n",
        "    name=\"JP_Morgan_RAG\",\n",
        "    func=jpm_tool,\n",
        "    description=\"Answers questions about JP Morgan earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "ubs_tool_instance = Tool(\n",
        "    name=\"UBS_RAG\",\n",
        "    func=ubs_tool,\n",
        "    description=\"Answers questions about UBS earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "master_agent = initialize_agent(\n",
        "    [jpm_tool_instance, ubs_tool_instance],\n",
        "    jpm_chatbot.llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=DEV_MODE,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"prefix\": MASTER_AGENT_PROMPT}\n",
        ")\n",
        "\n",
        "# --- ROUGE Evaluation Method ---\n",
        "def calculate_rouge_scores(generated_summary: str, reference_summary: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calculate and print ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L) between a generated summary and a reference summary.\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    print(\"ROUGE Scores:\")\n",
        "    for key, score in scores.items():\n",
        "        print(f\"{key}: precision={score.precision:.2f}, recall={score.recall:.2f}, fmeasure={score.fmeasure:.2f}\")\n",
        "    return scores\n",
        "\n",
        "# --- Batch Mode Chatbot Loop with CSV Logging ---\n",
        "def run_master_agent_batch(questions: list[str]):\n",
        "    print(\"Master Agent Chatbot Batch Mode (processing multiple questions)\")\n",
        "    csv_file = os.path.join(GPT_FOLDER, \"master_agent_results.csv\")\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        cols = [\"Year/Quarter\", \"Question\", \"Master Answer\", \"Full Output\"] if DEV_MODE else [\"Year/Quarter\", \"Question\", \"Master Answer\"]\n",
        "        df = pd.DataFrame(columns=cols)\n",
        "\n",
        "    for question in questions:\n",
        "        if question in df[\"Question\"].values:\n",
        "            if DEV_MODE:\n",
        "                print(f\"Skipping already processed question: {question}\")\n",
        "            continue\n",
        "\n",
        "        answer = master_agent.run(question)\n",
        "        year_quarter = \"Unknown\"\n",
        "        if DEV_MODE:\n",
        "            full_output = \"Full chain output logged in console.\"\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": question, \"Master Answer\": answer, \"Full Output\": full_output}])\n",
        "        else:\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": question, \"Master Answer\": answer}])\n",
        "\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Master Agent Answer:\\n{answer}\\n\")\n",
        "        print(f\"Results saved to {csv_file}\\n{'-'*60}\")\n",
        "\n",
        "# --- Interactive Mode Chatbot Loop with Follow-up Option ---\n",
        "def run_master_agent_interactive():\n",
        "    \"\"\"\n",
        "    Runs an interactive chatbot loop where you can ask a question and then add follow-up questions\n",
        "    based on the output of previous prompts.\n",
        "    \"\"\"\n",
        "    print(\"Master Agent Chatbot Interactive Mode (type 'exit' to quit)\")\n",
        "    csv_file = os.path.join(GPT_FOLDER, \"master_agent_results.csv\")\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        cols = [\"Year/Quarter\", \"Question\", \"Master Answer\", \"Full Output\"] if DEV_MODE else [\"Year/Quarter\", \"Question\", \"Master Answer\"]\n",
        "        df = pd.DataFrame(columns=cols)\n",
        "\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() == \"exit\":\n",
        "            print(\"Exiting Master Agent Chatbot. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        answer = master_agent.run(user_q)\n",
        "        year_quarter = \"Unknown\"\n",
        "        if DEV_MODE:\n",
        "            full_output = \"Full chain output logged in console.\"\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer, \"Full Output\": full_output}])\n",
        "        else:\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer}])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"\\nMaster Agent Answer:\\n{answer}\\n\")\n",
        "\n",
        "        follow = input(\"Would you like to ask a follow-up question? (yes/no): \")\n",
        "        while follow.lower() == \"yes\":\n",
        "            followup_q = input(\"Follow-up question: \")\n",
        "            followup_answer = master_agent.run(followup_q)\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": followup_q, \"Master Answer\": followup_answer}])\n",
        "            df = pd.concat([df, new_row], ignore_index=True)\n",
        "            df.to_csv(csv_file, index=False)\n",
        "            print(f\"\\nFollow-up Answer:\\n{followup_answer}\\n\")\n",
        "            follow = input(\"Would you like to ask another follow-up question? (yes/no): \")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eqaqKdZjDbRj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage for Evaluation ---\n",
        "if __name__ == \"__main__\":\n",
        "    # To run batch mode:\n",
        "    batch_questions = [\n",
        "        \"What are the key market and credit risk factors highlighted in the latest JP Morgan earnings call?\",\n",
        "        \"How are JP Morgan's balance sheet exposures and risk-weighted assets affecting its risk profile in the previous year?\",\n",
        "        \"What cost pressures or margin compressions were mentioned that could pose risks to JP Morgan's performance?\",\n",
        "        \"What are the primary challenges or risk factors affecting UBS's earnings call, particularly regarding its operating profit?\",\n",
        "        \"How have changes in UBS's revenue mix and expense growth impacted its overall risk profile?\",\n",
        "        \"What steps is UBS taking to manage or mitigate its identified risks, especially related to technology costs?\",\n",
        "        \"For each bank, identify the overall sentiment based on the tone used in the transcripts for the latest quarter.\"\n",
        "    ]\n",
        "    run_master_agent_batch(batch_questions)\n",
        "\n",
        "    # Replace the following generated_summary and reference_summary with actual outputs.\n",
        "    generated_summary = \"• Net income: $13.2B • EPS: $4.33 • Revenue: $40.7B • ROTCE: 22% • First Republic: $2.2B revenue, $858M expenses, $1.1B net income • Basel III: ~30% RWA increase (~$500B), capital requirements up ~25% • Strategic: conservative capital, moderate buybacks\"\n",
        "    reference_summary = (\n",
        "        \"• Net income: $13.2B\\n\"\n",
        "        \"• EPS: $4.33\\n\"\n",
        "        \"• Revenue: $40.7B\\n\"\n",
        "        \"• ROTCE: 22%\\n\"\n",
        "        \"• First Republic: $2.2B revenue, $858M expense, $1.1B net income\\n\"\n",
        "        \"• Basel III Endgame: ~30% RWA increase (~$500B), capital requirements up ~25%\\n\"\n",
        "        \"• Strategic: conservative capital management and moderate buybacks\"\n",
        "    )\n",
        "    calculate_rouge_scores(generated_summary, reference_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEbH4Fw2U718",
        "outputId": "f54304b1-cfa4-412e-f417-36c8ed6b66dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master Agent Chatbot Batch Mode (processing multiple questions)\n",
            "ROUGE Scores:\n",
            "rouge1: precision=1.00, recall=0.93, fmeasure=0.96\n",
            "rouge2: precision=0.95, recall=0.88, fmeasure=0.91\n",
            "rougeL: precision=1.00, recall=0.93, fmeasure=0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Chatbot interactively\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WwO7TqjU6UR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_master_agent_interactive()"
      ],
      "metadata": {
        "id": "466sK14Vr0T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d18a8a8-d4b7-4b9a-f56c-6394b3fff4b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master Agent Chatbot Interactive Mode (type 'exit' to quit)\n",
            "You: What steps is UBS taking to manage or mitigate its identified risks\n",
            "\n",
            "Master Agent Answer:\n",
            "UBS is likely taking several steps to manage or mitigate its identified risks. These steps may include enhancing internal processes for risk management, control, and measurement, improving financial models, implementing cybersecurity measures to address cyberattack threats, ensuring compliance with regulations, maintaining access to capital markets, developing robust business continuity plans, integrating acquired entities efficiently, reducing costs, divesting non-core assets, and addressing potential operational failures.\n",
            "\n",
            "Would you like to ask a follow-up question? (yes/no): yes\n",
            "Follow-up question: what steps are UBS taking to decommissioning legacy systems\n",
            "\n",
            "Follow-up Answer:\n",
            "UBS is actively decommissioning legacy systems as part of their integration and restructuring efforts. They have removed over 40% of Non-core and Legacy’s applications, worked through 16 petabytes of data, and reduced the number of legacy servers by over 40%. They are focusing on migrating client accounts, decommissioning Credit Suisse models and applications, and streamlining their technology infrastructure to enhance efficiency and reduce costs.\n",
            "\n",
            "Would you like to ask another follow-up question? (yes/no): yes\n",
            "Follow-up question: In the most recent quarter has there been further discussion relating to UBS actions to mitigate risk?\n",
            "\n",
            "Follow-up Answer:\n",
            "Yes, in the most recent quarter, there have been discussions about UBS's actions to mitigate risk. The statements mention ongoing execution of strategic plans, cost reduction and efficiency initiatives, managing risk-weighted assets, liquidity coverage ratio, and other financial resources. Additionally, there is a focus on implementing changes to meet changing market and regulatory conditions, especially in light of the acquisition of Credit Suisse. The integration of Credit Suisse entities into the UBS structure is expected to take between three to five years and presents significant risks.\n",
            "\n",
            "Would you like to ask another follow-up question? (yes/no): yes\n",
            "Follow-up question: Provide a summary of UBS most recent quarter,discussions about UBS's actions to mitigate risk.\n",
            "\n",
            "Follow-up Answer:\n",
            "UBS's actions to mitigate risk include resolving legal matters related to U.S. residential mortgage-backed securities and maintaining strong client relationships in Global Wealth Management and Asset Management.\n",
            "\n",
            "Would you like to ask another follow-up question? (yes/no): no\n",
            "You: exit\n",
            "Exiting Master Agent Chatbot. Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}