{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "cf7e6da9-a4bb-4d28-c0eb-a6289ebf9a26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb sentence-transformers pypdf datasets rouge-score  > /dev/null 2>&1\n",
        "!pip install --upgrade langchain_community   > /dev/null 2>&1\n",
        "!pip install -U langchain-huggingface  > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "import warnings\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')# Replace with your actual token"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29d4dec-a48a-4792-d41d-6107937bc5bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toggle flag: True for development (verbose output) and False for production (minimal output)\n",
        "DEV_MODE = False"
      ],
      "metadata": {
        "id": "SI-Sa_fcKhpt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally, suppress warnings in production mode\n",
        "if not DEV_MODE:\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    \"pdf_folders\": [\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"gpt-3.5-turbo\",  # Using GPT-3.5-turbo\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 1024,\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "# --- Master Agent Prompt (Refined) ---\n",
        "MASTER_AGENT_PROMPT = (\n",
        "    \"You are a highly accurate and detail-oriented assistant specialized in analyzing bank earnings call transcripts.\\n\"\n",
        "    \"ONLY use the information from the retrieved transcript context. Your final answer must be presented as a bullet-point list.\\n\\n\"\n",
        "    \"Follow EXACTLY this format:\\n\"\n",
        "    \"Thought: Briefly explain which transcript sections are relevant.\\n\"\n",
        "    \"Action: Use the appropriate tool (JP_Morgan_RAG or UBS_RAG) by writing e.g. JP_Morgan_RAG(\\\"<query>\\\").\\n\"\n",
        "    \"Observation: Summarize the retrieved context in a few words.\\n\"\n",
        "    \"Final Answer: Provide a concise bullet-point list with the key sentiments and takeaways.\\n\\n\"\n",
        "    \"Now, answer the following question:\\n\"\n",
        "    \"{input}\\n\"\n",
        "    \"Begin!\"\n",
        ")\n",
        "\n",
        "# --- RAGChatbot Class ---\n",
        "class RAGChatbot:\n",
        "    \"\"\"\n",
        "    A RAG chatbot that ingests PDF earnings call transcripts, builds a vector store,\n",
        "    and uses a ConversationalRetrievalChain for Q&A.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, bank: str):\n",
        "        self.config = config\n",
        "        # For GPT-3.5-turbo via ChatOpenAI, the API key must be set as OPENAI_API_KEY.\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        self.pdf_folders = config[\"pdf_folders\"]\n",
        "        self.persist_directory = config[\"persist_directory\"]\n",
        "        self.max_length = config[\"max_length\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.chunk_size = config[\"chunk_size\"]\n",
        "        self.chunk_overlap = config[\"chunk_overlap\"]\n",
        "        self.chunk_threshold = config[\"chunk_threshold\"]\n",
        "        self.memory_window_k = config[\"memory_window_k\"]\n",
        "        self.retriever_search_k = config[\"retriever_search_k\"]\n",
        "        self.bank = bank\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_embeddings()\n",
        "        # Create a separate tokenizer for splitting (using GPT-2 as a proxy)\n",
        "        from transformers import AutoTokenizer\n",
        "        self.split_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        self._load_documents()\n",
        "        self._build_vector_store()\n",
        "        self._build_summary_index()\n",
        "        self._setup_retrieval_chain()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        # Initialize GPT-3.5-turbo via ChatOpenAI with top_p passed in model_kwargs.\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=self.config[\"llm_model_name\"],\n",
        "            temperature=self.config[\"temperature\"],\n",
        "            max_tokens=self.max_length,\n",
        "            openai_api_key=self.api_key,\n",
        "            model_kwargs={\"top_p\": self.config[\"top_p\"]}\n",
        "        )\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        emb_model = self.config[\"embedding_model_name\"]\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=emb_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank = os.path.basename(folder).lower()\n",
        "            files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            for file in files:\n",
        "                path = os.path.join(folder, file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank\n",
        "                        doc.metadata[\"source_pdf\"] = file\n",
        "                        doc.metadata[\"year_quarter\"] = \"Unknown\"  # Temporarily set to \"Unknown\"\n",
        "                    self.documents.extend(docs)\n",
        "                    if DEV_MODE:\n",
        "                        print(f\"Loaded: {file} from {folder} -> Unknown\")\n",
        "                except Exception as e:\n",
        "                    if DEV_MODE:\n",
        "                        print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    def _chunk_document(self, doc: Document) -> list[Document]:\n",
        "        tokens = self.split_tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self._remove_duplicates(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    @staticmethod\n",
        "    def _remove_duplicates(chunks: list[Document]) -> list[Document]:\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def _build_vector_store(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "        if DEV_MODE:\n",
        "            print(f\"Built raw vector store with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _build_summary_index(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings,\n",
        "            persist_directory=os.path.join(self.persist_directory, \"summaries\"))\n",
        "        if DEV_MODE:\n",
        "            print(f\"Built summary vector index with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _setup_retrieval_chain(self):\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=self.memory_window_k, memory_key=\"chat_history\", return_messages=True)\n",
        "        self.retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_db.as_retriever(\n",
        "                search_kwargs={\"k\": self.retriever_search_k, \"filter\": {\"bank\": self.bank}})\n",
        "            ,\n",
        "            memory=memory,\n",
        "            verbose=DEV_MODE\n",
        "        )\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        response = self.retrieval_chain({\"question\": query})\n",
        "        return response.get(\"answer\", \"\").strip()\n",
        "\n",
        "# --- Create Multi-Agent Instances ---\n",
        "jpm_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"jpmorgan\" in folder.lower()]\n",
        "ubs_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"ubs\" in folder.lower()]\n",
        "\n",
        "CONFIG_JPM = CONFIG.copy()\n",
        "CONFIG_JPM[\"pdf_folders\"] = jpm_folders\n",
        "\n",
        "CONFIG_UBS = CONFIG.copy()\n",
        "CONFIG_UBS[\"pdf_folders\"] = ubs_folders\n",
        "\n",
        "jpm_chatbot = RAGChatbot(CONFIG_JPM, bank=\"jpmorgan\")\n",
        "ubs_chatbot = RAGChatbot(CONFIG_UBS, bank=\"ubs\")\n",
        "\n",
        "def jpm_tool(query: str) -> str:\n",
        "    return jpm_chatbot.answer_query(query)\n",
        "\n",
        "def ubs_tool(query: str) -> str:\n",
        "    return ubs_chatbot.answer_query(query)\n",
        "\n",
        "jpm_tool_instance = Tool(\n",
        "    name=\"JP_Morgan_RAG\",\n",
        "    func=jpm_tool,\n",
        "    description=\"Answers questions about JP Morgan earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "ubs_tool_instance = Tool(\n",
        "    name=\"UBS_RAG\",\n",
        "    func=ubs_tool,\n",
        "    description=\"Answers questions about UBS earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "master_agent = initialize_agent(\n",
        "    [jpm_tool_instance, ubs_tool_instance],\n",
        "    jpm_chatbot.llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=DEV_MODE,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"prefix\": MASTER_AGENT_PROMPT}\n",
        ")\n",
        "\n",
        "# --- Chatbot Loop (Master Agent) with CSV Logging ---\n",
        "def run_master_agent():\n",
        "    print(\"Master Agent Chatbot (type 'exit' to quit)\")\n",
        "    csv_file = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs/gpt/master_agent_results.csv\"\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        if DEV_MODE:\n",
        "            df = pd.DataFrame(columns=[\"Year/Quarter\", \"Question\", \"Master Answer\", \"Full Output\"])\n",
        "        else:\n",
        "            df = pd.DataFrame(columns=[\"Year/Quarter\", \"Question\", \"Master Answer\"])\n",
        "\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() == \"exit\":\n",
        "            print(\"Exiting Master Agent Chatbot. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Check if the question has already been asked\n",
        "        if user_q in df[\"Question\"].values:\n",
        "            if DEV_MODE:\n",
        "                print(\"This question has already been asked. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        answer = master_agent.run(user_q)\n",
        "        year_quarter = \"Unknown\"  # Year/Quarter extraction is disabled for now.\n",
        "        if DEV_MODE:\n",
        "            full_output = \"Full chain output logged in console.\"\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer, \"Full Output\": full_output}])\n",
        "        else:\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer}])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        # Always display the master answer on screen\n",
        "        print(f\"\\nMaster Agent Answer:\\n{answer}\\n\")\n",
        "        print(f\"Results saved to {csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "eqaqKdZjDbRj",
        "outputId": "5de9db0d-5073-4ecb-dfda-d98883716c41"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master Agent Chatbot (type 'exit' to quit)\n",
            "You: What are the key market and credit risk factors highlighted in the latest JP Morgan earnings call?\n",
            "\n",
            "Master Agent Answer:\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The key market and credit risk factors highlighted in the latest JP Morgan earnings call are related to market competition, changes in market structure, the impact of new competitors like Jane Street, disruption to bank lending caused by private credit, the need for innovation and adaptation, and the implications of capital and liquidity regulations on the U.S. capital markets ecosystem.\"\n",
            "}\n",
            "\n",
            "Results saved to master_agent_results.csv\n",
            "You: In your final answer, please include an overall sentiment (e.g., positive, negative, or neutral) based on the tone and language used in the transcripts\n",
            "\n",
            "Master Agent Answer:\n",
            "Please include the overall sentiment in the final response.\n",
            "\n",
            "Results saved to master_agent_results.csv\n",
            "You: Identify the overall sentiment (e.g., positive, negative, or neutral) based on the tone and language used in the transcripts\n",
            "\n",
            "Master Agent Answer:\n",
            "The overall sentiment in the earnings call transcripts is cautiously optimistic for JP Morgan and positive and optimistic for UBS.\n",
            "\n",
            "Results saved to master_agent_results.csv\n",
            "You: Identify the overall sentiment (e.g., positive, negative, or neutral) based on the tone and language used in the transcripts for the latest quarter\n",
            "\n",
            "Master Agent Answer:\n",
            "The overall sentiment in the latest quarter's earnings call transcripts for UBS is positive and forward-looking, with a focus on execution, planning for the future, confidence in the company's position, and emphasis on creating a foundation for future growth and success.\n",
            "\n",
            "Results saved to master_agent_results.csv\n",
            "You: For each bank identify the overall sentiment (e.g., positive, negative, or neutral) based on the tone and language used in the transcripts for the latest quarter\n",
            "\n",
            "Master Agent Answer:\n",
            "Positive for UBS and positive for JP Morgan\n",
            "\n",
            "Results saved to master_agent_results.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0472f82fa2d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mrun_master_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-0472f82fa2d3>\u001b[0m in \u001b[0;36mrun_master_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0muser_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting Master Agent Chatbot. Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_master_agent()"
      ],
      "metadata": {
        "id": "zCpuYOKMbtwO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}