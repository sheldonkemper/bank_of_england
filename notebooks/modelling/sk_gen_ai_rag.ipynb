{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "cf7e6da9-a4bb-4d28-c0eb-a6289ebf9a26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb sentence-transformers pypdf datasets rouge-score  > /dev/null 2>&1\n",
        "!pip install --upgrade langchain_community   > /dev/null 2>&1\n",
        "!pip install -U langchain-huggingface  > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "# Use the new ChatOpenAI wrapper instead of OpenAI\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN') # Replace with your actual token"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238306e0-c5cd-42f1-a182-c6fc0d6a5137"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Summarize the latest JP Morgan earnings call.\"\n",
        "\n",
        "\"What are the key challenges mentioned by UBS?\"\n",
        "\n",
        "\"What cost pressures or margin compressions were mentioned that could pose risks to JP Morgan's performance?\"\n",
        "\n",
        "\"How are JP Morgan's balance sheet exposures and risk-weighted assets affecting its risk profile?\""
      ],
      "metadata": {
        "id": "563_YKBr8tFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY') # Replace with your actual token"
      ],
      "metadata": {
        "id": "SI-Sa_fcKhpt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    \"pdf_folders\": [\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"gpt-3.5-turbo\",  # Using GPT-3.5-turbo\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 1024,\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "# --- Master Agent Prompt (Refined) ---\n",
        "MASTER_AGENT_PROMPT = (\n",
        "    \"You are a highly accurate and detail-oriented assistant specialized in analyzing bank earnings call transcripts.\\n\"\n",
        "    \"ONLY use the information from the retrieved transcript context. Your final answer must be presented as a bullet-point list.\\n\\n\"\n",
        "    \"Follow EXACTLY this format:\\n\"\n",
        "    \"Thought: Briefly explain which transcript sections are relevant.\\n\"\n",
        "    \"Action: Use the appropriate tool (JP_Morgan_RAG or UBS_RAG) by writing e.g. JP_Morgan_RAG(\\\"<query>\\\").\\n\"\n",
        "    \"Observation: Summarize the retrieved context in a few words.\\n\"\n",
        "    \"Final Answer: Provide a concise bullet-point list with the key sentiments and takeaways.\\n\\n\"\n",
        "    \"Now, answer the following question:\\n\"\n",
        "    \"{input}\\n\"\n",
        "    \"Begin!\"\n",
        ")\n",
        "\n",
        "# --- RAGChatbot Class ---\n",
        "class RAGChatbot:\n",
        "    \"\"\"\n",
        "    A RAG chatbot that ingests PDF earnings call transcripts, builds a vector store,\n",
        "    and uses a ConversationalRetrievalChain for Q&A.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, bank: str):\n",
        "        self.config = config\n",
        "        # For GPT-3.5-turbo via ChatOpenAI, the API key must be set as OPENAI_API_KEY.\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        self.pdf_folders = config[\"pdf_folders\"]\n",
        "        self.persist_directory = config[\"persist_directory\"]\n",
        "        self.max_length = config[\"max_length\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.chunk_size = config[\"chunk_size\"]\n",
        "        self.chunk_overlap = config[\"chunk_overlap\"]\n",
        "        self.chunk_threshold = config[\"chunk_threshold\"]\n",
        "        self.memory_window_k = config[\"memory_window_k\"]\n",
        "        self.retriever_search_k = config[\"retriever_search_k\"]\n",
        "        self.bank = bank\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_embeddings()\n",
        "        # Create a separate tokenizer for splitting (using GPT-2 as a proxy)\n",
        "        from transformers import AutoTokenizer\n",
        "        self.split_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        self._load_documents()\n",
        "        self._build_vector_store()\n",
        "        self._build_summary_index()\n",
        "        self._setup_retrieval_chain()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        # Initialize GPT-3.5-turbo via ChatOpenAI.\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=self.config[\"llm_model_name\"],\n",
        "            temperature=self.config[\"temperature\"],\n",
        "            top_p=self.config[\"top_p\"],\n",
        "            max_tokens=self.max_length,\n",
        "            openai_api_key=self.api_key\n",
        "        )\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        emb_model = self.config[\"embedding_model_name\"]\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=emb_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank = os.path.basename(folder).lower()\n",
        "            files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            for file in files:\n",
        "                path = os.path.join(folder, file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank\n",
        "                        doc.metadata[\"source_pdf\"] = file\n",
        "                    self.documents.extend(docs)\n",
        "                    print(f\"Loaded: {file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    def _chunk_document(self, doc: Document) -> list[Document]:\n",
        "        tokens = self.split_tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self._remove_duplicates(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    @staticmethod\n",
        "    def _remove_duplicates(chunks: list[Document]) -> list[Document]:\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def _build_vector_store(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "        print(f\"Built raw vector store with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _build_summary_index(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings,\n",
        "            persist_directory=os.path.join(self.persist_directory, \"summaries\"))\n",
        "        print(f\"Built summary vector index with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _setup_retrieval_chain(self):\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=self.memory_window_k, memory_key=\"chat_history\", return_messages=True)\n",
        "        self.retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_db.as_retriever(\n",
        "                search_kwargs={\"k\": self.retriever_search_k, \"filter\": {\"bank\": self.bank}})\n",
        "            ,\n",
        "            memory=memory,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        response = self.retrieval_chain({\"question\": query})\n",
        "        return response.get(\"answer\", \"\").strip()\n",
        "\n",
        "# --- Create Multi-Agent Instances ---\n",
        "# Filter PDF folders for each bank.\n",
        "jpm_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"jpmorgan\" in folder.lower()]\n",
        "ubs_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"ubs\" in folder.lower()]\n",
        "\n",
        "# Create separate configurations.\n",
        "CONFIG_JPM = CONFIG.copy()\n",
        "CONFIG_JPM[\"pdf_folders\"] = jpm_folders\n",
        "\n",
        "CONFIG_UBS = CONFIG.copy()\n",
        "CONFIG_UBS[\"pdf_folders\"] = ubs_folders\n",
        "\n",
        "# Initialize separate RAGChatbot instances.\n",
        "jpm_chatbot = RAGChatbot(CONFIG_JPM, bank=\"jpmorgan\")\n",
        "ubs_chatbot = RAGChatbot(CONFIG_UBS, bank=\"ubs\")\n",
        "\n",
        "# --- Define Tools for Each Agent ---\n",
        "def jpm_tool(query: str) -> str:\n",
        "    return jpm_chatbot.answer_query(query)\n",
        "\n",
        "def ubs_tool(query: str) -> str:\n",
        "    return ubs_chatbot.answer_query(query)\n",
        "\n",
        "jpm_tool_instance = Tool(\n",
        "    name=\"JP_Morgan_RAG\",\n",
        "    func=jpm_tool,\n",
        "    description=\"Answers questions about JP Morgan earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "ubs_tool_instance = Tool(\n",
        "    name=\"UBS_RAG\",\n",
        "    func=ubs_tool,\n",
        "    description=\"Answers questions about UBS earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "# --- Master Agent Integration ---\n",
        "master_agent = initialize_agent(\n",
        "    [jpm_tool_instance, ubs_tool_instance],\n",
        "    jpm_chatbot.llm,  # Using the same GPT-3.5-turbo LLM for all agents.\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"prefix\": MASTER_AGENT_PROMPT}\n",
        ")\n",
        "\n",
        "# --- Chatbot Loop (Master Agent) ---\n",
        "def run_master_agent():\n",
        "    print(\"Master Agent Chatbot (type 'exit' to quit)\")\n",
        "    # Check if CSV file exists and load it; otherwise, create an empty DataFrame.\n",
        "    csv_file = \"master_agent_results.csv\"\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=[\"Year/Quarter\", \"Question\", \"Master Answer\"])\n",
        "\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() == \"exit\":\n",
        "            print(\"Exiting Master Agent Chatbot. Goodbye!\")\n",
        "            break\n",
        "        # Only process if the question is not already in the DataFrame\n",
        "        if user_q in df[\"Question\"].values:\n",
        "            print(\"This question has already been asked. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        answer = master_agent.run(user_q)\n",
        "        # For now, set year/quarter as \"Unknown\" or add logic to determine it.\n",
        "        year_quarter = \"Unknown\"\n",
        "        new_row = {\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer}\n",
        "        df = df.append(new_row, ignore_index=True)\n",
        "        # Save updated DataFrame to CSV\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"\\nMaster Agent Answer:\\n{answer}\\n\")\n",
        "        print(f\"Results saved to {csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqaqKdZjDbRj",
        "outputId": "611a897e-7044-4e53-864b-d8ccb8df5eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 1q23-earnings-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: 2q23-earnings-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: 4q24-earnings-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-1q24-earnings-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-2q24-earnings-call-transcript-final.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-3q23-earnings-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-4q23-earnings-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Built raw vector store with 252 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built summary vector index with 252 chunks.\n",
            "Loaded: 1q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 1q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 2q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 2q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 3q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 3q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 4q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 4q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Built raw vector store with 228 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    run_master_agent()\n"
      ],
      "metadata": {
        "id": "zCpuYOKMbtwO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}