{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "11f4112a-6ec2-4891-f67a-f86f638765a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install langchain-community\n",
        "!pip install -q langchain-community pypdf tiktoken chromadb sentence-transformers datasets rouge-score huggingface_hub torch transformers  > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import drive\n",
        "# For evaluation metrics (ROUGE)\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "import warnings\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.environ[\"HF\"] = userdata.get('HF') # Replace with your actual token"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Thoroughly: Ask the master agent the following questions:\n",
        "\n",
        "\"What bank are these transcripts for?\"\n",
        "\n",
        "\"What is Jamie Dimon's role?\"\n",
        "\n",
        "\"What was the operating profit at UBS in Q2 2023?\"\n",
        "\n",
        "\"Summarize the latest JP Morgan earnings call.\"\n",
        "\n",
        "\"What are the key challenges mentioned by UBS?\""
      ],
      "metadata": {
        "id": "563_YKBr8tFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    \"pdf_folders\": [\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"google/bigbird-pegasus-large-arxiv\",  # For long sequences\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"text_generation_pipeline_task\": \"text2text-generation\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 1024,\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "# --- Master Agent Prompt (Refined) ---\n",
        "MASTER_AGENT_PROMPT = (\n",
        "    \"You are an expert in analyzing bank earnings call transcripts. \"\n",
        "    \"Your task is to answer questions accurately using provided tools. Be very concise.\\n\\n\"\n",
        "    \"When answering, follow EXACTLY this format:\\n\\n\"\n",
        "    \"Thought: Briefly explain my reasoning. What information do I need? Which tool should I use?\\n\"\n",
        "    \"Action: Use one of the tools: JP_Morgan_RAG or UBS_RAG. \"\n",
        "    \"To use a tool, write: Tool_Name(\\\"query\\\"), for example: JP_Morgan_RAG(\\\"what is net income at JP Morgan?\\\").\"\n",
        "    \"If the question asks about JP Morgan, use JP_Morgan_RAG. If the question asks about UBS, use UBS_RAG.\\n\"\n",
        "    \"Observation: Report the result from the tool. Keep it concise.\\n\"\n",
        "    \"Final Answer: Give a final, short answer to the original question.\\n\\n\"\n",
        "    \"Example 1:\\n\"\n",
        "    \"Question: What is Jamie Dimon's role at JP Morgan?\\n\"\n",
        "    \"Thought: I need to find information about Jamie Dimon's role at JP Morgan. Since the question is about JP Morgan I should use the JP_Morgan_RAG tool.\\n\"\n",
        "    \"Action: JP_Morgan_RAG(\\\"What is Jamie Dimon's role?\\\")\\n\"\n",
        "    \"Observation: Jamie Dimon is the CEO of JP Morgan Chase.\\n\"\n",
        "    \"Final Answer: Jamie Dimon is the CEO of JP Morgan Chase.\\n\\n\"\n",
        "    \"Example 2:\\n\"\n",
        "    \"Question: What was the operating profit at UBS in Q2 2023?\\n\"\n",
        "    \"Thought: I need to find the operating profit for UBS in Q2 2023. Since the question is about UBS, I should use the UBS_RAG tool.\\n\"\n",
        "    \"Action: UBS_RAG(\\\"What was the operating profit in Q2 2023?\\\")\\n\"\n",
        "    \"Observation: UBS's operating profit in Q2 2023 was $1.5 billion.\\n\"\n",
        "    \"Final Answer: UBS's operating profit in Q2 2023 was $1.5 billion.\\n\\n\"\n",
        "    \"Question: {input}\\n\"  # Place the question directly in the prompt for clarity\n",
        "    \"Begin!\\n\" #Include newline\n",
        ")\n",
        "\n",
        "\n",
        "# --- RAGChatbot Class ---\n",
        "class RAGChatbot:\n",
        "    \"\"\"\n",
        "    A RAG chatbot that ingests PDF earnings call transcripts, builds a vector store,\n",
        "    and uses a ConversationalRetrievalChain for Q&A.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, bank: str): #Added bank parameter for metadata filtering\n",
        "        self.config = config\n",
        "        self.hf_token = os.environ.get('HF') # Access token from environment variable\n",
        "        self.pdf_folders = config[\"pdf_folders\"]\n",
        "        self.persist_directory = config[\"persist_directory\"]\n",
        "        self.max_length = config[\"max_length\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.chunk_size = config[\"chunk_size\"]\n",
        "        self.chunk_overlap = config[\"chunk_overlap\"]\n",
        "        self.chunk_threshold = config[\"chunk_threshold\"]\n",
        "        self.memory_window_k = config[\"memory_window_k\"]\n",
        "        self.retriever_search_k = config[\"retriever_search_k\"]\n",
        "        self.bank = bank #Added bank attribute\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_embeddings()\n",
        "        self._load_documents()\n",
        "        self._build_vector_store()\n",
        "        self._build_summary_index()\n",
        "        self._setup_retrieval_chain()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        model_name = self.config[\"llm_model_name\"]\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=self.hf_token) #Fixed: Use token instead of use_auth_token\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16, token=self.hf_token) #Fixed: Use token instead of use_auth_token\n",
        "        if self.model.config.pad_token_id is None:\n",
        "            self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.to(\"cuda\")\n",
        "        # Use num_beams=1 to avoid beam search issues that trigger CUDA asserts.\n",
        "        self.pipe = pipeline(\n",
        "            self.config[\"text_generation_pipeline_task\"],\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=self.max_length,\n",
        "            temperature=self.config[\"temperature\"],\n",
        "            top_p=self.config[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            batch_size=self.batch_size,\n",
        "            num_beams=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        emb_model = self.config[\"embedding_model_name\"]\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=emb_model) # Removed token argument\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank = os.path.basename(folder).lower()\n",
        "            files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            for file in files:\n",
        "                path = os.path.join(folder, file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank\n",
        "                        doc.metadata[\"source_pdf\"] = file\n",
        "                    self.documents.extend(docs)\n",
        "                    print(f\"Loaded: {file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    def _chunk_document(self, doc: Document) -> list[Document]:\n",
        "        tokens = self.tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self._remove_duplicates(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    @staticmethod\n",
        "    def _remove_duplicates(chunks: list[Document]) -> list[Document]:\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def _build_vector_store(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "        self.raw_db.persist()\n",
        "        print(f\"Built raw vector store with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _build_summary_index(self):\n",
        "        # For simplicity, we use the same chunks as summaries.\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings,\n",
        "            persist_directory=os.path.join(self.persist_directory, \"summaries\"))\n",
        "        self.summary_db.persist()\n",
        "        print(f\"Built summary vector index with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _setup_retrieval_chain(self):\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=self.memory_window_k, memory_key=\"chat_history\", return_messages=True)\n",
        "        #Added filter to retriever for bank metadata\n",
        "        self.retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_db.as_retriever(search_kwargs={\"k\": self.retriever_search_k, \"filter\": {\"bank\": self.bank}}),\n",
        "            memory=memory,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        response = self.retrieval_chain({\"question\": query})\n",
        "        return response.get(\"answer\", \"\").strip() #Fixed missing return for anser_query\n",
        "\n",
        "# --- Create Multi-Agent Instances ---\n",
        "# Filter PDF folders for each bank.\n",
        "jpm_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"jpmorgan\" in folder.lower()]\n",
        "ubs_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"ubs\" in folder.lower()]\n",
        "\n",
        "# Create separate configurations for each bank.\n",
        "CONFIG_JPM = CONFIG.copy()\n",
        "CONFIG_JPM[\"pdf_folders\"] = jpm_folders\n",
        "\n",
        "CONFIG_UBS = CONFIG.copy()\n",
        "CONFIG_UBS[\"pdf_folders\"] = ubs_folders\n",
        "\n",
        "# Initialize separate RAGChatbot instances.\n",
        "jpm_chatbot = RAGChatbot(CONFIG_JPM, bank=\"jpmorgan\") #Added bank parameter\n",
        "ubs_chatbot = RAGChatbot(CONFIG_UBS, bank=\"ubs\") #Added bank parameter\n",
        "\n",
        "# --- Define Tools for Each Agent ---\n",
        "def jpm_tool(query: str) -> str:\n",
        "    return jpm_chatbot.answer_query(query)\n",
        "\n",
        "def ubs_tool(query: str) -> str:\n",
        "    return ubs_chatbot.answer_query(query)\n",
        "\n",
        "jpm_tool_instance = Tool(\n",
        "    name=\"JP_Morgan_RAG\",\n",
        "    func=jpm_tool,\n",
        "    description=\"Answers questions about JP Morgan earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "ubs_tool_instance = Tool(\n",
        "    name=\"UBS_RAG\",\n",
        "    func=ubs_tool,\n",
        "    description=\"Answers questions about UBS earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "# --- Master Agent Integration ---\n",
        "master_agent = initialize_agent(\n",
        "    [jpm_tool_instance, ubs_tool_instance],\n",
        "    jpm_chatbot.llm,  # Using the same LLM pipeline; both agents use similar config.\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"prefix\": MASTER_AGENT_PROMPT} # Using the updated MASTER_AGENT_PROMPT\n",
        ")\n",
        "\n",
        "# --- Chatbot Loop (Master Agent) ---\n",
        "def run_master_agent():\n",
        "    print(\"Master Agent Chatbot (type 'exit' to quit)\")\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() == \"exit\":\n",
        "            print(\"Exiting Master Agent Chatbot. Goodbye!\")\n",
        "            break\n",
        "        answer = master_agent.run(user_q)\n",
        "        print(f\"\\nMaster Agent Answer:\\n{answer}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_master_agent()\n"
      ],
      "metadata": {
        "id": "mpB3aljX3dwi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}