{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcCHgfmdAvE9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "953210d4-1033-425d-83aa-49bdec309a3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install langchain-community\n",
        "!pip install -q langchain-community pypdf tiktoken chromadb sentence-transformers datasets rouge-score huggingface_hub torch transformers  > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import drive\n",
        "# For evaluation metrics (ROUGE)\n",
        "from rouge_score import rouge_scorer\n",
        "from huggingface_hub import hf_hub_download\n",
        "import warnings\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "userdata.get('HF')\n",
        "os.environ[\"HF\"] = userdata.get('HF') # Replace with your actual token"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ae6efc-8481-4ff2-b2d1-c4003ec1df7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    \"pdf_folders\": [\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"google/bigbird-pegasus-large-arxiv\",  # For long sequences\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"text_generation_pipeline_task\": \"text2text-generation\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 1024,\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "# --- Master Agent Prompt ---\n",
        "MASTER_AGENT_PROMPT = (\n",
        "    \"You are an expert in analyzing bank earnings call transcripts. \"\n",
        "    \"Your task is to answer questions accurately using provided tools.\\n\\n\"\n",
        "    \"When answering, follow EXACTLY this format:\\n\\n\"\n",
        "    \"Thought: Briefly explain your reasoning.\\n\"\n",
        "    \"Action: Use one of the tools: JP_Morgan_RAG or UBS_RAG. \"\n",
        "    \"To use a tool, write: Tool_Name(\\\"query\\\"), for example: JP_Morgan_RAG(\\\"what is net income?\\\").\\n\"\n",
        "    \"Observation: Report the result from the tool. Keep it concise.\\n\"\n",
        "    \"Final Answer: Give a final, short answer to the original question.\\n\\n\"\n",
        "    \"Question: {input}\\n\"  # Place the question directly in the prompt for clarity\n",
        "    \"Begin!\"\n",
        ")\n",
        "\n",
        "# --- RAGChatbot Class ---\n",
        "class RAGChatbot:\n",
        "    \"\"\"\n",
        "    A RAG chatbot that ingests PDF earnings call transcripts, builds a vector store,\n",
        "    and uses a ConversationalRetrievalChain for Q&A.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.hf_token = userdata.get('HF')\n",
        "        self.pdf_folders = config[\"pdf_folders\"]\n",
        "        self.persist_directory = config[\"persist_directory\"]\n",
        "        self.max_length = config[\"max_length\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.chunk_size = config[\"chunk_size\"]\n",
        "        self.chunk_overlap = config[\"chunk_overlap\"]\n",
        "        self.chunk_threshold = config[\"chunk_threshold\"]\n",
        "        self.memory_window_k = config[\"memory_window_k\"]\n",
        "        self.retriever_search_k = config[\"retriever_search_k\"]\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_embeddings()\n",
        "        self._load_documents()\n",
        "        self._build_vector_store()\n",
        "        self._build_summary_index()\n",
        "        self._setup_retrieval_chain()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        model_name = self.config[\"llm_model_name\"]\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=self.hf_token)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16, token=self.hf_token)\n",
        "        if self.model.config.pad_token_id is None:\n",
        "            self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.to(\"cuda\")\n",
        "        # Use num_beams=1 to avoid beam search issues that trigger CUDA asserts.\n",
        "        self.pipe = pipeline(\n",
        "            self.config[\"text_generation_pipeline_task\"],\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=self.max_length,\n",
        "            temperature=self.config[\"temperature\"],\n",
        "            top_p=self.config[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            batch_size=self.batch_size,\n",
        "            num_beams=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        emb_model = self.config[\"embedding_model_name\"]\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=emb_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank = os.path.basename(folder).lower()\n",
        "            files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            for file in files:\n",
        "                path = os.path.join(folder, file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank\n",
        "                        doc.metadata[\"source_pdf\"] = file\n",
        "                    self.documents.extend(docs)\n",
        "                    print(f\"Loaded: {file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    def _chunk_document(self, doc: Document) -> list[Document]:\n",
        "        tokens = self.tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self._remove_duplicates(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    @staticmethod\n",
        "    def _remove_duplicates(chunks: list[Document]) -> list[Document]:\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def _build_vector_store(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "        self.raw_db.persist()\n",
        "        print(f\"Built raw vector store with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _build_summary_index(self):\n",
        "        # For simplicity, we use the same chunks as summaries.\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings,\n",
        "            persist_directory=os.path.join(self.persist_directory, \"summaries\"))\n",
        "        self.summary_db.persist()\n",
        "        print(f\"Built summary vector index with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _setup_retrieval_chain(self):\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=self.memory_window_k, memory_key=\"chat_history\", return_messages=True)\n",
        "        self.retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_db.as_retriever(search_kwargs={\"k\": self.retriever_search_k}),\n",
        "            memory=memory,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        response = self.retrieval_chain({\"question\": query})\n",
        "        return response.get(\"answer\", \"\").strip()\n",
        "\n",
        "\n",
        "# --- Create Multi-Agent Instances ---\n",
        "# Filter PDF folders for each bank.\n",
        "jpm_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"jpmorgan\" in folder.lower()]\n",
        "ubs_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"ubs\" in folder.lower()]\n",
        "\n",
        "# Create separate configurations for each bank.\n",
        "CONFIG_JPM = CONFIG.copy()\n",
        "CONFIG_JPM[\"pdf_folders\"] = jpm_folders\n",
        "\n",
        "CONFIG_UBS = CONFIG.copy()\n",
        "CONFIG_UBS[\"pdf_folders\"] = ubs_folders\n",
        "\n",
        "# Initialize separate RAGChatbot instances.\n",
        "jpm_chatbot = RAGChatbot(CONFIG_JPM)\n",
        "ubs_chatbot = RAGChatbot(CONFIG_UBS)\n",
        "\n",
        "# --- Define Tools for Each Agent ---\n",
        "def jpm_tool(query: str) -> str:\n",
        "    return jpm_chatbot.answer_query(query)\n",
        "\n",
        "def ubs_tool(query: str) -> str:\n",
        "    return ubs_chatbot.answer_query(query)\n",
        "\n",
        "jpm_tool_instance = Tool(\n",
        "    name=\"JP_Morgan_RAG\",\n",
        "    func=jpm_tool,\n",
        "    description=\"Answers questions about JP Morgan earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "ubs_tool_instance = Tool(\n",
        "    name=\"UBS_RAG\",\n",
        "    func=ubs_tool,\n",
        "    description=\"Answers questions about UBS earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "# --- Master Agent Integration ---\n",
        "master_agent = initialize_agent(\n",
        "    [jpm_tool_instance, ubs_tool_instance],\n",
        "    jpm_chatbot.llm,  # Using the same LLM pipeline; both agents use similar config.\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"prefix\": MASTER_AGENT_PROMPT}\n",
        ")\n",
        "\n",
        "# --- Chatbot Loop (Master Agent) ---\n",
        "def run_master_agent():\n",
        "    print(\"Master Agent Chatbot (type 'exit' to quit)\")\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() == \"exit\":\n",
        "            print(\"Exiting Master Agent Chatbot. Goodbye!\")\n",
        "            break\n",
        "        answer = master_agent.run(user_q)\n",
        "        print(f\"\\nMaster Agent Answer:\\n{answer}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ALdXKe__o1hQ",
        "outputId": "5c97953e-5b83-4888-99b8-fc72dff441ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-358e9363ec6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m# Initialize separate RAGChatbot instances.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0mjpm_chatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_JPM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0mubs_chatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_UBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-358e9363ec6f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever_search_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"retriever_search_k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-358e9363ec6f>\u001b[0m in \u001b[0;36m_setup_llm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Use num_beams=1 to avoid beam search issues that trigger CUDA asserts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         self.pipe = pipeline(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m                 )\n\u001b[0;32m-> 3110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the chatbot object"
      ],
      "metadata": {
        "id": "jaKJRslPWoj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch an interactive  Chatbot session"
      ],
      "metadata": {
        "id": "CLH9TcRtWL85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_master_agent()"
      ],
      "metadata": {
        "id": "g5TI2IMJWLQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}