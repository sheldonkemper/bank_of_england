{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This module implements a Retrieval Augmented Generation (RAG) engine for analyzing bank\n",
        "    quarterly earnings call transcripts (PDF format) stored on Google Drive. It leverages\n",
        "    Langchain and Hugging Face Transformers for document loading, intelligent document chunking,\n",
        "    embedding, and question answering.\n",
        "\n",
        "    Key features:\n",
        "    - Configures an LLM pipeline using GPT-3.5-turbo for interactive Q&A.\n",
        "    - Utilizes sentence-transformer embeddings for semantic vectorization of documents.\n",
        "    - Loads and processes PDF documents from specified Google Drive directories.\n",
        "    - Intelligently chunks documents using RecursiveCharacterTextSplitter with an adjustable\n",
        "      token threshold to avoid splitting when documents already fit within a larger context window.\n",
        "    - Builds and persists two Chroma vector store indexes (raw and summary) in a dedicated folder\n",
        "      (\"gpt_chatbot\"), which also stores the master CSV log file.\n",
        "    - Supports reusing existing persisted indexes to avoid unnecessary re-indexing.\n",
        "    - Implements context retrieval with token truncation to ensure LLM input limits are maintained.\n",
        "    - Manages conversation history using ConversationBufferWindowMemory for interactive sessions.\n",
        "    - Provides both batch and interactive modes for prompt-based querying and logging of Q&A sessions.\n",
        "    - Integrates a ROUGE evaluation method to quantitatively measure summary quality.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "55b1901e-74e8-444b-c733-8e390f04605f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This module implements a Retrieval Augmented Generation (RAG) engine for analyzing bank\\n    quarterly earnings call transcripts (PDF format) stored on Google Drive. It leverages\\n    Langchain and Hugging Face Transformers for document loading, intelligent document chunking,\\n    embedding, and question answering.\\n\\n    Key features:\\n    - Configures an LLM pipeline using GPT-3.5-turbo for interactive Q&A.\\n    - Utilizes sentence-transformer embeddings for semantic vectorization of documents.\\n    - Loads and processes PDF documents from specified Google Drive directories.\\n    - Intelligently chunks documents using RecursiveCharacterTextSplitter with an adjustable\\n      token threshold to avoid splitting when documents already fit within a larger context window.\\n    - Builds and persists two Chroma vector store indexes (raw and summary) in a dedicated folder\\n      (\"gpt_chatbot\"), which also stores the master CSV log file.\\n    - Supports reusing existing persisted indexes to avoid unnecessary re-indexing.\\n    - Implements context retrieval with token truncation to ensure LLM input limits are maintained.\\n    - Manages conversation history using ConversationBufferWindowMemory for interactive sessions.\\n    - Provides both batch and interactive modes for prompt-based querying and logging of Q&A sessions.\\n    - Integrates a ROUGE evaluation method to quantitatively measure summary quality.\\n===================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai chromadb sentence-transformers pypdf datasets rouge-score  > /dev/null 2>&1\n",
        "!pip install --upgrade langchain_community   > /dev/null 2>&1\n",
        "!pip install -U langchain-huggingface  > /dev/null 2>&1\n",
        "!pip install --upgrade openai > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from rouge_score import rouge_scorer\n",
        "import shutil\n",
        "from transformers import AutoTokenizer\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')# Replace with your actual token"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings in production mode if not in DEV_MODE\n",
        "DEV_MODE = False\n",
        "if not DEV_MODE:\n",
        "    warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SI-Sa_fcKhpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    \"pdf_folders\": [\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"gpt-3.5-turbo\",  # Using GPT-3.5-turbo\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 3000,  # Increased to avoid splitting when not needed\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "# We'll use a dedicated folder \"gpt_chatbot\" under the persist directory for saving vector stores and CSV logs.\n",
        "GPT_FOLDER = os.path.join(CONFIG[\"persist_directory\"], \"gpt_chatbot\")\n",
        "os.makedirs(GPT_FOLDER, exist_ok=True)\n",
        "\n",
        "MASTER_AGENT_PROMPT = (\n",
        "    \"You are a highly accurate and detail-oriented assistant specialized in analyzing bank earnings call transcripts.\\n\"\n",
        "    \"ONLY use the information from the retrieved transcript context. Your final answer must be presented as a bullet-point list.\\n\\n\"\n",
        "    \"Follow EXACTLY this format:\\n\"\n",
        "    \"Thought: Briefly explain which transcript sections are relevant.\\n\"\n",
        "    \"Action: Use the appropriate tool (JP_Morgan_RAG or UBS_RAG) by writing e.g. JP_Morgan_RAG(\\\"<query>\\\").\\n\"\n",
        "    \"Observation: Summarize the retrieved context in a few words.\\n\"\n",
        "    \"Final Answer: Provide a concise bullet-point list with the key sentiments and takeaways.\\n\\n\"\n",
        "    \"Now, answer the following question:\\n\"\n",
        "    \"{input}\\n\"\n",
        "    \"Begin!\"\n",
        ")\n",
        "\n",
        "# --- RAGChatbot Class ---\n",
        "class RAGChatbot:\n",
        "    \"\"\"\n",
        "    A RAG chatbot that ingests PDF earnings call transcripts, builds vector stores,\n",
        "    and uses a ConversationalRetrievalChain for Q&A.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, bank: str):\n",
        "        self.config = config\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        self.pdf_folders = config[\"pdf_folders\"]\n",
        "        self.persist_directory = config[\"persist_directory\"]\n",
        "        self.max_length = config[\"max_length\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.chunk_size = config[\"chunk_size\"]\n",
        "        self.chunk_overlap = config[\"chunk_overlap\"]\n",
        "        self.chunk_threshold = config[\"chunk_threshold\"]\n",
        "        self.memory_window_k = config[\"memory_window_k\"]\n",
        "        self.retriever_search_k = config[\"retriever_search_k\"]\n",
        "        self.bank = bank\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_embeddings()\n",
        "        from transformers import AutoTokenizer\n",
        "        self.split_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        self._load_documents()\n",
        "        self._build_vector_store()\n",
        "        self._build_summary_index()\n",
        "        self._setup_retrieval_chain()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=self.config[\"llm_model_name\"],\n",
        "            temperature=self.config[\"temperature\"],\n",
        "            max_tokens=self.max_length,\n",
        "            openai_api_key=self.api_key,\n",
        "            model_kwargs={\"top_p\": self.config[\"top_p\"]}\n",
        "        )\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        emb_model = self.config[\"embedding_model_name\"]\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=emb_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank = os.path.basename(folder).lower()\n",
        "            files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            for file in files:\n",
        "                path = os.path.join(folder, file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank\n",
        "                        doc.metadata[\"source_pdf\"] = file\n",
        "                        doc.metadata[\"year_quarter\"] = \"Unknown\"\n",
        "                    self.documents.extend(docs)\n",
        "                    if DEV_MODE:\n",
        "                        print(f\"Loaded: {file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    if DEV_MODE:\n",
        "                        print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    def _chunk_document(self, doc: Document) -> list[Document]:\n",
        "        tokens = self.split_tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self._remove_duplicates(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    @staticmethod\n",
        "    def _remove_duplicates(chunks: list[Document]) -> list[Document]:\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def _build_vector_store(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings, persist_directory=GPT_FOLDER)\n",
        "        if DEV_MODE:\n",
        "            print(f\"Built raw vector store with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _build_summary_index(self):\n",
        "        all_chunks = []\n",
        "        for doc in self.documents:\n",
        "            all_chunks.extend(self._chunk_document(doc))\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            all_chunks, embedding=self.embeddings,\n",
        "            persist_directory=os.path.join(GPT_FOLDER, \"summaries\"))\n",
        "        if DEV_MODE:\n",
        "            print(f\"Built summary vector index with {len(all_chunks)} chunks.\")\n",
        "\n",
        "    def _setup_retrieval_chain(self):\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=self.memory_window_k, memory_key=\"chat_history\", return_messages=True)\n",
        "        self.retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_db.as_retriever(\n",
        "                search_kwargs={\"k\": self.retriever_search_k, \"filter\": {\"bank\": self.bank}}\n",
        "            ),\n",
        "            memory=memory,\n",
        "            verbose=DEV_MODE\n",
        "        )\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        response = self.retrieval_chain({\"question\": query})\n",
        "        return response.get(\"answer\", \"\").strip()\n",
        "\n",
        "\n",
        "# --- Multi-Agent Instances ---\n",
        "jpm_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"jpmorgan\" in folder.lower()]\n",
        "ubs_folders = [folder for folder in CONFIG[\"pdf_folders\"] if \"ubs\" in folder.lower()]\n",
        "\n",
        "CONFIG_JPM = CONFIG.copy()\n",
        "CONFIG_JPM[\"pdf_folders\"] = jpm_folders\n",
        "\n",
        "CONFIG_UBS = CONFIG.copy()\n",
        "CONFIG_UBS[\"pdf_folders\"] = ubs_folders\n",
        "\n",
        "jpm_chatbot = RAGChatbot(CONFIG_JPM, bank=\"jpmorgan\")\n",
        "ubs_chatbot = RAGChatbot(CONFIG_UBS, bank=\"ubs\")\n",
        "\n",
        "def jpm_tool(query: str) -> str:\n",
        "    return jpm_chatbot.answer_query(query)\n",
        "\n",
        "def ubs_tool(query: str) -> str:\n",
        "    return ubs_chatbot.answer_query(query)\n",
        "\n",
        "jpm_tool_instance = Tool(\n",
        "    name=\"JP_Morgan_RAG\",\n",
        "    func=jpm_tool,\n",
        "    description=\"Answers questions about JP Morgan earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "ubs_tool_instance = Tool(\n",
        "    name=\"UBS_RAG\",\n",
        "    func=ubs_tool,\n",
        "    description=\"Answers questions about UBS earnings call transcripts.\"\n",
        ")\n",
        "\n",
        "master_agent = initialize_agent(\n",
        "    [jpm_tool_instance, ubs_tool_instance],\n",
        "    jpm_chatbot.llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=DEV_MODE,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"prefix\": MASTER_AGENT_PROMPT}\n",
        ")\n",
        "\n",
        "\n",
        "# --- ROUGE Evaluation Method ---\n",
        "def calculate_rouge_scores(generated_summary: str, reference_summary: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calculate and print ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L) between a generated summary and a reference summary.\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    print(\"ROUGE Scores:\")\n",
        "    for key, score in scores.items():\n",
        "        print(f\"{key}: precision={score.precision:.2f}, recall={score.recall:.2f}, fmeasure={score.fmeasure:.2f}\")\n",
        "    return scores\n",
        "\n",
        "\n",
        "# --- Batch Mode Chatbot Loop with CSV Logging ---\n",
        "def run_master_agent_batch(questions: list[str]):\n",
        "    print(\"Master Agent Chatbot Batch Mode (processing multiple questions)\")\n",
        "    csv_file = os.path.join(GPT_FOLDER, \"master_agent_results.csv\")\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        cols = [\"Year/Quarter\", \"Question\", \"Master Answer\", \"Full Output\"] if DEV_MODE else [\"Year/Quarter\", \"Question\", \"Master Answer\"]\n",
        "        df = pd.DataFrame(columns=cols)\n",
        "\n",
        "    for question in questions:\n",
        "        if question in df[\"Question\"].values:\n",
        "            if DEV_MODE:\n",
        "                print(f\"Skipping already processed question: {question}\")\n",
        "            continue\n",
        "\n",
        "        answer = master_agent.run(question)\n",
        "        year_quarter = \"Unknown\"\n",
        "        if DEV_MODE:\n",
        "            full_output = \"Full chain output logged in console.\"\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": question, \"Master Answer\": answer, \"Full Output\": full_output}])\n",
        "        else:\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": question, \"Master Answer\": answer}])\n",
        "\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Master Agent Answer:\\n{answer}\\n\")\n",
        "        print(f\"Results saved to {csv_file}\\n{'-'*60}\")\n",
        "\n",
        "\n",
        "# --- Interactive Mode Chatbot Loop with Follow-up Option ---\n",
        "def run_master_agent_interactive():\n",
        "    \"\"\"\n",
        "    Runs an interactive chatbot loop where you can ask a question and then add follow-up questions\n",
        "    based on the output of previous prompts.\n",
        "    \"\"\"\n",
        "    print(\"Master Agent Chatbot Interactive Mode (type 'exit' to quit)\")\n",
        "    csv_file = os.path.join(GPT_FOLDER, \"master_agent_results.csv\")\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        cols = [\"Year/Quarter\", \"Question\", \"Master Answer\", \"Full Output\"] if DEV_MODE else [\"Year/Quarter\", \"Question\", \"Master Answer\"]\n",
        "        df = pd.DataFrame(columns=cols)\n",
        "\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() == \"exit\":\n",
        "            print(\"Exiting Master Agent Chatbot. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        answer = master_agent.run(user_q)\n",
        "        year_quarter = \"Unknown\"\n",
        "        if DEV_MODE:\n",
        "            full_output = \"Full chain output logged in console.\"\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer, \"Full Output\": full_output}])\n",
        "        else:\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": user_q, \"Master Answer\": answer}])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"\\nMaster Agent Answer:\\n{answer}\\n\")\n",
        "\n",
        "        follow = input(\"Would you like to ask a follow-up question? (yes/no): \")\n",
        "        while follow.lower() == \"yes\":\n",
        "            followup_q = input(\"Follow-up question: \")\n",
        "            followup_answer = master_agent.run(followup_q)\n",
        "            new_row = pd.DataFrame([{\"Year/Quarter\": year_quarter, \"Question\": followup_q, \"Master Answer\": followup_answer}])\n",
        "            df = pd.concat([df, new_row], ignore_index=True)\n",
        "            df.to_csv(csv_file, index=False)\n",
        "            print(f\"\\nFollow-up Answer:\\n{followup_answer}\\n\")\n",
        "            follow = input(\"Would you like to ask another follow-up question? (yes/no): \")\n",
        "\n"
      ],
      "metadata": {
        "id": "eqaqKdZjDbRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage of ROUGE Evaluation ---\n",
        "if __name__ == \"__main__\":\n",
        "  # To run batch mode:\n",
        "  questions = [\n",
        "        \"What are the key market and credit risk factors highlighted in the latest JP Morgan earnings call?\",\n",
        "        \"How are JP Morgan's balance sheet exposures and risk-weighted assets affecting its risk profile?\",\n",
        "        \"What cost pressures or margin compressions were mentioned that could pose risks to JP Morgan's performance?\",\n",
        "        \"What are the primary challenges or risk factors affecting UBS's earnings call, particularly regarding its operating profit?\",\n",
        "        \"How have changes in UBS's revenue mix and expense growth impacted its overall risk profile?\",\n",
        "        \"What steps is UBS taking to manage or mitigate its identified risks, especially related to technology costs?\",\n",
        "        \"For each bank, identify the overall sentiment based on the tone used in the transcripts for the latest quarter.\"\n",
        "    ]\n",
        "    run_master_agent_batch(questions)\n",
        "\n",
        "\n",
        "    # For demonstration: evaluate ROUGE score for a generated summary vs. a reference.\n",
        "    generated_summary = \"• Net income: $13.2B • EPS: $4.33 • Revenue: $40.7B • ROTCE: 22% • First Republic: $2.2B revenue, $858M expenses, $1.1B net income • Basel III Endgame: ~30% RWA increase (~$500B), capital up ~25% • Strategic: conservative capital, moderate buybacks\"\n",
        "    reference_summary = (\n",
        "        \"• Net income: $13.2B\\n\"\n",
        "        \"• EPS: $4.33\\n\"\n",
        "        \"• Revenue: $40.7B\\n\"\n",
        "        \"• ROTCE: 22%\\n\"\n",
        "        \"• First Republic: revenue $2.2B, expenses $858M, net income $1.1B\\n\"\n",
        "        \"• Basel III Endgame: ~30% RWA increase (~$500B), capital up ~25%\\n\"\n",
        "        \"• Strategic: conservative capital, moderate buybacks\"\n",
        "    )\n",
        "    calculate_rouge_scores(generated_summary, reference_summary)"
      ],
      "metadata": {
        "id": "_PmIaMMnPR0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Chatbot interactively\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WwO7TqjU6UR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run_master_agent_interactive()"
      ],
      "metadata": {
        "id": "466sK14Vr0T8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}