{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcCHgfmdAvE9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "47e0875e-10a1-4451-8416-e5cf5da7ce61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install langchain-community\n",
        "!pip install -q langchain-community pypdf tiktoken chromadb sentence-transformers datasets rouge-score huggingface_hub > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import drive\n",
        "# For evaluation metrics (ROUGE)\n",
        "from rouge_score import rouge_scorer\n",
        "from huggingface_hub import hf_hub_download\n",
        "import warnings"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"HF\" # Replace with your actual token"
      ],
      "metadata": {
        "id": "0VgyuhE5N7BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JL-O9CQQAvFB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# class BankEarningsChatbot:\n",
        "#     \"\"\"\n",
        "#     A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine\n",
        "#     designed to analyze bank quarterly earnings call transcripts. It loads PDF documents\n",
        "#     from one or more specified folders, builds a Chroma vector index, and sets up an interactive\n",
        "#     conversational chain for prompt-based queries.\n",
        "\n",
        "#     Parameters:\n",
        "#         pdf_folders (list or str): A list of folder paths containing PDF files, or a single folder path as a string.\n",
        "#         persist_directory (str): Directory path to persist the vector index and model outputs.\n",
        "#         max_length (int): Maximum output length for the T5 model.\n",
        "#         test_mode (bool): If True, only loads one PDF (from the first folder) for quick testing.\n",
        "#         rebuild_index (bool): If True, reprocess all PDFs and rebuild the index even if a persisted index exists.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, pdf_folders,\n",
        "#                  persist_directory=\"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "#                  max_length=256, test_mode=False, rebuild_index=False):\n",
        "#         # Allow a single folder (string) or a list of folders.\n",
        "#         if isinstance(pdf_folders, str):\n",
        "#             self.pdf_folders = [pdf_folders]\n",
        "#         else:\n",
        "#             self.pdf_folders = pdf_folders\n",
        "\n",
        "#         self.persist_directory = persist_directory\n",
        "#         self.test_mode = test_mode\n",
        "\n",
        "#         # Set up the LLM pipeline using the Flan-T5 model.\n",
        "#         self._setup_llm(max_length)\n",
        "\n",
        "#         # Configure embeddings.\n",
        "#         self._setup_embeddings()\n",
        "\n",
        "#         # Check if we need to rebuild the vector index.\n",
        "#         if rebuild_index or (not os.path.exists(self.persist_directory)) or (not os.listdir(self.persist_directory)):\n",
        "#             # Load documents and build the vector index.\n",
        "#             self._load_documents()\n",
        "#             self._build_vector_index()\n",
        "#         else:\n",
        "#             print(\"Loading existing vector index from persistence directory.\")\n",
        "#             self.db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "#             # Note: If you need to update the in-memory index from the persisted data, this method should suffice.\n",
        "\n",
        "#         # Configure retriever from the persisted vector database.\n",
        "#         self._setup_retriever()\n",
        "\n",
        "#         # Initialize conversation memory.\n",
        "#         self.memory = ConversationBufferWindowMemory(k=10, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "#         # Set up the conversational retrieval chain.\n",
        "#         self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "#             llm=self.llm,\n",
        "#             retriever=self.retriever,\n",
        "#             memory=self.memory,\n",
        "#             verbose=False\n",
        "#         )\n",
        "\n",
        "#     def _setup_llm(self, max_length):\n",
        "#         \"\"\"\n",
        "#         Configures the language model pipeline using a T5 model.\n",
        "#         \"\"\"\n",
        "#         self.model_name = \"google/flan-t5-large\"\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "#         self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "#             self.model_name,\n",
        "#             device_map=\"auto\",\n",
        "#             torch_dtype=torch.float16\n",
        "#         )\n",
        "#         # Using the text2text-generation pipeline for T5.\n",
        "#         self.pipe = pipeline(\n",
        "#             \"text2text-generation\",\n",
        "#             model=self.model,\n",
        "#             tokenizer=self.tokenizer,\n",
        "#             max_length=max_length,\n",
        "#             temperature=0.5,\n",
        "#             top_p=0.8,\n",
        "#             do_sample=True\n",
        "#         )\n",
        "#         self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "#     def _setup_embeddings(self):\n",
        "#         \"\"\"\n",
        "#         Initializes the sentence-transformer based embeddings.\n",
        "#         \"\"\"\n",
        "#         self.embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "#         self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "#     def _load_documents(self):\n",
        "#         \"\"\"\n",
        "#         Loads and splits PDF documents from the specified folders.\n",
        "#         In test_mode, only the first PDF (from the first folder) is loaded.\n",
        "#         \"\"\"\n",
        "#         self.documents = []\n",
        "#         for folder in self.pdf_folders:\n",
        "#             file_names = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "#             if not file_names:\n",
        "#                 continue\n",
        "#             # If in test_mode, load only the first PDF file from this folder.\n",
        "#             if self.test_mode:\n",
        "#                 file_names = file_names[:1]\n",
        "#             for pdf_file in file_names:\n",
        "#                 pdf_path = os.path.join(folder, pdf_file)\n",
        "#                 try:\n",
        "#                     loader = PyPDFLoader(pdf_path, extract_images=False)\n",
        "#                     self.documents.extend(loader.load_and_split())\n",
        "#                     print(f\"Loaded: {pdf_file} from {folder}\")\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error loading {pdf_file} from {folder}: {e}\")\n",
        "#             # In test mode, break after processing the first folder.\n",
        "#             if self.test_mode:\n",
        "#                 break\n",
        "\n",
        "#     def _build_vector_index(self):\n",
        "#         \"\"\"\n",
        "#         Chunks documents and builds a Chroma vector index.\n",
        "#         \"\"\"\n",
        "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "#         chunks = text_splitter.split_documents(self.documents)\n",
        "#         # Remove duplicate chunks.\n",
        "#         chunks = self.remove_duplicate_chunks(chunks)\n",
        "#         self.chunks = chunks\n",
        "#         self.db = Chroma.from_documents(chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "#         self.db.persist()\n",
        "\n",
        "#     def _setup_retriever(self):\n",
        "#         \"\"\"\n",
        "#         Initializes the retriever from the persisted vector database.\n",
        "#         \"\"\"\n",
        "#         self.vectordb = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "#         self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "#     @staticmethod\n",
        "#     def remove_duplicate_chunks(chunks):\n",
        "#         \"\"\"\n",
        "#         Eliminates duplicate document chunks based on their content.\n",
        "#         \"\"\"\n",
        "#         seen = set()\n",
        "#         unique_chunks = []\n",
        "#         for chunk in chunks:\n",
        "#             chunk_text = chunk.page_content.strip()\n",
        "#             if chunk_text not in seen:\n",
        "#                 seen.add(chunk_text)\n",
        "#                 unique_chunks.append(chunk)\n",
        "#         return unique_chunks\n",
        "\n",
        "#     def truncate_context(self, context_list, max_tokens=800):\n",
        "#         \"\"\"\n",
        "#         Truncates the retrieved context to avoid overloading the model's input.\n",
        "#         \"\"\"\n",
        "#         truncated_docs = []\n",
        "#         current_tokens = 0\n",
        "#         for doc in context_list:\n",
        "#             doc_tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "#             if current_tokens + doc_tokens <= max_tokens:\n",
        "#                 truncated_docs.append(doc)\n",
        "#                 current_tokens += doc_tokens\n",
        "#             else:\n",
        "#                 break\n",
        "#         return truncated_docs\n",
        "\n",
        "#     @staticmethod\n",
        "#     def clean_user_input(user_input):\n",
        "#         \"\"\"\n",
        "#         Cleans and standardizes user input.\n",
        "#         \"\"\"\n",
        "#         return user_input.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "#     def reset_memory_if_needed(self):\n",
        "#         \"\"\"\n",
        "#         Clears conversation history if the number of exchanges exceeds a threshold.\n",
        "#         \"\"\"\n",
        "#         if len(self.memory.chat_memory.messages) > 6:\n",
        "#             print(\"\\nMemory Full: Resetting Conversation History...\\n\")\n",
        "#             self.memory.clear()\n",
        "\n",
        "#     def format_response(self, question, response):\n",
        "#         \"\"\"\n",
        "#         Formats the output to clearly present both the question and the answer.\n",
        "#         \"\"\"\n",
        "#         response_text = response.strip()\n",
        "#         unwanted_phrases = [\n",
        "#             \"Use the following pieces of context\",\n",
        "#             \"If you don't know the answer, just say that you don't know\",\n",
        "#             \"Don't try to make up an answer.\"\n",
        "#         ]\n",
        "#         for phrase in unwanted_phrases:\n",
        "#             if phrase in response_text:\n",
        "#                 response_text = response_text.split(phrase)[-1].strip()\n",
        "#         return f\"Question: {question}\\nHelpful Answer: {response_text}\"\n",
        "\n",
        "#     def trim_final_input(self, question, context, max_tokens=512):\n",
        "#       \"\"\"\n",
        "#       Truncates the final input to meet the token limit, preserving document metadata.\n",
        "#       \"\"\"\n",
        "#       system_message = (\n",
        "#           \"You are analyzing a bank's quarterly earnings call transcript. \"\n",
        "#           \"Provide a bullet-point summary of the most important takeaways with specific details: \"\n",
        "#           \"list key revenue trends (include any percentage changes if available), major expense drivers, \"\n",
        "#           \"and management's outlook for the future. If numerical details are not available, provide qualitative insights.\"\n",
        "#       )\n",
        "#       # Join the context documents into one coherent string.\n",
        "#       context_str = \"\\n\".join([doc.page_content for doc in context])\n",
        "#       input_text = f\"{system_message}\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "#       tokens = self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens)\n",
        "#       return self.tokenizer.decode(tokens)\n",
        "\n",
        "\n",
        "#     def answer_question(self, question):\n",
        "#         \"\"\"\n",
        "#         Processes the user query: retrieves context, prepares the prompt,\n",
        "#         and returns a formatted answer. If no relevant documents are retrieved,\n",
        "#         a fallback message is returned.\n",
        "#         \"\"\"\n",
        "#         question = self.clean_user_input(question)\n",
        "#         self.reset_memory_if_needed()\n",
        "\n",
        "#         # Retrieve and process context.\n",
        "#         context = self.retriever.get_relevant_documents(question)\n",
        "#         context = self.remove_duplicate_chunks(context)\n",
        "#         context = self.truncate_context(context, max_tokens=800)\n",
        "\n",
        "#         # Fallback: if no relevant context is found.\n",
        "#         if not context:\n",
        "#             return f\"Question: {question}\\nHelpful Answer: I don't have information regarding that query.\"\n",
        "\n",
        "#         print(\"\\nRetrieved Context:\")\n",
        "#         for doc in context:\n",
        "#             source = doc.metadata.get('source', 'Unknown Source')\n",
        "#             page = doc.metadata.get('page', 'Unknown Page')\n",
        "#             print(f\"- Source: {source}, Page: {page}\")\n",
        "\n",
        "#         # Enforce a 512-token limit for the final prompt.\n",
        "#         formatted_input = self.trim_final_input(question, context, max_tokens=512)\n",
        "#         response = self.qa_chain({\"question\": formatted_input})\n",
        "#         return self.format_response(question, response['answer'])\n",
        "\n",
        "#     def run_chatbot(self):\n",
        "#         \"\"\"\n",
        "#         Initiates an interactive loop for prompt-based queries.\n",
        "#         \"\"\"\n",
        "#         print(\"\\nðŸ’¬ Bank Earnings Chatbot (Type 'exit' to stop)\")\n",
        "#         while True:\n",
        "#             user_input = input(\"\\nYou: \")\n",
        "#             if user_input.lower() == \"exit\":\n",
        "#                 print(\"\\nExiting Chatbot. Have a great day!\")\n",
        "#                 break\n",
        "#             answer = self.answer_question(user_input)\n",
        "#             print(\"\\n\" + answer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ----------------------------\n",
        "# # Example usage of the BankEarningsChatbot class with T5, multiple data sources, and test mode enabled\n",
        "# # ----------------------------\n",
        "\n",
        "# # Define your PDF folder paths (ensure these paths contain your earnings transcripts in PDF format).\n",
        "# pdf_folders = [\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "# ]\n",
        "\n",
        "# # Define the persistence directory for model outputs and the vector index.\n",
        "# persist_directory = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\n",
        "\n",
        "# # Instantiate the chatbot object.\n",
        "# # Set rebuild_index=True if you want to force re-indexing, otherwise it will load the persisted index if it exists.\n",
        "# chatbot = BankEarningsChatbot(pdf_folders, persist_directory=persist_directory, test_mode=False, rebuild_index=True)"
      ],
      "metadata": {
        "id": "mr-5lzazWgqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import re\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "# from datasets import Dataset  # For batch processing\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from langchain.memory import ConversationBufferWindowMemory\n",
        "# from langchain_community.llms import HuggingFacePipeline\n",
        "# from langchain.chains import ConversationalRetrievalChain\n",
        "# from langchain.docstore.document import Document\n",
        "\n",
        "# # For evaluation metrics (ROUGE)\n",
        "# from rouge_score import rouge_scorer\n",
        "\n",
        "# class BankEarningsChatbotTwoStage:\n",
        "#     \"\"\"\n",
        "#     A two-stage RAG approach using LED (long-context model):\n",
        "#       - Stage 1: Summarize each document (or chunk, if needed) and store summaries in a separate vector store.\n",
        "#       - Stage 2: For each user query, retrieve relevant summaries and perform final Q&A.\n",
        "#       - Also includes an evaluation method for summarization quality (using ROUGE).\n",
        "#     \"\"\"\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         pdf_folders,\n",
        "#         persist_directory=\"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "#         max_length=1024,  # LED supports up to 16k tokens; adjust as needed.\n",
        "#         test_mode=False,\n",
        "#         rebuild_index=False,\n",
        "#         verbose=False,\n",
        "#         chunk_size=1000,\n",
        "#         chunk_overlap=100,\n",
        "#         chunk_threshold=1024  # If a doc's token count <= threshold, do not split it.\n",
        "#     ):\n",
        "#         if isinstance(pdf_folders, str):\n",
        "#             self.pdf_folders = [pdf_folders]\n",
        "#         else:\n",
        "#             self.pdf_folders = pdf_folders\n",
        "\n",
        "#         self.persist_directory = persist_directory\n",
        "#         self.test_mode = test_mode\n",
        "#         self.verbose = verbose\n",
        "#         self.chunk_size = chunk_size\n",
        "#         self.chunk_overlap = chunk_overlap\n",
        "#         self.chunk_threshold = chunk_threshold  # New parameter\n",
        "\n",
        "#         # 1) Set up the LLM pipeline using LED.\n",
        "#         self._setup_llm(max_length)\n",
        "#         # 2) Set up embeddings.\n",
        "#         self._setup_embeddings()\n",
        "\n",
        "#         # 3) Load raw documents and (optionally) build a raw vector index.\n",
        "#         if rebuild_index or (not os.path.exists(self.persist_directory)) or (not os.listdir(self.persist_directory)):\n",
        "#             self._load_documents()\n",
        "#             self._build_raw_vector_index()\n",
        "#         else:\n",
        "#             if self.verbose:\n",
        "#                 print(\"Loading existing raw vector index from persistence directory.\")\n",
        "#             self.raw_db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "\n",
        "#         # 4) Build the summary vector index (Stage 1).\n",
        "#         self.summary_persist_dir = os.path.join(self.persist_directory, \"summaries\")\n",
        "#         if rebuild_index or (not os.path.exists(self.summary_persist_dir)) or (not os.listdir(self.summary_persist_dir)):\n",
        "#             if not hasattr(self, \"documents\"):\n",
        "#                 self._load_documents()\n",
        "#             self._build_summary_vector_index()\n",
        "#         else:\n",
        "#             if self.verbose:\n",
        "#                 print(\"Loading existing summary vector index from 'summaries' directory.\")\n",
        "#             self.summary_db = Chroma(persist_directory=self.summary_persist_dir, embedding_function=self.embeddings)\n",
        "\n",
        "#         # 5) Create a retriever for the summary DB.\n",
        "#         self._setup_summary_retriever()\n",
        "\n",
        "#         # 6) Create conversation memory.\n",
        "#         self.memory = ConversationBufferWindowMemory(k=10, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "#         # 7) Create the final Q&A chain (Stage 2) using the summary retriever.\n",
        "#         self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "#             llm=self.llm,\n",
        "#             retriever=self.summary_retriever,\n",
        "#             memory=self.memory,\n",
        "#             verbose=False\n",
        "#         )\n",
        "\n",
        "#     def _setup_llm(self, max_length):\n",
        "#         # Use LED which supports a larger context window.\n",
        "#         self.model_name = \"allenai/led-base-16384\"\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "#         self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "#             self.model_name,\n",
        "#             torch_dtype=torch.float16\n",
        "#         )\n",
        "#         if torch.cuda.is_available():\n",
        "#             self.model.to(\"cuda\")\n",
        "#         self.pipe = pipeline(\n",
        "#             \"text2text-generation\",\n",
        "#             model=self.model,\n",
        "#             tokenizer=self.tokenizer,\n",
        "#             max_length=max_length,\n",
        "#             temperature=0.1,\n",
        "#             top_p=0.8,\n",
        "#             do_sample=True,\n",
        "#             batch_size=8\n",
        "#         )\n",
        "#         self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "#     def _setup_embeddings(self):\n",
        "#         self.embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "#         self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "#     def _load_documents(self):\n",
        "#         self.documents = []\n",
        "#         total_files_loaded = 0\n",
        "#         for folder in self.pdf_folders:\n",
        "#             bank_name = os.path.basename(folder).lower()\n",
        "#             file_names = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "#             if not file_names:\n",
        "#                 continue\n",
        "#             if self.test_mode:\n",
        "#                 file_names = file_names[:1]\n",
        "#             for pdf_file in file_names:\n",
        "#                 pdf_path = os.path.join(folder, pdf_file)\n",
        "#                 try:\n",
        "#                     loader = PyPDFLoader(pdf_path, extract_images=False)\n",
        "#                     docs = loader.load_and_split()\n",
        "#                     for doc in docs:\n",
        "#                         doc.metadata[\"bank\"] = bank_name\n",
        "#                         doc.metadata[\"source_pdf\"] = pdf_file\n",
        "#                     self.documents.extend(docs)\n",
        "#                     total_files_loaded += 1\n",
        "#                     if self.verbose:\n",
        "#                         print(f\"Loaded: {pdf_file} from {folder}\")\n",
        "#                 except Exception as e:\n",
        "#                     if self.verbose:\n",
        "#                         print(f\"Error loading {pdf_file} from {folder}: {e}\")\n",
        "#             if self.test_mode:\n",
        "#                 break\n",
        "#         if self.verbose:\n",
        "#             print(f\"Total PDF files loaded: {total_files_loaded}\")\n",
        "\n",
        "#     def _build_raw_vector_index(self):\n",
        "#         # Instead of always chunking, check if a documentâ€™s token length exceeds our threshold.\n",
        "#         processed_docs = []\n",
        "#         for doc in self.documents:\n",
        "#             tokens = self.tokenizer.encode(doc.page_content)\n",
        "#             if len(tokens) > self.chunk_threshold:\n",
        "#                 # Use chunking\n",
        "#                 splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "#                 chunks = splitter.split_documents([doc])\n",
        "#                 processed_docs.extend(self.remove_duplicate_chunks(chunks))\n",
        "#             else:\n",
        "#                 processed_docs.append(doc)\n",
        "#         self.raw_db = Chroma.from_documents(\n",
        "#             processed_docs,\n",
        "#             embedding=self.embeddings,\n",
        "#             persist_directory=self.persist_directory\n",
        "#         )\n",
        "#         self.raw_db.persist()\n",
        "\n",
        "#     def _build_summary_vector_index(self):\n",
        "#         # Stage 1: Chunk (if necessary) the documents.\n",
        "#         processed_docs = []\n",
        "#         for doc in self.documents:\n",
        "#             tokens = self.tokenizer.encode(doc.page_content)\n",
        "#             if len(tokens) > self.chunk_threshold:\n",
        "#                 splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "#                 chunks = splitter.split_documents([doc])\n",
        "#                 processed_docs.extend(self.remove_duplicate_chunks(chunks))\n",
        "#             else:\n",
        "#                 processed_docs.append(doc)\n",
        "\n",
        "#         # Stage 2: Summarize each processed document/chunk.\n",
        "#         prompts = []\n",
        "#         for d in processed_docs:\n",
        "#             prompt = (\n",
        "#                 \"Summarize the following text in bullet points, focusing on key financials, sentiment, \"\n",
        "#                 \"and forward-looking statements:\\n\\n\" + d.page_content\n",
        "#             )\n",
        "#             prompts.append(prompt)\n",
        "\n",
        "#         if self.verbose:\n",
        "#             print(f\"Summarizing {len(prompts)} documents/chunks...\")\n",
        "\n",
        "#         # Use a Dataset for batch processing.\n",
        "#         ds = Dataset.from_dict({\"text\": prompts})\n",
        "#         summary_responses = self.llm.generate(ds[\"text\"])\n",
        "\n",
        "#         summary_docs = []\n",
        "#         for orig_doc, resp in zip(processed_docs, summary_responses):\n",
        "#             if isinstance(resp, dict) and 'answer' in resp:\n",
        "#                 summary_text = resp['answer']\n",
        "#             else:\n",
        "#                 summary_text = str(resp)\n",
        "#             summary_doc = Document(\n",
        "#                 page_content=summary_text,\n",
        "#                 metadata={\n",
        "#                     \"source_pdf\": orig_doc.metadata.get(\"source_pdf\", \"Unknown\"),\n",
        "#                     \"bank\": orig_doc.metadata.get(\"bank\", \"unknown\"),\n",
        "#                     \"orig_chunk\": orig_doc.page_content[:50]\n",
        "#                 }\n",
        "#             )\n",
        "#             summary_docs.append(summary_doc)\n",
        "\n",
        "#         self.summary_db = Chroma.from_documents(\n",
        "#             summary_docs,\n",
        "#             embedding=self.embeddings,\n",
        "#             persist_directory=os.path.join(self.persist_directory, \"summaries\")\n",
        "#         )\n",
        "#         self.summary_db.persist()\n",
        "#         if self.verbose:\n",
        "#             print(f\"Built summary vector index with {len(summary_docs)} summarized docs.\")\n",
        "\n",
        "#     def _setup_summary_retriever(self):\n",
        "#         self.summary_retriever = self.summary_db.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "#     @staticmethod\n",
        "#     def remove_duplicate_chunks(chunks):\n",
        "#         seen = set()\n",
        "#         unique = []\n",
        "#         for chunk in chunks:\n",
        "#             text = chunk.page_content.strip()\n",
        "#             if text not in seen:\n",
        "#                 seen.add(text)\n",
        "#                 unique.append(chunk)\n",
        "#         return unique\n",
        "\n",
        "#     def truncate_context(self, context_list, max_tokens=800):\n",
        "#         truncated = []\n",
        "#         current = 0\n",
        "#         for doc in context_list:\n",
        "#             doc_tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "#             if current + doc_tokens <= max_tokens:\n",
        "#                 truncated.append(doc)\n",
        "#                 current += doc_tokens\n",
        "#             else:\n",
        "#                 break\n",
        "#         return truncated\n",
        "\n",
        "#     @staticmethod\n",
        "#     def clean_user_input(user_input):\n",
        "#         return user_input.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "#     def reset_memory_if_needed(self):\n",
        "#         if len(self.memory.chat_memory.messages) > 6:\n",
        "#             if self.verbose:\n",
        "#                 print(\"\\nMemory Full: Resetting Conversation History...\\n\")\n",
        "#             self.memory.clear()\n",
        "\n",
        "#     def format_response(self, question, response):\n",
        "#         if isinstance(response, dict) and 'answer' in response:\n",
        "#             resp_text = response['answer'].strip()\n",
        "#         else:\n",
        "#             resp_text = str(response).strip()\n",
        "#         for phrase in [\n",
        "#             \"Use the following pieces of context\",\n",
        "#             \"If you don't know the answer, just say that you don't know\",\n",
        "#             \"Don't try to make up an answer.\"\n",
        "#         ]:\n",
        "#             if phrase in resp_text:\n",
        "#                 resp_text = resp_text.split(phrase)[-1].strip()\n",
        "#         return f\"Question: {question}\\nHelpful Answer: {resp_text}\"\n",
        "\n",
        "#     def trim_final_input(self, question, context, max_tokens=512):\n",
        "#         system_message = (\n",
        "#             \"You are analyzing a bank's quarterly earnings call transcript. \"\n",
        "#             \"Provide a bullet-point summary of the most important takeaways with specific details: \"\n",
        "#             \"list key revenue trends (include any percentage changes if available), major expense drivers, \"\n",
        "#             \"and management's outlook for the future. If numerical details are not available, provide qualitative insights.\"\n",
        "#         )\n",
        "#         # Batch intermediate summarization for each context document.\n",
        "#         summarized_context = []\n",
        "#         per_doc_limit = max_tokens // max(1, len(context))\n",
        "#         batch_prompts = []\n",
        "#         for doc in context:\n",
        "#             doc_text = doc.page_content\n",
        "#             tokens = self.tokenizer.encode(doc_text)\n",
        "#             if len(tokens) > per_doc_limit:\n",
        "#                 summary_prompt = f\"Summarize the following text in a concise bullet-point format:\\n\\n{doc_text}\"\n",
        "#                 batch_prompts.append(summary_prompt)\n",
        "#             else:\n",
        "#                 summarized_context.append(doc_text)\n",
        "#         if batch_prompts:\n",
        "#             ds = Dataset.from_dict({\"text\": batch_prompts})\n",
        "#             summary_responses = self.llm.generate(ds[\"text\"])\n",
        "#             for resp in summary_responses:\n",
        "#                 if isinstance(resp, dict) and 'answer' in resp:\n",
        "#                     summarized_context.append(resp['answer'])\n",
        "#                 else:\n",
        "#                     summarized_context.append(str(resp))\n",
        "#         context_str = \"\\n\".join(summarized_context)\n",
        "#         input_text = f\"{system_message}\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "#         token_length = len(self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens))\n",
        "#         if self.verbose:\n",
        "#             print(f\"Final input token length: {token_length}\")\n",
        "#         tokens = self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens)\n",
        "#         return self.tokenizer.decode(tokens)\n",
        "\n",
        "#     def answer_question(self, question: str) -> str:\n",
        "#         self.reset_memory_if_needed()\n",
        "#         response_dict = self.qa_chain({\"question\": question})\n",
        "#         final_answer = response_dict[\"answer\"].strip()\n",
        "#         return f\"Question: {question}\\nHelpful Answer: {final_answer}\"\n",
        "\n",
        "#     def run_chatbot(self):\n",
        "#         print(\"\\nðŸ’¬ Bank Earnings Chatbot - Two Stage (Type 'exit' to stop)\")\n",
        "#         while True:\n",
        "#             user_input = input(\"\\nYou: \")\n",
        "#             if user_input.lower() == \"exit\":\n",
        "#                 print(\"\\nExiting Chatbot. Have a great day!\")\n",
        "#                 break\n",
        "#             answer = self.answer_question(user_input)\n",
        "#             print(\"\\n\" + answer)\n",
        "\n",
        "#     # ---------------- Evaluation Methods ----------------\n",
        "#     def evaluate_summaries(self, generated_summaries, reference_summaries, verbose=False):\n",
        "#         scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "#         eval_scores = {}\n",
        "#         for key, gen_dict in generated_summaries.items():\n",
        "#             gen_summary = gen_dict.get(\"summary\", \"\")\n",
        "#             if key in reference_summaries:\n",
        "#                 ref_summary = reference_summaries[key]\n",
        "#                 scores = scorer.score(ref_summary, gen_summary)\n",
        "#                 eval_scores[key] = scores\n",
        "#                 if verbose:\n",
        "#                     print(f\"Evaluation for {key}: {scores}\")\n",
        "#             else:\n",
        "#                 if verbose:\n",
        "#                     print(f\"No reference summary provided for {key}.\")\n",
        "#         return eval_scores\n",
        "\n",
        "#     def summarize_individual_transcripts(self):\n",
        "#         prompts = []\n",
        "#         sources = []\n",
        "#         for doc in self.documents:\n",
        "#             source = doc.metadata.get('source_pdf', 'Unknown Source')\n",
        "#             bank = doc.metadata.get('bank', 'unknown')\n",
        "#             transcript_text = doc.page_content\n",
        "#             prompt = f\"Please provide a bullet-point sentiment summary for the following transcript:\\n\\n{transcript_text}\"\n",
        "#             prompts.append(prompt)\n",
        "#             sources.append((source, bank))\n",
        "#         if self.verbose:\n",
        "#             print(f\"Batch summarizing {len(prompts)} transcripts...\")\n",
        "#         ds = Dataset.from_dict({\"text\": prompts})\n",
        "#         responses = self.llm.generate(ds[\"text\"])\n",
        "#         summaries = {}\n",
        "#         for (source, bank), response in zip(sources, responses):\n",
        "#             if isinstance(response, dict) and 'answer' in response:\n",
        "#                 answer_text = response['answer']\n",
        "#             else:\n",
        "#                 answer_text = str(response)\n",
        "#             summaries[source] = {\"bank\": bank, \"summary\": answer_text}\n",
        "#             if self.verbose:\n",
        "#                 print(f\"Summarized transcript from {source}\")\n",
        "#         return summaries\n",
        "\n",
        "#     def group_summaries_by_bank_and_quarter(self, summaries):\n",
        "#         grouped = {}\n",
        "#         for source, info in summaries.items():\n",
        "#             bank = info[\"bank\"]\n",
        "#             summary = info[\"summary\"]\n",
        "#             match = re.search(\n",
        "#                 r'((\\d{1}[qQ]|[qQ]\\d)|((first|second|third|fourth)[-_ ]?quarter))[-_ ]?(\\d{2,4})',\n",
        "#                 source,\n",
        "#                 re.IGNORECASE\n",
        "#             )\n",
        "#             if match:\n",
        "#                 if match.group(2):\n",
        "#                     quarter_raw = match.group(2).lower()\n",
        "#                     quarter_digit_match = re.search(r'\\d', quarter_raw)\n",
        "#                     quarter_num = quarter_digit_match.group(0) if quarter_digit_match else \"1\"\n",
        "#                 elif match.group(4):\n",
        "#                     word = match.group(4).lower()\n",
        "#                     mapping = {\"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\"}\n",
        "#                     quarter_num = mapping.get(word, \"1\")\n",
        "#                 else:\n",
        "#                     quarter_num = \"1\"\n",
        "#                 year_str = match.group(5)\n",
        "#                 year_val = int(year_str)\n",
        "#                 if year_val < 100:\n",
        "#                     year_val += 2000\n",
        "#                 key = f\"{year_val}-Q{quarter_num}\"\n",
        "#             else:\n",
        "#                 key = \"Unknown\"\n",
        "#             if bank not in grouped:\n",
        "#                 grouped[bank] = {}\n",
        "#             if key not in grouped[bank]:\n",
        "#                 grouped[bank][key] = []\n",
        "#             grouped[bank][key].append(summary)\n",
        "#         return grouped\n",
        "\n",
        "#     def aggregate_quarterly_summaries_by_bank(self, grouped_quarterly):\n",
        "#         quarterly_aggregates = {}\n",
        "#         for bank, quarters in grouped_quarterly.items():\n",
        "#             quarterly_aggregates[bank] = {}\n",
        "#             for key, summaries in quarters.items():\n",
        "#                 combined = \"\\n\".join(summaries)\n",
        "#                 prompt = (f\"Based on the following quarterly sentiment summaries for {bank.upper()} ({key}), \"\n",
        "#                           \"provide a concise bullet-point overview of the overall sentiment for that quarter:\\n\\n\" + combined)\n",
        "#                 response = self.llm(prompt)\n",
        "#                 if isinstance(response, dict) and 'answer' in response:\n",
        "#                     answer_text = response['answer']\n",
        "#                 else:\n",
        "#                     answer_text = str(response)\n",
        "#                 quarterly_aggregates[bank][key] = answer_text\n",
        "#         return quarterly_aggregates\n",
        "\n",
        "#     def forecast_next_quarter_sentiment(self, bank, historical_quarterly):\n",
        "#         combined = \"\\n\".join(historical_quarterly)\n",
        "#         prompt = (\n",
        "#             f\"Based on the following historical quarterly sentiment summaries for {bank.upper()}, \"\n",
        "#             \"forecast the overall sentiment for the next quarter. Provide a bullet-point summary of the expected trends, \"\n",
        "#             \"including any changes in tone, risk factors, or optimism:\\n\\n\" + combined\n",
        "#         )\n",
        "#         response = self.llm(prompt)\n",
        "#         if isinstance(response, dict) and 'answer' in response:\n",
        "#             return response['answer']\n",
        "#         return str(response)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def parse_quarter_key(key):\n",
        "#         try:\n",
        "#             parts = key.split(\"-\")\n",
        "#             year = int(parts[0])\n",
        "#             quarter = int(re.search(r'\\d', parts[1]).group(0))\n",
        "#             return year, quarter\n",
        "#         except Exception:\n",
        "#             return (0, 0)\n",
        "\n",
        "#     def analyze_and_forecast_sentiment_by_bank(self):\n",
        "#         print(\"Generating individual transcript summaries...\")\n",
        "#         summaries = self.summarize_individual_transcripts()\n",
        "#         print(\"Grouping summaries by bank and quarter...\")\n",
        "#         grouped_quarterly = self.group_summaries_by_bank_and_quarter(summaries)\n",
        "#         print(\"Aggregating quarterly summaries...\")\n",
        "#         quarterly_aggregates = self.aggregate_quarterly_summaries_by_bank(grouped_quarterly)\n",
        "\n",
        "#         analysis = {}\n",
        "#         for bank, quarters in quarterly_aggregates.items():\n",
        "#             analysis[bank] = {}\n",
        "#             valid_keys = [k for k in quarters.keys() if k != \"Unknown\"]\n",
        "#             if not valid_keys:\n",
        "#                 print(f\"No valid quarter keys found for {bank}: {valid_keys}\")\n",
        "#                 continue\n",
        "#             sorted_keys = sorted(valid_keys, key=lambda k: self.parse_quarter_key(k))\n",
        "#             most_recent_key = sorted_keys[-1]\n",
        "#             current_year, current_quarter = self.parse_quarter_key(most_recent_key)\n",
        "\n",
        "#             years = sorted({self.parse_quarter_key(k)[0] for k in quarters if k != \"Unknown\"})\n",
        "#             previous_year = max([y for y in years if y < current_year], default=None)\n",
        "#             previous_year_summaries = []\n",
        "#             if previous_year is not None:\n",
        "#                 for key in quarters:\n",
        "#                     year, _ = self.parse_quarter_key(key)\n",
        "#                     if year == previous_year:\n",
        "#                         previous_year_summaries.extend(quarters[key])\n",
        "#                 combined_prev = \"\\n\".join(previous_year_summaries)\n",
        "#                 prompt_prev = (f\"Based on the following sentiment summaries for all quarters in {previous_year} for {bank.upper()}, \"\n",
        "#                                \"provide a bullet-point summary of the overall sentiment trends for that year:\\n\\n\" + combined_prev)\n",
        "#                 response_prev = self.llm(prompt_prev)\n",
        "#                 if isinstance(response_prev, dict) and 'answer' in response_prev:\n",
        "#                     previous_year_summary = response_prev['answer']\n",
        "#                 else:\n",
        "#                     previous_year_summary = str(response_prev)\n",
        "#             else:\n",
        "#                 previous_year_summary = \"Not available\"\n",
        "\n",
        "#             current_summary = quarters.get(most_recent_key, \"Not available\")\n",
        "\n",
        "#             historical = []\n",
        "#             for key in sorted_keys:\n",
        "#                 historical.extend(quarters[key])\n",
        "#             forecast = self.forecast_next_quarter_sentiment(bank, historical) if historical else \"Not available\"\n",
        "\n",
        "#             analysis[bank] = {\n",
        "#                 \"previous_year_summary\": previous_year_summary,\n",
        "#                 \"current_quarter_summary\": current_summary,\n",
        "#                 \"forecast_next_quarter\": forecast,\n",
        "#                 \"most_recent_key\": most_recent_key\n",
        "#             }\n",
        "#         return analysis\n",
        "\n",
        "#     # ---------------- Example Usage and Evaluation ----------------\n",
        "#     def evaluate_summaries(self, generated_summaries, reference_summaries, verbose=False):\n",
        "#         scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "#         eval_scores = {}\n",
        "#         for key, gen_dict in generated_summaries.items():\n",
        "#             gen_summary = gen_dict.get(\"summary\", \"\")\n",
        "#             if key in reference_summaries:\n",
        "#                 ref_summary = reference_summaries[key]\n",
        "#                 scores = scorer.score(ref_summary, gen_summary)\n",
        "#                 eval_scores[key] = scores\n",
        "#                 if verbose:\n",
        "#                     print(f\"Evaluation for {key}: {scores}\")\n",
        "#             else:\n",
        "#                 if verbose:\n",
        "#                     print(f\"No reference summary provided for {key}.\")\n",
        "#         return eval_scores\n",
        "\n",
        "\n",
        "# # ----------------------------\n",
        "# # Example usage:\n",
        "# pdf_folders = [\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "# ]\n",
        "# persist_directory = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\n",
        "\n",
        "# chatbot = BankEarningsChatbotTwoStage(\n",
        "#     pdf_folders,\n",
        "#     persist_directory=persist_directory,\n",
        "#     test_mode=False,\n",
        "#     rebuild_index=True,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# analysis = chatbot.analyze_and_forecast_sentiment_by_bank()\n",
        "\n",
        "# print(\"Yearly and Quarterly Sentiment Analysis by Bank:\")\n",
        "# for bank, data in analysis.items():\n",
        "#     print(f\"{bank.upper()} Analysis:\")\n",
        "#     print(f\"Most Recent Quarter Key: {data.get('most_recent_key', 'N/A')}\")\n",
        "#     print(\"Previous Year Sentiment Summary:\")\n",
        "#     print(data[\"previous_year_summary\"])\n",
        "#     print(\"Current Quarter Sentiment Summary:\")\n",
        "#     print(data[\"current_quarter_summary\"])\n",
        "#     print(\"Forecast for Next Quarter:\")\n",
        "#     print(data[\"forecast_next_quarter\"])\n",
        "#     print(\"-\" * 60)\n",
        "\n",
        "# # ----------------------------\n",
        "# # Evaluation Example:\n",
        "# # Suppose you have reference summaries in a dictionary:\n",
        "# reference_summaries = {\n",
        "#     \"1q23-earnings-transcript.pdf\": \"Reference summary for 1Q23 transcript...\",\n",
        "#     \"4q24-earnings-call-remarks.pdf\": \"Reference summary for 4Q24 transcript...\"\n",
        "#     # Add more as available.\n",
        "# }\n",
        "\n",
        "# generated_summaries = chatbot.summarize_individual_transcripts()\n",
        "# evaluation_results = chatbot.evaluate_summaries(generated_summaries, reference_summaries, verbose=True)\n",
        "# print(\"Evaluation Results (ROUGE):\")\n",
        "# for key, scores in evaluation_results.items():\n",
        "#     print(f\"{key}: {scores}\")\n"
      ],
      "metadata": {
        "id": "c4MfUHIQQcSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import Dataset  # For batch processing\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# For evaluation metrics (ROUGE)\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# --- Configuration Section ---\n",
        "CHATBOT_CONFIG = {\n",
        "    \"pdf_folders\": [  # Example folders, replace with your actual paths\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "        \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "    ],\n",
        "    \"persist_directory\": \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "    \"llm_model_name\": \"google/bigbird-pegasus-large\",  # Updated to BigBird-Pegasus\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"text_generation_pipeline_task\": \"text2text-generation\",\n",
        "    \"max_length\": 1024,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.8,\n",
        "    \"batch_size\": 8,\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"chunk_threshold\": 1024,\n",
        "    \"memory_window_k\": 10,\n",
        "    \"retriever_search_k\": 5\n",
        "}\n",
        "\n",
        "\n",
        "class BankEarningsChatbotTwoStage:\n",
        "    \"\"\"\n",
        "    A two-stage RAG chatbot for analyzing bank earnings call transcripts using BigBird-Pegasus.\n",
        "\n",
        "    Stage 1: Summarizes each document (or chunk if needed) and stores summaries in a vector store.\n",
        "    Stage 2: Retrieves relevant summaries for user queries and performs final Q&A.\n",
        "\n",
        "    Includes evaluation methods for summarization quality using ROUGE.\n",
        "    \"\"\"\n",
        "\n",
        "    BIGBIRD_PEGASUS_MODEL_NAME = CHATBOT_CONFIG[\"llm_model_name\"] # Changed to BIGBIRD_PEGASUS_MODEL_NAME\n",
        "    EMBEDDING_MODEL_NAME = CHATBOT_CONFIG[\"embedding_model_name\"]\n",
        "    TEXT_GENERATION_TASK = CHATBOT_CONFIG[\"text_generation_pipeline_task\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pdf_folders: list[str],\n",
        "        persist_directory: str = CHATBOT_CONFIG[\"persist_directory\"],\n",
        "        max_length: int = CHATBOT_CONFIG[\"max_length\"],\n",
        "        test_mode: bool = False,\n",
        "        rebuild_index: bool = False,\n",
        "        verbose: bool = False,\n",
        "        chunk_size: int = CHATBOT_CONFIG[\"chunk_size\"],\n",
        "        chunk_overlap: int = CHATBOT_CONFIG[\"chunk_overlap\"],\n",
        "        chunk_threshold: int = CHATBOT_CONFIG[\"chunk_threshold\"]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the BankEarningsChatbotTwoStage with BigBird-Pegasus.\n",
        "\n",
        "        Args:\n",
        "            pdf_folders (list[str]): List of paths to folders containing PDF transcripts.\n",
        "            persist_directory (str, optional): Directory to persist vector stores. Defaults to CHATBOT_CONFIG[\"persist_directory\"].\n",
        "            max_length (int, optional): Maximum length for text generation. Defaults to CHATBOT_CONFIG[\"max_length\"].\n",
        "            test_mode (bool, optional): If True, limits processing for faster testing. Defaults to False.\n",
        "            rebuild_index (bool, optional): If True, rebuilds vector indexes from scratch. Defaults to False.\n",
        "            verbose (bool, optional): If True, enables verbose output. Defaults to False.\n",
        "            chunk_size (int, optional): Chunk size for text splitting. Defaults to CHATBOT_CONFIG[\"chunk_size\"].\n",
        "            chunk_overlap (int, optional): Chunk overlap for text splitting. Defaults to CHATBOT_CONFIG[\"chunk_overlap\"].\n",
        "            chunk_threshold (int, optional): Token length threshold for chunking documents. Defaults to CHATBOT_CONFIG[\"chunk_threshold\"].\n",
        "        \"\"\"\n",
        "        if not isinstance(pdf_folders, list):\n",
        "            raise TypeError(\"pdf_folders must be a list of strings.\")\n",
        "        self.pdf_folders = pdf_folders\n",
        "        self.persist_directory = persist_directory\n",
        "        self.test_mode = test_mode\n",
        "        self.verbose = verbose\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.chunk_threshold = chunk_threshold\n",
        "\n",
        "        self._setup_llm(max_length)\n",
        "        self._setup_embeddings()\n",
        "\n",
        "        if rebuild_index or not os.path.exists(self.persist_directory) or not os.listdir(self.persist_directory):\n",
        "            self._load_documents()\n",
        "            self._build_raw_vector_index()\n",
        "        else:\n",
        "            if self.verbose:\n",
        "                print(\"Loading existing raw vector index from persistence directory.\")\n",
        "            self.raw_db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "\n",
        "        self.summary_persist_dir = os.path.join(self.persist_directory, \"summaries\")\n",
        "        if rebuild_index or not os.path.exists(self.summary_persist_dir) or not os.listdir(self.summary_persist_dir):\n",
        "            if not hasattr(self, \"documents\"):\n",
        "                self._load_documents()\n",
        "            self._build_summary_vector_index()\n",
        "        else:\n",
        "            if self.verbose:\n",
        "                print(\"Loading existing summary vector index from 'summaries' directory.\")\n",
        "            self.summary_db = Chroma(persist_directory=self.summary_persist_dir, embedding_function=self.embeddings)\n",
        "\n",
        "        self._setup_summary_retriever()\n",
        "        self.memory = ConversationBufferWindowMemory(k=CHATBOT_CONFIG[\"memory_window_k\"], memory_key=\"chat_history\", return_messages=True)\n",
        "        self._setup_qa_chain()\n",
        "\n",
        "    def _setup_llm(self, max_length: int) -> None:\n",
        "        \"\"\"\n",
        "        Sets up the Language Model (LLM) pipeline using Hugging Face Transformers (BigBird-Pegasus model).\n",
        "\n",
        "        Args:\n",
        "            max_length (int): Maximum token length for text generation.\n",
        "        \"\"\"\n",
        "        self.model_name = self.BIGBIRD_PEGASUS_MODEL_NAME # Changed to BIGBIRD_PEGASUS_MODEL_NAME\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, token=os.environ.get(\"HF\")) # Use token from Colab Secrets\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16, # Consider keeping/removing float16 based on memory\n",
        "            token=os.environ.get(\"HF\") # Use token from Colab Secrets\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.to(\"cuda\")\n",
        "        self.pipe = pipeline(\n",
        "            self.TEXT_GENERATION_TASK,\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=max_length,\n",
        "            temperature=CHATBOT_CONFIG[\"temperature\"],\n",
        "            top_p=CHATBOT_CONFIG[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            batch_size=CHATBOT_CONFIG[\"batch_size\"]\n",
        "        )\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "    def _setup_embeddings(self) -> None:\n",
        "        \"\"\"\n",
        "        Sets up the Hugging Face Embeddings.\n",
        "        \"\"\"\n",
        "        self.embedding_model = self.EMBEDDING_MODEL_NAME\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "    def _load_documents(self) -> None:\n",
        "        \"\"\"\n",
        "        Loads PDF documents from specified folders.\n",
        "        \"\"\"\n",
        "        self.documents = []\n",
        "        total_files_loaded = 0\n",
        "        for folder in self.pdf_folders:\n",
        "            bank_name = os.path.basename(folder).lower()\n",
        "            file_names = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            if not file_names:\n",
        "                continue\n",
        "            if self.test_mode:\n",
        "                file_names = file_names[:1]\n",
        "            for pdf_file in file_names:\n",
        "                pdf_path = os.path.join(folder, pdf_file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(pdf_path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank_name\n",
        "                        doc.metadata[\"source_pdf\"] = pdf_file\n",
        "                    self.documents.extend(docs)\n",
        "                    total_files_loaded += 1\n",
        "                    if self.verbose:\n",
        "                        print(f\"Loaded: {pdf_file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {pdf_file} from {folder}: {e}\") # Always print errors, even in non-verbose mode\n",
        "            if self.test_mode:\n",
        "                break\n",
        "        if self.verbose:\n",
        "            print(f\"Total PDF files loaded: {total_files_loaded}\")\n",
        "\n",
        "    def _chunk_document_if_needed(self, doc: Document) -> list[Document]:\n",
        "        \"\"\"\n",
        "        Chunks a document if its token length exceeds the chunk threshold.\n",
        "\n",
        "        Args:\n",
        "            doc (Document): The document to process.\n",
        "\n",
        "        Returns:\n",
        "            list[Document]: A list containing either the original document or its chunks.\n",
        "        \"\"\"\n",
        "        tokens = self.tokenizer.encode(doc.page_content)\n",
        "        if len(tokens) > self.chunk_threshold:\n",
        "            splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "            chunks = splitter.split_documents([doc])\n",
        "            return self.remove_duplicate_chunks(chunks)\n",
        "        return [doc]\n",
        "\n",
        "    def _build_raw_vector_index(self) -> None:\n",
        "        \"\"\"\n",
        "        Builds the raw document vector index (first vector store).\n",
        "        \"\"\"\n",
        "        processed_docs = []\n",
        "        for doc in self.documents:\n",
        "            processed_docs.extend(self._chunk_document_if_needed(doc)) # Use helper function for chunking\n",
        "\n",
        "        self.raw_db = Chroma.from_documents(\n",
        "            processed_docs,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "        self.raw_db.persist()\n",
        "        if self.verbose:\n",
        "            print(f\"Built raw vector index with {len(processed_docs)} documents/chunks.\")\n",
        "\n",
        "\n",
        "    def _summarize_document_batch(self, prompts: list[str]) -> list[str]:\n",
        "        \"\"\"\n",
        "        Summarizes a batch of documents using the LLM.\n",
        "\n",
        "        Args:\n",
        "            prompts (list[str]): List of prompts, each corresponding to a document/chunk.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: List of summary texts.\n",
        "        \"\"\"\n",
        "        ds = Dataset.from_dict({\"text\": prompts})\n",
        "        if self.verbose:\n",
        "            print(\"Dataset created for summarization prompts:\")\n",
        "            print(ds)\n",
        "        try:\n",
        "            summary_responses_list = self.llm.generate(ds[\"text\"])\n",
        "            summary_responses = [resp[0].page_content if isinstance(resp, list) and resp else str(resp) for resp in summary_responses_list] # Extract from list of lists\n",
        "            return summary_responses\n",
        "        except Exception as e:\n",
        "            print(f\"Error during LLM summarization batch: {e}\")\n",
        "            return [\"\"] * len(prompts) # Return empty summaries to avoid breaking the flow\n",
        "\n",
        "    def _create_summary_documents(self, processed_docs: list[Document], summary_texts: list[str]) -> list[Document]:\n",
        "        \"\"\"\n",
        "        Creates summary documents from original documents and their summaries.\n",
        "\n",
        "        Args:\n",
        "            processed_docs (list[Document]): List of original processed documents/chunks.\n",
        "            summary_texts (list[str]): List of corresponding summary texts.\n",
        "\n",
        "        Returns:\n",
        "            list[Document]: List of summary documents.\n",
        "        \"\"\"\n",
        "        summary_docs = []\n",
        "        for orig_doc, summary_text in zip(processed_docs, summary_texts):\n",
        "            summary_doc = Document(\n",
        "                page_content=summary_text,\n",
        "                metadata={\n",
        "                    \"source_pdf\": orig_doc.metadata.get(\"source_pdf\", \"Unknown\"),\n",
        "                    \"bank\": orig_doc.metadata.get(\"bank\", \"unknown\"),\n",
        "                    \"orig_chunk\": orig_doc.page_content[:50]\n",
        "                }\n",
        "            )\n",
        "            summary_docs.append(summary_doc)\n",
        "        return summary_docs\n",
        "\n",
        "\n",
        "    def _build_summary_vector_index(self) -> None:\n",
        "        \"\"\"\n",
        "        Builds the summary vector index (second vector store) using summarized documents.\n",
        "        \"\"\"\n",
        "        processed_docs = []\n",
        "        for doc in self.documents:\n",
        "            processed_docs.extend(self._chunk_document_if_needed(doc))\n",
        "\n",
        "        prompts = [\n",
        "            \"Summarize the following text in bullet points, focusing on key financials, sentiment, and forward-looking statements:\\n\\n\" + d.page_content\n",
        "            for d in processed_docs\n",
        "        ]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Summarizing {len(prompts)} documents/chunks...\")\n",
        "\n",
        "        summary_texts = self._summarize_document_batch(prompts) # Use batch summarization helper\n",
        "\n",
        "        summary_docs = self._create_summary_documents(processed_docs, summary_texts) # Use helper to create summary docs\n",
        "\n",
        "        self.summary_db = Chroma.from_documents(\n",
        "            summary_docs,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=self.summary_persist_dir\n",
        "        )\n",
        "        self.summary_db.persist()\n",
        "        if self.verbose:\n",
        "            print(f\"Built summary vector index with {len(summary_docs)} summarized docs.\")\n",
        "\n",
        "\n",
        "    def _setup_summary_retriever(self) -> None:\n",
        "        \"\"\"\n",
        "        Sets up the retriever for the summary vector store.\n",
        "        \"\"\"\n",
        "        self.summary_retriever = self.summary_db.as_retriever(search_kwargs={\"k\": CHATBOT_CONFIG[\"retriever_search_k\"]})\n",
        "\n",
        "    def _setup_qa_chain(self) -> None:\n",
        "        \"\"\"\n",
        "        Sets up the Conversational Retrieval Chain for Question Answering.\n",
        "        \"\"\"\n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.summary_retriever,\n",
        "            memory=self.memory,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_duplicate_chunks(chunks: list[Document]) -> list[Document]:\n",
        "        \"\"\"\n",
        "        Removes duplicate chunks based on page content.\n",
        "\n",
        "        Args:\n",
        "            chunks (list[Document]): List of document chunks.\n",
        "\n",
        "        Returns:\n",
        "            list[Document]: List of unique document chunks.\n",
        "        \"\"\"\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for chunk in chunks:\n",
        "            text = chunk.page_content.strip()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                unique.append(chunk)\n",
        "        return unique\n",
        "\n",
        "    def truncate_context(self, context_list: list[Document], max_tokens: int = 800) -> list[Document]:\n",
        "        \"\"\"\n",
        "        Truncates a list of context documents to fit within a maximum token limit.\n",
        "\n",
        "        Args:\n",
        "            context_list (list[Document]): List of context documents.\n",
        "            max_tokens (int, optional): Maximum token limit. Defaults to 800.\n",
        "\n",
        "        Returns:\n",
        "            list[Document]: Truncated list of context documents.\n",
        "        \"\"\"\n",
        "        truncated = []\n",
        "        current = 0\n",
        "        for doc in context_list:\n",
        "            doc_tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "            if current + doc_tokens <= max_tokens:\n",
        "                truncated.append(doc)\n",
        "                current += doc_tokens\n",
        "            else:\n",
        "                break\n",
        "        return truncated\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_user_input(user_input: str) -> str:\n",
        "        \"\"\"\n",
        "        Cleans user input by stripping whitespace and replacing newlines and tabs.\n",
        "\n",
        "        Args:\n",
        "            user_input (str): User input string.\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned user input string.\n",
        "        \"\"\"\n",
        "        return user_input.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "    def reset_memory_if_needed(self) -> None:\n",
        "        \"\"\"\n",
        "        Resets conversation memory if it exceeds a certain length.\n",
        "        \"\"\"\n",
        "        if len(self.memory.chat_memory.messages) > 6: # Magic number - consider making configurable if needed\n",
        "            if self.verbose:\n",
        "                print(\"\\nMemory Full: Resetting Conversation History...\\n\")\n",
        "            self.memory.clear()\n",
        "\n",
        "    def format_response(self, question: str, response: dict) -> str:\n",
        "        \"\"\"\n",
        "        Formats the LLM response into a user-friendly string.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question.\n",
        "            response (dict): The raw response dictionary from the QA chain.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted response string.\n",
        "        \"\"\"\n",
        "        if isinstance(response, dict) and 'answer' in response:\n",
        "            resp_text = response['answer'].strip()\n",
        "        else:\n",
        "            resp_text = str(response).strip()\n",
        "        for phrase in [\n",
        "            \"Use the following pieces of context\",\n",
        "            \"If you don't know the answer, just say that you don't know\",\n",
        "            \"Don't try to make up an answer.\"\n",
        "        ]:\n",
        "            if phrase in resp_text:\n",
        "                resp_text = resp_text.split(phrase)[-1].strip()\n",
        "        return f\"Question: {question}\\nHelpful Answer: {resp_text}\"\n",
        "\n",
        "    def _batch_summarize_context_docs(self, context: list[Document], per_doc_limit: int) -> list[str]:\n",
        "        \"\"\"\n",
        "        Batch summarizes context documents if they exceed a per-document token limit.\n",
        "\n",
        "        Args:\n",
        "            context (list[Document]): List of context documents.\n",
        "            per_doc_limit (int): Token limit per document.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: List of summarized context document texts.\n",
        "        \"\"\"\n",
        "        summarized_context = []\n",
        "        batch_prompts = []\n",
        "        for doc in context:\n",
        "            doc_text = doc.page_content\n",
        "            tokens = self.tokenizer.encode(doc_text)\n",
        "            if len(tokens) > per_doc_limit:\n",
        "                summary_prompt = f\"Summarize the following text in a concise bullet-point format:\\n\\n{doc_text}\"\n",
        "                batch_prompts.append(summary_prompt)\n",
        "            else:\n",
        "                summarized_context.append(doc_text) # Keep original text if short enough\n",
        "\n",
        "        if batch_prompts:\n",
        "            ds = Dataset.from_dict({\"text\": batch_prompts})\n",
        "            try:\n",
        "                summary_responses_list = self.llm.generate(ds[\"text\"])\n",
        "                batch_summaries = [resp[0].page_content if isinstance(resp, list) and resp else str(resp) for resp in summary_responses_list] # Extract summaries\n",
        "                summarized_context.extend(batch_summaries)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during batch summarization of context documents: {e}\")\n",
        "                summarized_context.extend([\"\"] * len(batch_prompts)) # Add empty summaries to avoid breaking flow\n",
        "        return summarized_context\n",
        "\n",
        "\n",
        "    def trim_final_input(self, question: str, context: list[Document], max_tokens: int = 512) -> str:\n",
        "        \"\"\"\n",
        "        Trims the final input to the QA chain to be within the token limit.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question.\n",
        "            context (list[Document]): List of retrieved context documents.\n",
        "            max_tokens (int, optional): Maximum token limit for the final input. Defaults to 512.\n",
        "\n",
        "        Returns:\n",
        "            str: Trimmed final input text.\n",
        "        \"\"\"\n",
        "        system_message = (\n",
        "            \"You are analyzing a bank's quarterly earnings call transcript. \"\n",
        "            \"Provide a bullet-point summary of the most important takeaways with specific details: \"\n",
        "            \"list key revenue trends (include any percentage changes if available), major expense drivers, \"\n",
        "            \"and management's outlook for the future. If numerical details are not available, provide qualitative insights.\"\n",
        "        )\n",
        "\n",
        "        per_doc_limit = max_tokens // max(1, len(context)) # Limit per doc based on number of context docs\n",
        "        summarized_context = self._batch_summarize_context_docs(context, per_doc_limit) # Use batch summary helper\n",
        "\n",
        "        context_str = \"\\n\".join(summarized_context)\n",
        "        input_text = f\"{system_message}\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "        token_length = len(self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens))\n",
        "        if self.verbose:\n",
        "            print(f\"Final input token length: {token_length}\") # Log final input token length\n",
        "\n",
        "        tokens = self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens) # Truncate input if too long\n",
        "        return self.tokenizer.decode(tokens)\n",
        "\n",
        "\n",
        "    def answer_question(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Answers a user question using the conversational retrieval chain.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted answer string.\n",
        "        \"\"\"\n",
        "        self.reset_memory_if_needed()\n",
        "        response_dict = self.qa_chain({\"question\": question})\n",
        "        final_answer = response_dict[\"answer\"].strip()\n",
        "        return f\"Question: {question}\\nHelpful Answer: {final_answer}\"\n",
        "\n",
        "    def run_chatbot(self) -> None:\n",
        "        \"\"\"\n",
        "        Runs the interactive chatbot loop.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸ’¬ Bank Earnings Chatbot - Two Stage (Type 'exit' to stop)\")\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \")\n",
        "            if user_input.lower() == \"exit\":\n",
        "                print(\"\\nExiting Chatbot. Have a great day!\")\n",
        "                break\n",
        "            answer = self.answer_question(user_input)\n",
        "            print(\"\\n\" + answer)\n",
        "\n",
        "    # ---------------- Evaluation Methods ----------------\n",
        "    def evaluate_summaries(self, generated_summaries: dict, reference_summaries: dict, verbose: bool = False) -> dict:\n",
        "        \"\"\"\n",
        "        Evaluates generated summaries against reference summaries using ROUGE scores.\n",
        "\n",
        "        Args:\n",
        "            generated_summaries (dict): Dictionary of generated summaries.\n",
        "            reference_summaries (dict): Dictionary of reference summaries.\n",
        "            verbose (bool, optional): If True, prints evaluation scores for each summary. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary of ROUGE scores.\n",
        "        \"\"\"\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        eval_scores = {}\n",
        "        for key, gen_dict in generated_summaries.items():\n",
        "            gen_summary = gen_dict.get(\"summary\", \"\")\n",
        "            if key in reference_summaries:\n",
        "                ref_summary = reference_summaries[key]\n",
        "                scores = scorer.score(ref_summary, gen_summary)\n",
        "                eval_scores[key] = scores\n",
        "                if verbose:\n",
        "                    print(f\"Evaluation for {key}: {scores}\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(f\"No reference summary provided for {key}.\")\n",
        "        return eval_scores\n",
        "\n",
        "    def summarize_individual_transcripts(self) -> dict:\n",
        "        \"\"\"\n",
        "        Summarizes individual transcripts in bullet-point sentiment summaries.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary of summaries, keyed by source PDF filename.\n",
        "        \"\"\"\n",
        "        prompts = []\n",
        "        sources = []\n",
        "        for doc in self.documents:\n",
        "            source = doc.metadata.get('source_pdf', 'Unknown Source')\n",
        "            bank = doc.metadata.get('bank', 'unknown')\n",
        "            transcript_text = doc.page_content\n",
        "            prompt = f\"Please provide a bullet-point sentiment summary for the following transcript:\\n\\n{transcript_text}\"\n",
        "            prompts.append(prompt)\n",
        "            sources.append((source, bank))\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Batch summarizing {len(prompts)} transcripts...\")\n",
        "\n",
        "        ds = Dataset.from_dict({\"text\": prompts})\n",
        "        if self.verbose:\n",
        "            print(\"Dataset created for transcript summarization:\")\n",
        "            print(ds)\n",
        "        try:\n",
        "            responses_list = self.llm.generate(ds[\"text\"])\n",
        "            responses = [resp[0].page_content if isinstance(resp, list) and resp else str(resp) for resp in responses_list] # Extract from list of lists\n",
        "        except Exception as e:\n",
        "            print(f\"Error during batch summarization of transcripts: {e}\")\n",
        "            responses = [\"\"] * len(prompts) # Return empty responses to continue processing\n",
        "\n",
        "        summaries = {}\n",
        "        for (source, bank), response in zip(sources, responses):\n",
        "            summaries[source] = {\"bank\": bank, \"summary\": response}\n",
        "            if self.verbose:\n",
        "                print(f\"Summarized transcript from {source}\")\n",
        "        return summaries\n",
        "\n",
        "    @staticmethod\n",
        "    def group_summaries_by_bank_and_quarter(summaries: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Groups transcript summaries by bank and quarter.\n",
        "\n",
        "        Args:\n",
        "            summaries (dict): Dictionary of transcript summaries.\n",
        "\n",
        "        Returns:\n",
        "            dict: Grouped summaries, nested by bank and then quarter.\n",
        "        \"\"\"\n",
        "        grouped = {}\n",
        "        for source, info in summaries.items():\n",
        "            bank = info[\"bank\"]\n",
        "            summary = info[\"summary\"]\n",
        "            match = re.search(\n",
        "                r'((\\d{1}[qQ]|[qQ]\\d)|((first|second|third|fourth)[-_ ]?quarter))[-_ ]?(\\d{2,4})',\n",
        "                source,\n",
        "                re.IGNORECASE\n",
        "            )\n",
        "            if match:\n",
        "                if match.group(2):\n",
        "                    quarter_raw = match.group(2).lower()\n",
        "                    quarter_digit_match = re.search(r'\\d', quarter_raw)\n",
        "                    quarter_num = quarter_digit_match.group(0) if quarter_digit_match else \"1\"\n",
        "                elif match.group(4):\n",
        "                    word = match.group(4).lower()\n",
        "                    mapping = {\"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\"}\n",
        "                    quarter_num = mapping.get(word, \"1\")\n",
        "                else:\n",
        "                    quarter_num = \"1\"\n",
        "                year_str = match.group(5)\n",
        "                year_val = int(year_str)\n",
        "                if year_val < 100:\n",
        "                    year_val += 2000\n",
        "                key = f\"{year_val}-Q{quarter_num}\"\n",
        "            else:\n",
        "                key = \"Unknown\" # Handle cases where quarter can't be extracted\n",
        "            if bank not in grouped:\n",
        "                grouped[bank] = {}\n",
        "            if key not in grouped[bank]:\n",
        "                grouped[bank][key] = []\n",
        "            grouped[bank][key].append(summary)\n",
        "        return grouped\n",
        "\n",
        "    def aggregate_quarterly_summaries_by_bank(self, grouped_quarterly: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Aggregates quarterly summaries for each bank into a single summary per quarter.\n",
        "\n",
        "        Args:\n",
        "            grouped_quarterly (dict): Grouped quarterly summaries from `group_summaries_by_bank_and_quarter`.\n",
        "\n",
        "        Returns:\n",
        "            dict: Aggregated quarterly summaries, by bank and quarter.\n",
        "        \"\"\"\n",
        "        quarterly_aggregates = {}\n",
        "        for bank, quarters in grouped_quarterly.items():\n",
        "            quarterly_aggregates[bank] = {}\n",
        "            for key, summaries in quarters.items():\n",
        "                combined = \"\\n\".join(summaries)\n",
        "                prompt = (f\"Based on the following quarterly sentiment summaries for {bank.upper()} ({key}), \"\n",
        "                          \"provide a concise bullet-point overview of the overall sentiment for that quarter:\\n\\n\" + combined)\n",
        "                try:\n",
        "                    response_list = self.llm.generate([prompt]) # Generate for a single prompt in list\n",
        "                    response = response_list[0] if response_list else \"\" # Get the first (and only) response, handle empty list\n",
        "                    if isinstance(response, list) and response and 'answer' in response[0]: # Check if response is list and answer is in first element\n",
        "                        answer_text = response[0]['answer'] # Access answer if structured response\n",
        "                    elif isinstance(response, dict) and 'answer' in response:\n",
        "                        answer_text = response['answer'] # Handle dict-like response\n",
        "                    else:\n",
        "                        answer_text = str(response) # Fallback to string conversion\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during quarterly aggregation for {bank} - {key}: {e}\")\n",
        "                    answer_text = \"Error in aggregation.\" # Provide error message instead of breaking\n",
        "\n",
        "                quarterly_aggregates[bank][key] = answer_text\n",
        "        return quarterly_aggregates\n",
        "\n",
        "    def forecast_next_quarter_sentiment(self, bank: str, historical_quarterly: list[str]) -> str:\n",
        "        \"\"\"\n",
        "        Forecasts sentiment for the next quarter based on historical quarterly summaries.\n",
        "\n",
        "        Args:\n",
        "            bank (str): Bank name.\n",
        "            historical_quarterly (list[str]): List of historical quarterly summaries.\n",
        "\n",
        "        Returns:\n",
        "            str: Forecasted sentiment summary for the next quarter.\n",
        "        \"\"\"\n",
        "        combined = \"\\n\".join(historical_quarterly)\n",
        "        prompt = (\n",
        "            f\"Based on the following historical quarterly sentiment summaries for {bank.upper()}, \"\n",
        "            \"forecast the overall sentiment for the next quarter. Provide a bullet-point summary of the expected trends, \"\n",
        "            \"including any changes in tone, risk factors, or optimism:\\n\\n\" + combined\n",
        "        )\n",
        "        try:\n",
        "            response_list = self.llm.generate([prompt]) # Generate for single prompt in list\n",
        "            response = response_list[0] if response_list else \"\" # Get first response, handle empty\n",
        "            if isinstance(response, list) and response and 'answer' in response[0]:\n",
        "                return response[0]['answer'] # Access answer if structured\n",
        "            elif isinstance(response, dict) and 'answer' in response:\n",
        "                return response['answer'] # Handle dict response\n",
        "            else:\n",
        "                return str(response) # Fallback to string\n",
        "        except Exception as e:\n",
        "            print(f\"Error forecasting next quarter sentiment for {bank}: {e}\")\n",
        "            return \"Forecast unavailable due to an error.\" # Indicate forecast error\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_quarter_key(key: str) -> tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Parses a quarter key string (e.g., \"2023-Q1\") into year and quarter integers.\n",
        "\n",
        "        Args:\n",
        "            key (str): Quarter key string.\n",
        "\n",
        "        Returns:\n",
        "            tuple[int, int]: Tuple containing year and quarter as integers. Returns (0, 0) on parsing error.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            parts = key.split(\"-\")\n",
        "            year = int(parts[0])\n",
        "            quarter = int(re.search(r'\\d', parts[1]).group(0))\n",
        "            return year, quarter\n",
        "        except Exception:\n",
        "            return (0, 0)\n",
        "\n",
        "    def analyze_and_forecast_sentiment_by_bank(self) -> dict:\n",
        "        \"\"\"\n",
        "        Analyzes sentiment by bank, aggregating quarterly data and forecasting next quarter sentiment.\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results, including previous year summary, current quarter summary, and next quarter forecast, per bank.\n",
        "        \"\"\"\n",
        "        print(\"Generating individual transcript summaries...\")\n",
        "        summaries = self.summarize_individual_transcripts()\n",
        "        print(\"Grouping summaries by bank and quarter...\")\n",
        "        grouped_quarterly = self.group_summaries_by_bank_and_quarter(summaries)\n",
        "        print(\"Aggregating quarterly summaries...\")\n",
        "        quarterly_aggregates = self.aggregate_quarterly_summaries_by_bank(grouped_quarterly)\n",
        "\n",
        "        analysis = {}\n",
        "        for bank, quarters in quarterly_aggregates.items():\n",
        "            analysis[bank] = {}\n",
        "            valid_keys = [k for k in quarters.keys() if k != \"Unknown\"] # Exclude \"Unknown\" keys for sorting\n",
        "            if not valid_keys:\n",
        "                print(f\"No valid quarter keys found for {bank}: {valid_keys}\")\n",
        "                continue # Skip banks with no valid quarter keys\n",
        "\n",
        "            sorted_keys = sorted(valid_keys, key=lambda k: self.parse_quarter_key(k)) # Sort valid keys by year, quarter\n",
        "            most_recent_key = sorted_keys[-1] # Get most recent key from sorted list\n",
        "            current_year, current_quarter = self.parse_quarter_key(most_recent_key)\n",
        "\n",
        "            years = sorted({self.parse_quarter_key(k)[0] for k in quarters if k != \"Unknown\"}) # Get unique years for valid keys\n",
        "            previous_year = max([y for y in years if y < current_year], default=None) # Find prev year, default None if not exist\n",
        "\n",
        "            previous_year_summaries = []\n",
        "            if previous_year is not None:\n",
        "                for key in quarters:\n",
        "                    year, _ = self.parse_quarter_key(key)\n",
        "                    if year == previous_year:\n",
        "                        previous_year_summaries.extend(quarters[key])\n",
        "                combined_prev = \"\\n\".join(previous_year_summaries)\n",
        "                prompt_prev = (f\"Based on the following sentiment summaries for all quarters in {previous_year} for {bank.upper()}, \"\n",
        "                               \"provide a bullet-point summary of the overall sentiment trends for that year:\\n\\n\" + combined_prev)\n",
        "                try:\n",
        "                    response_list_prev = self.llm.generate([prompt_prev]) # Generate prev year summary\n",
        "                    response_prev = response_list_prev[0] if response_list_prev else \"\"\n",
        "                    if isinstance(response_prev, list) and response_prev and 'answer' in response_prev[0]:\n",
        "                        previous_year_summary = response_prev[0]['answer']\n",
        "                    elif isinstance(response_prev, dict) and 'answer' in response_prev:\n",
        "                        previous_year_summary = response_prev['answer']\n",
        "                    else:\n",
        "                        previous_year_summary = str(response_prev)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error summarizing previous year sentiment for {bank}: {e}, Exception: {e}\") # Improved error logging\n",
        "                    previous_year_summary = \"Error summarizing previous year.\"\n",
        "            else:\n",
        "                previous_year_summary = \"Not available (no previous year data).\"\n",
        "\n",
        "            current_summary = quarters.get(most_recent_key, \"Not available\")\n",
        "\n",
        "            historical = []\n",
        "            for key in sorted_keys:\n",
        "                historical.extend(quarters[key])\n",
        "            try:\n",
        "                forecast = self.forecast_next_quarter_sentiment(bank, historical) if historical else \"Not available (no historical data for forecast).\" # Forecast, handle no historical data\n",
        "            except Exception as e:\n",
        "                print(f\"Error forecasting next quarter sentiment for {bank}: {e}\") # Improved error logging\n",
        "                forecast = \"Forecast unavailable due to an error.\"\n",
        "\n",
        "\n",
        "            analysis[bank] = {\n",
        "                \"previous_year_summary\": previous_year_summary,\n",
        "                \"current_quarter_summary\": current_summary,\n",
        "                \"forecast_next_quarter\": forecast,\n",
        "                \"most_recent_key\": most_recent_key\n",
        "            }\n",
        "        return analysis\n",
        "\n",
        "\n",
        "# ---------------- Example Usage and Evaluation ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_folders = CHATBOT_CONFIG[\"pdf_folders\"] # Use folders from config\n",
        "    persist_directory = CHATBOT_CONFIG[\"persist_directory\"] # Use persist dir from config\n",
        "\n",
        "    chatbot = BankEarningsChatbotTwoStage(\n",
        "        pdf_folders,\n",
        "        persist_directory=persist_directory,\n",
        "        test_mode=False,\n",
        "        rebuild_index=True, # IMPORTANT: Rebuild index for the new model\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    analysis = chatbot.analyze_and_forecast_sentiment_by_bank()\n",
        "\n",
        "    print(\"\\nYearly and Quarterly Sentiment Analysis by Bank:\")\n",
        "    for bank, data in analysis.items():\n",
        "        print(f\"\\n{bank.upper()} Analysis:\")\n",
        "        print(f\"Most Recent Quarter Key: {data.get('most_recent_key', 'N/A')}\")\n",
        "        print(\"Previous Year Sentiment Summary:\")\n",
        "        print(data[\"previous_year_summary\"])\n",
        "        print(\"Current Quarter Sentiment Summary:\")\n",
        "        print(data[\"current_quarter_summary\"])\n",
        "        print(\"Forecast for Next Quarter:\")\n",
        "        print(data[\"forecast_next_quarter\"])\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Evaluation Example:\n",
        "    # Suppose you have reference summaries in a dictionary:\n",
        "    reference_summaries = {\n",
        "        \"1q23-earnings-transcript.pdf\": \"Reference summary for 1Q23 transcript...\",\n",
        "        \"4q24-earnings-call-remarks.pdf\": \"Reference summary for 4Q24 transcript...\"\n",
        "        # Add more as available.\n",
        "    }\n",
        "\n",
        "    generated_summaries = chatbot.summarize_individual_transcripts()\n",
        "    evaluation_results = chatbot.evaluate_summaries(generated_summaries, reference_summaries, verbose=True)\n",
        "    print(\"\\nEvaluation Results (ROUGE):\")\n",
        "    for key, scores in evaluation_results.items():\n",
        "        print(f\"{key}: {scores}\")\n",
        "\n",
        "    # To run the chatbot interactively:\n",
        "    # chatbot.run_chatbot()"
      ],
      "metadata": {
        "id": "XeKY3w_dBRN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# pdf_folders = [\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "# ]\n",
        "\n",
        "# persist_directory = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\n",
        "\n",
        "# chatbot = BankEarningsChatbotTwoStage(\n",
        "#     pdf_folders=pdf_folders,\n",
        "#     persist_directory=persist_directory,\n",
        "#     max_length=256,\n",
        "#     test_mode=False,\n",
        "#     rebuild_index=True,    # If True, it reprocesses everything\n",
        "#     verbose=True,\n",
        "#     chunk_size=1000,\n",
        "#     chunk_overlap=100\n",
        "# )\n",
        "\n",
        "# # Now you have your two-stage chatbot.\n"
      ],
      "metadata": {
        "id": "mYRXEi7-RhxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the chatbot object"
      ],
      "metadata": {
        "id": "jaKJRslPWoj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch an interactive  Chatbot session"
      ],
      "metadata": {
        "id": "CLH9TcRtWL85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.run_chatbot()"
      ],
      "metadata": {
        "id": "g5TI2IMJWLQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}