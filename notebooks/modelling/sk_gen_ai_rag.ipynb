{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcCHgfmdAvE9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "3bf44adc-e1e9-4a2a-a08a-9b652529b8d3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install langchain-community\n",
        "!pip install -q langchain-community pypdf tiktoken chromadb sentence-transformers > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAkI_0h4A9Zj",
        "outputId": "37571411-54f1-40e9-cb26-50f64455af22"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JL-O9CQQAvFB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# class BankEarningsChatbot:\n",
        "#     \"\"\"\n",
        "#     A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine\n",
        "#     designed to analyze bank quarterly earnings call transcripts. It loads PDF documents\n",
        "#     from one or more specified folders, builds a Chroma vector index, and sets up an interactive\n",
        "#     conversational chain for prompt-based queries.\n",
        "\n",
        "#     Parameters:\n",
        "#         pdf_folders (list or str): A list of folder paths containing PDF files, or a single folder path as a string.\n",
        "#         persist_directory (str): Directory path to persist the vector index and model outputs.\n",
        "#         max_length (int): Maximum output length for the T5 model.\n",
        "#         test_mode (bool): If True, only loads one PDF (from the first folder) for quick testing.\n",
        "#         rebuild_index (bool): If True, reprocess all PDFs and rebuild the index even if a persisted index exists.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, pdf_folders,\n",
        "#                  persist_directory=\"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "#                  max_length=256, test_mode=False, rebuild_index=False):\n",
        "#         # Allow a single folder (string) or a list of folders.\n",
        "#         if isinstance(pdf_folders, str):\n",
        "#             self.pdf_folders = [pdf_folders]\n",
        "#         else:\n",
        "#             self.pdf_folders = pdf_folders\n",
        "\n",
        "#         self.persist_directory = persist_directory\n",
        "#         self.test_mode = test_mode\n",
        "\n",
        "#         # Set up the LLM pipeline using the Flan-T5 model.\n",
        "#         self._setup_llm(max_length)\n",
        "\n",
        "#         # Configure embeddings.\n",
        "#         self._setup_embeddings()\n",
        "\n",
        "#         # Check if we need to rebuild the vector index.\n",
        "#         if rebuild_index or (not os.path.exists(self.persist_directory)) or (not os.listdir(self.persist_directory)):\n",
        "#             # Load documents and build the vector index.\n",
        "#             self._load_documents()\n",
        "#             self._build_vector_index()\n",
        "#         else:\n",
        "#             print(\"Loading existing vector index from persistence directory.\")\n",
        "#             self.db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "#             # Note: If you need to update the in-memory index from the persisted data, this method should suffice.\n",
        "\n",
        "#         # Configure retriever from the persisted vector database.\n",
        "#         self._setup_retriever()\n",
        "\n",
        "#         # Initialize conversation memory.\n",
        "#         self.memory = ConversationBufferWindowMemory(k=10, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "#         # Set up the conversational retrieval chain.\n",
        "#         self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "#             llm=self.llm,\n",
        "#             retriever=self.retriever,\n",
        "#             memory=self.memory,\n",
        "#             verbose=False\n",
        "#         )\n",
        "\n",
        "#     def _setup_llm(self, max_length):\n",
        "#         \"\"\"\n",
        "#         Configures the language model pipeline using a T5 model.\n",
        "#         \"\"\"\n",
        "#         self.model_name = \"google/flan-t5-large\"\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "#         self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "#             self.model_name,\n",
        "#             device_map=\"auto\",\n",
        "#             torch_dtype=torch.float16\n",
        "#         )\n",
        "#         # Using the text2text-generation pipeline for T5.\n",
        "#         self.pipe = pipeline(\n",
        "#             \"text2text-generation\",\n",
        "#             model=self.model,\n",
        "#             tokenizer=self.tokenizer,\n",
        "#             max_length=max_length,\n",
        "#             temperature=0.5,\n",
        "#             top_p=0.8,\n",
        "#             do_sample=True\n",
        "#         )\n",
        "#         self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "#     def _setup_embeddings(self):\n",
        "#         \"\"\"\n",
        "#         Initializes the sentence-transformer based embeddings.\n",
        "#         \"\"\"\n",
        "#         self.embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "#         self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "#     def _load_documents(self):\n",
        "#         \"\"\"\n",
        "#         Loads and splits PDF documents from the specified folders.\n",
        "#         In test_mode, only the first PDF (from the first folder) is loaded.\n",
        "#         \"\"\"\n",
        "#         self.documents = []\n",
        "#         for folder in self.pdf_folders:\n",
        "#             file_names = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "#             if not file_names:\n",
        "#                 continue\n",
        "#             # If in test_mode, load only the first PDF file from this folder.\n",
        "#             if self.test_mode:\n",
        "#                 file_names = file_names[:1]\n",
        "#             for pdf_file in file_names:\n",
        "#                 pdf_path = os.path.join(folder, pdf_file)\n",
        "#                 try:\n",
        "#                     loader = PyPDFLoader(pdf_path, extract_images=False)\n",
        "#                     self.documents.extend(loader.load_and_split())\n",
        "#                     print(f\"Loaded: {pdf_file} from {folder}\")\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error loading {pdf_file} from {folder}: {e}\")\n",
        "#             # In test mode, break after processing the first folder.\n",
        "#             if self.test_mode:\n",
        "#                 break\n",
        "\n",
        "#     def _build_vector_index(self):\n",
        "#         \"\"\"\n",
        "#         Chunks documents and builds a Chroma vector index.\n",
        "#         \"\"\"\n",
        "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "#         chunks = text_splitter.split_documents(self.documents)\n",
        "#         # Remove duplicate chunks.\n",
        "#         chunks = self.remove_duplicate_chunks(chunks)\n",
        "#         self.chunks = chunks\n",
        "#         self.db = Chroma.from_documents(chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "#         self.db.persist()\n",
        "\n",
        "#     def _setup_retriever(self):\n",
        "#         \"\"\"\n",
        "#         Initializes the retriever from the persisted vector database.\n",
        "#         \"\"\"\n",
        "#         self.vectordb = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "#         self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "#     @staticmethod\n",
        "#     def remove_duplicate_chunks(chunks):\n",
        "#         \"\"\"\n",
        "#         Eliminates duplicate document chunks based on their content.\n",
        "#         \"\"\"\n",
        "#         seen = set()\n",
        "#         unique_chunks = []\n",
        "#         for chunk in chunks:\n",
        "#             chunk_text = chunk.page_content.strip()\n",
        "#             if chunk_text not in seen:\n",
        "#                 seen.add(chunk_text)\n",
        "#                 unique_chunks.append(chunk)\n",
        "#         return unique_chunks\n",
        "\n",
        "#     def truncate_context(self, context_list, max_tokens=800):\n",
        "#         \"\"\"\n",
        "#         Truncates the retrieved context to avoid overloading the model's input.\n",
        "#         \"\"\"\n",
        "#         truncated_docs = []\n",
        "#         current_tokens = 0\n",
        "#         for doc in context_list:\n",
        "#             doc_tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "#             if current_tokens + doc_tokens <= max_tokens:\n",
        "#                 truncated_docs.append(doc)\n",
        "#                 current_tokens += doc_tokens\n",
        "#             else:\n",
        "#                 break\n",
        "#         return truncated_docs\n",
        "\n",
        "#     @staticmethod\n",
        "#     def clean_user_input(user_input):\n",
        "#         \"\"\"\n",
        "#         Cleans and standardizes user input.\n",
        "#         \"\"\"\n",
        "#         return user_input.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "#     def reset_memory_if_needed(self):\n",
        "#         \"\"\"\n",
        "#         Clears conversation history if the number of exchanges exceeds a threshold.\n",
        "#         \"\"\"\n",
        "#         if len(self.memory.chat_memory.messages) > 6:\n",
        "#             print(\"\\nMemory Full: Resetting Conversation History...\\n\")\n",
        "#             self.memory.clear()\n",
        "\n",
        "#     def format_response(self, question, response):\n",
        "#         \"\"\"\n",
        "#         Formats the output to clearly present both the question and the answer.\n",
        "#         \"\"\"\n",
        "#         response_text = response.strip()\n",
        "#         unwanted_phrases = [\n",
        "#             \"Use the following pieces of context\",\n",
        "#             \"If you don't know the answer, just say that you don't know\",\n",
        "#             \"Don't try to make up an answer.\"\n",
        "#         ]\n",
        "#         for phrase in unwanted_phrases:\n",
        "#             if phrase in response_text:\n",
        "#                 response_text = response_text.split(phrase)[-1].strip()\n",
        "#         return f\"Question: {question}\\nHelpful Answer: {response_text}\"\n",
        "\n",
        "#     def trim_final_input(self, question, context, max_tokens=512):\n",
        "#       \"\"\"\n",
        "#       Truncates the final input to meet the token limit, preserving document metadata.\n",
        "#       \"\"\"\n",
        "#       system_message = (\n",
        "#           \"You are analyzing a bank's quarterly earnings call transcript. \"\n",
        "#           \"Provide a bullet-point summary of the most important takeaways with specific details: \"\n",
        "#           \"list key revenue trends (include any percentage changes if available), major expense drivers, \"\n",
        "#           \"and management's outlook for the future. If numerical details are not available, provide qualitative insights.\"\n",
        "#       )\n",
        "#       # Join the context documents into one coherent string.\n",
        "#       context_str = \"\\n\".join([doc.page_content for doc in context])\n",
        "#       input_text = f\"{system_message}\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "#       tokens = self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens)\n",
        "#       return self.tokenizer.decode(tokens)\n",
        "\n",
        "\n",
        "#     def answer_question(self, question):\n",
        "#         \"\"\"\n",
        "#         Processes the user query: retrieves context, prepares the prompt,\n",
        "#         and returns a formatted answer. If no relevant documents are retrieved,\n",
        "#         a fallback message is returned.\n",
        "#         \"\"\"\n",
        "#         question = self.clean_user_input(question)\n",
        "#         self.reset_memory_if_needed()\n",
        "\n",
        "#         # Retrieve and process context.\n",
        "#         context = self.retriever.get_relevant_documents(question)\n",
        "#         context = self.remove_duplicate_chunks(context)\n",
        "#         context = self.truncate_context(context, max_tokens=800)\n",
        "\n",
        "#         # Fallback: if no relevant context is found.\n",
        "#         if not context:\n",
        "#             return f\"Question: {question}\\nHelpful Answer: I don't have information regarding that query.\"\n",
        "\n",
        "#         print(\"\\nRetrieved Context:\")\n",
        "#         for doc in context:\n",
        "#             source = doc.metadata.get('source', 'Unknown Source')\n",
        "#             page = doc.metadata.get('page', 'Unknown Page')\n",
        "#             print(f\"- Source: {source}, Page: {page}\")\n",
        "\n",
        "#         # Enforce a 512-token limit for the final prompt.\n",
        "#         formatted_input = self.trim_final_input(question, context, max_tokens=512)\n",
        "#         response = self.qa_chain({\"question\": formatted_input})\n",
        "#         return self.format_response(question, response['answer'])\n",
        "\n",
        "#     def run_chatbot(self):\n",
        "#         \"\"\"\n",
        "#         Initiates an interactive loop for prompt-based queries.\n",
        "#         \"\"\"\n",
        "#         print(\"\\nðŸ’¬ Bank Earnings Chatbot (Type 'exit' to stop)\")\n",
        "#         while True:\n",
        "#             user_input = input(\"\\nYou: \")\n",
        "#             if user_input.lower() == \"exit\":\n",
        "#                 print(\"\\nExiting Chatbot. Have a great day!\")\n",
        "#                 break\n",
        "#             answer = self.answer_question(user_input)\n",
        "#             print(\"\\n\" + answer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ----------------------------\n",
        "# # Example usage of the BankEarningsChatbot class with T5, multiple data sources, and test mode enabled\n",
        "# # ----------------------------\n",
        "\n",
        "# # Define your PDF folder paths (ensure these paths contain your earnings transcripts in PDF format).\n",
        "# pdf_folders = [\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "#     \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "# ]\n",
        "\n",
        "# # Define the persistence directory for model outputs and the vector index.\n",
        "# persist_directory = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\n",
        "\n",
        "# # Instantiate the chatbot object.\n",
        "# # Set rebuild_index=True if you want to force re-indexing, otherwise it will load the persisted index if it exists.\n",
        "# chatbot = BankEarningsChatbot(pdf_folders, persist_directory=persist_directory, test_mode=False, rebuild_index=True)"
      ],
      "metadata": {
        "id": "mr-5lzazWgqc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "class BankEarningsChatbot:\n",
        "    \"\"\"\n",
        "    A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts. It loads PDF documents\n",
        "    from one or more specified folders, builds a Chroma vector index, and sets up an interactive\n",
        "    conversational chain for prompt-based queries.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_folders (list or str): Folder paths containing PDF files.\n",
        "        persist_directory (str): Directory to persist the vector index.\n",
        "        max_length (int): Maximum output length for the T5 model.\n",
        "        test_mode (bool): If True, loads only one PDF per folder for quick testing.\n",
        "        rebuild_index (bool): If True, reprocess all PDFs and rebuild the index.\n",
        "        verbose (bool): If True, prints additional debug information.\n",
        "    \"\"\"\n",
        "    def __init__(self, pdf_folders,\n",
        "                 persist_directory=\"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "                 max_length=256, test_mode=False, rebuild_index=False, verbose=False):\n",
        "        if isinstance(pdf_folders, str):\n",
        "            self.pdf_folders = [pdf_folders]\n",
        "        else:\n",
        "            self.pdf_folders = pdf_folders\n",
        "\n",
        "        self.persist_directory = persist_directory\n",
        "        self.test_mode = test_mode\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self._setup_llm(max_length)\n",
        "        self._setup_embeddings()\n",
        "\n",
        "        if rebuild_index or (not os.path.exists(self.persist_directory)) or (not os.listdir(self.persist_directory)):\n",
        "            self._load_documents()\n",
        "            self._build_vector_index()\n",
        "        else:\n",
        "            if self.verbose:\n",
        "                print(\"Loading existing vector index from persistence directory.\")\n",
        "            self.db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "\n",
        "        self._setup_retriever()\n",
        "        self.memory = ConversationBufferWindowMemory(k=10, memory_key=\"chat_history\", return_messages=True)\n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.retriever,\n",
        "            memory=self.memory,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    def _setup_llm(self, max_length):\n",
        "        self.model_name = \"google/flan-t5-large\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        # Set batch_size here so that the pipeline can process lists of prompts\n",
        "        self.pipe = pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=max_length,\n",
        "            temperature=0.5,\n",
        "            top_p=0.8,\n",
        "            do_sample=True,\n",
        "            batch_size=8\n",
        "        )\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        self.embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            bank_name = os.path.basename(folder).lower()\n",
        "            file_names = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            if not file_names:\n",
        "                continue\n",
        "            if self.test_mode:\n",
        "                file_names = file_names[:1]\n",
        "            for pdf_file in file_names:\n",
        "                pdf_path = os.path.join(folder, pdf_file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(pdf_path, extract_images=False)\n",
        "                    docs = loader.load_and_split()\n",
        "                    for doc in docs:\n",
        "                        doc.metadata[\"bank\"] = bank_name\n",
        "                    self.documents.extend(docs)\n",
        "                    if self.verbose:\n",
        "                        print(f\"Loaded: {pdf_file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    if self.verbose:\n",
        "                        print(f\"Error loading {pdf_file} from {folder}: {e}\")\n",
        "            if self.test_mode:\n",
        "                break\n",
        "\n",
        "    def _build_vector_index(self):\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        chunks = text_splitter.split_documents(self.documents)\n",
        "        chunks = self.remove_duplicate_chunks(chunks)\n",
        "        self.chunks = chunks\n",
        "        self.db = Chroma.from_documents(chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "        self.db.persist()\n",
        "\n",
        "    def _setup_retriever(self):\n",
        "        self.vectordb = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_duplicate_chunks(chunks):\n",
        "        seen = set()\n",
        "        unique_chunks = []\n",
        "        for chunk in chunks:\n",
        "            chunk_text = chunk.page_content.strip()\n",
        "            if chunk_text not in seen:\n",
        "                seen.add(chunk_text)\n",
        "                unique_chunks.append(chunk)\n",
        "        return unique_chunks\n",
        "\n",
        "    def truncate_context(self, context_list, max_tokens=800):\n",
        "        truncated_docs = []\n",
        "        current_tokens = 0\n",
        "        for doc in context_list:\n",
        "            doc_tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "            if current_tokens + doc_tokens <= max_tokens:\n",
        "                truncated_docs.append(doc)\n",
        "                current_tokens += doc_tokens\n",
        "            else:\n",
        "                break\n",
        "        return truncated_docs\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_user_input(user_input):\n",
        "        return user_input.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "    def reset_memory_if_needed(self):\n",
        "        if len(self.memory.chat_memory.messages) > 6:\n",
        "            if self.verbose:\n",
        "                print(\"\\nMemory Full: Resetting Conversation History...\\n\")\n",
        "            self.memory.clear()\n",
        "\n",
        "    def format_response(self, question, response):\n",
        "        if isinstance(response, dict) and 'answer' in response:\n",
        "            response_text = response['answer'].strip()\n",
        "        else:\n",
        "            response_text = str(response).strip()\n",
        "        unwanted_phrases = [\n",
        "            \"Use the following pieces of context\",\n",
        "            \"If you don't know the answer, just say that you don't know\",\n",
        "            \"Don't try to make up an answer.\"\n",
        "        ]\n",
        "        for phrase in unwanted_phrases:\n",
        "            if phrase in response_text:\n",
        "                response_text = response_text.split(phrase)[-1].strip()\n",
        "        return f\"Question: {question}\\nHelpful Answer: {response_text}\"\n",
        "\n",
        "    def trim_final_input(self, question, context, max_tokens=512):\n",
        "        system_message = (\n",
        "            \"You are analyzing a bank's quarterly earnings call transcript. \"\n",
        "            \"Provide a bullet-point summary of the most important takeaways with specific details: \"\n",
        "            \"list key revenue trends (include any percentage changes if available), major expense drivers, \"\n",
        "            \"and management's outlook for the future. If numerical details are not available, provide qualitative insights.\"\n",
        "        )\n",
        "        # --- New Step: Batch intermediate summarization for each context document ---\n",
        "        summarized_context = []\n",
        "        per_doc_limit = max_tokens // max(1, len(context))  # Distribute token limit among documents\n",
        "        batch_prompts = []\n",
        "        for doc in context:\n",
        "            doc_text = doc.page_content\n",
        "            tokens = self.tokenizer.encode(doc_text)\n",
        "            if len(tokens) > per_doc_limit:\n",
        "                summary_prompt = f\"Summarize the following text in a concise bullet-point format:\\n\\n{doc_text}\"\n",
        "                batch_prompts.append(summary_prompt)\n",
        "            else:\n",
        "                summarized_context.append(doc_text)\n",
        "        if batch_prompts:\n",
        "            # Process the batch of summarization prompts together\n",
        "            summary_responses = self.llm(batch_prompts)\n",
        "            for resp in summary_responses:\n",
        "                if isinstance(resp, dict) and 'answer' in resp:\n",
        "                    summarized_context.append(resp['answer'])\n",
        "                else:\n",
        "                    summarized_context.append(str(resp))\n",
        "        context_str = \"\\n\".join(summarized_context)\n",
        "        input_text = f\"{system_message}\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "        tokens = self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens)\n",
        "        return self.tokenizer.decode(tokens)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        question = self.clean_user_input(question)\n",
        "        self.reset_memory_if_needed()\n",
        "        context = self.retriever.get_relevant_documents(question)\n",
        "        context = self.remove_duplicate_chunks(context)\n",
        "        context = self.truncate_context(context, max_tokens=800)\n",
        "        if not context:\n",
        "            return f\"Question: {question}\\nHelpful Answer: I don't have information regarding that query.\"\n",
        "        if self.verbose:\n",
        "            print(\"\\nRetrieved Context:\")\n",
        "            for doc in context:\n",
        "                source = doc.metadata.get('source', 'Unknown Source')\n",
        "                page = doc.metadata.get('page', 'Unknown Page')\n",
        "                print(f\"- Source: {source}, Page: {page}\")\n",
        "        formatted_input = self.trim_final_input(question, context, max_tokens=512)\n",
        "        response = self.qa_chain(formatted_input)\n",
        "        return self.format_response(question, response)\n",
        "\n",
        "    def run_chatbot(self):\n",
        "        print(\"\\nðŸ’¬ Bank Earnings Chatbot (Type 'exit' to stop)\")\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \")\n",
        "            if user_input.lower() == \"exit\":\n",
        "                print(\"\\nExiting Chatbot. Have a great day!\")\n",
        "                break\n",
        "            answer = self.answer_question(user_input)\n",
        "            print(\"\\n\" + answer)\n",
        "\n",
        "    # ---------------- Multi-Step Methods for Year-on-Year & Quarterly Sentiment Analysis ----------------\n",
        "\n",
        "    def summarize_individual_transcripts(self):\n",
        "        prompts = []\n",
        "        sources = []\n",
        "        for doc in self.documents:\n",
        "            source = doc.metadata.get('source', 'Unknown Source')\n",
        "            bank = doc.metadata.get('bank', 'unknown')\n",
        "            transcript_text = doc.page_content\n",
        "            prompt = f\"Please provide a bullet-point sentiment summary for the following transcript:\\n\\n{transcript_text}\"\n",
        "            prompts.append(prompt)\n",
        "            sources.append((source, bank))\n",
        "        # Batch process all transcript prompts\n",
        "        responses = self.llm(prompts)\n",
        "        summaries = {}\n",
        "        for (source, bank), response in zip(sources, responses):\n",
        "            if isinstance(response, dict) and 'answer' in response:\n",
        "                answer_text = response['answer']\n",
        "            else:\n",
        "                answer_text = str(response)\n",
        "            summaries[source] = {\"bank\": bank, \"summary\": answer_text}\n",
        "            if self.verbose:\n",
        "                print(f\"Summarized transcript from {source}\")\n",
        "        return summaries\n",
        "\n",
        "    def group_summaries_by_bank_and_quarter(self, summaries):\n",
        "        grouped = {}\n",
        "        for source, info in summaries.items():\n",
        "            bank = info[\"bank\"]\n",
        "            summary = info[\"summary\"]\n",
        "            match = re.search(r'(\\d{1}[qQ]|[qQ]\\d)[-_]?(\\d{2,4})', source)\n",
        "            if match:\n",
        "                quarter_raw = match.group(1)\n",
        "                year = match.group(2)\n",
        "                quarter_num = re.search(r'\\d', quarter_raw).group(0)\n",
        "                key = f\"{year}-Q{quarter_num}\"\n",
        "            else:\n",
        "                key = \"Unknown\"\n",
        "            if bank not in grouped:\n",
        "                grouped[bank] = {}\n",
        "            if key not in grouped[bank]:\n",
        "                grouped[bank][key] = []\n",
        "            grouped[bank][key].append(summary)\n",
        "        return grouped\n",
        "\n",
        "    def aggregate_quarterly_summaries_by_bank(self, grouped_quarterly):\n",
        "        quarterly_aggregates = {}\n",
        "        for bank, quarters in grouped_quarterly.items():\n",
        "            quarterly_aggregates[bank] = {}\n",
        "            for key, summaries in quarters.items():\n",
        "                combined = \"\\n\".join(summaries)\n",
        "                prompt = (f\"Based on the following quarterly sentiment summaries for {bank.upper()} ({key}), \"\n",
        "                          \"provide a concise bullet-point overview of the overall sentiment for that quarter:\\n\\n\" + combined)\n",
        "                response = self.llm(prompt)\n",
        "                if isinstance(response, dict) and 'answer' in response:\n",
        "                    answer_text = response['answer']\n",
        "                else:\n",
        "                    answer_text = str(response)\n",
        "                quarterly_aggregates[bank][key] = answer_text\n",
        "        return quarterly_aggregates\n",
        "\n",
        "    def forecast_next_quarter_sentiment(self, bank, historical_quarterly):\n",
        "        combined = \"\\n\".join(historical_quarterly)\n",
        "        prompt = (\n",
        "            f\"Based on the following historical quarterly sentiment summaries for {bank.upper()}, \"\n",
        "            \"forecast the overall sentiment for the next quarter. Provide a bullet-point summary of the expected trends, \"\n",
        "            \"including any changes in tone, risk factors, or optimism:\\n\\n\" + combined\n",
        "        )\n",
        "        response = self.llm(prompt)\n",
        "        if isinstance(response, dict) and 'answer' in response:\n",
        "            return response['answer']\n",
        "        return str(response)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_quarter_key(key):\n",
        "        try:\n",
        "            parts = key.split(\"-\")\n",
        "            year = int(parts[0])\n",
        "            quarter = int(re.search(r'\\d', parts[1]).group(0))\n",
        "            return year, quarter\n",
        "        except Exception:\n",
        "            return (0, 0)\n",
        "\n",
        "    def analyze_and_forecast_sentiment_by_bank(self):\n",
        "        print(\"Generating individual transcript summaries...\")\n",
        "        summaries = self.summarize_individual_transcripts()\n",
        "        print(\"Grouping summaries by bank and quarter...\")\n",
        "        grouped_quarterly = self.group_summaries_by_bank_and_quarter(summaries)\n",
        "        print(\"Aggregating quarterly summaries...\")\n",
        "        quarterly_aggregates = self.aggregate_quarterly_summaries_by_bank(grouped_quarterly)\n",
        "\n",
        "        analysis = {}\n",
        "        for bank, quarters in quarterly_aggregates.items():\n",
        "            analysis[bank] = {}\n",
        "            valid_keys = [k for k in quarters.keys() if k != \"Unknown\"]\n",
        "            if not valid_keys:\n",
        "                continue\n",
        "            sorted_keys = sorted(valid_keys, key=lambda k: self.parse_quarter_key(k))\n",
        "            most_recent_key = sorted_keys[-1]\n",
        "            current_year, current_quarter = self.parse_quarter_key(most_recent_key)\n",
        "\n",
        "            years = sorted({self.parse_quarter_key(k)[0] for k in quarters if k != \"Unknown\"})\n",
        "            previous_year = max([y for y in years if y < current_year], default=None)\n",
        "            previous_year_summaries = []\n",
        "            if previous_year is not None:\n",
        "                for key in quarters:\n",
        "                    year, _ = self.parse_quarter_key(key)\n",
        "                    if year == previous_year:\n",
        "                        previous_year_summaries.extend(quarters[key])\n",
        "                combined_prev = \"\\n\".join(previous_year_summaries)\n",
        "                prompt_prev = (f\"Based on the following sentiment summaries for all quarters in {previous_year} for {bank.upper()}, \"\n",
        "                               \"provide a bullet-point summary of the overall sentiment trends for that year:\\n\\n\" + combined_prev)\n",
        "                response_prev = self.llm(prompt_prev)\n",
        "                if isinstance(response_prev, dict) and 'answer' in response_prev:\n",
        "                    previous_year_summary = response_prev['answer']\n",
        "                else:\n",
        "                    previous_year_summary = str(response_prev)\n",
        "            else:\n",
        "                previous_year_summary = \"Not available\"\n",
        "\n",
        "            current_summary = quarters.get(most_recent_key, \"Not available\")\n",
        "\n",
        "            historical = []\n",
        "            for key in sorted_keys:\n",
        "                historical.extend(quarters[key])\n",
        "            forecast = self.forecast_next_quarter_sentiment(bank, historical) if historical else \"Not available\"\n",
        "\n",
        "            analysis[bank] = {\n",
        "                \"previous_year_summary\": previous_year_summary,\n",
        "                \"current_quarter_summary\": current_summary,\n",
        "                \"forecast_next_quarter\": forecast,\n",
        "                \"most_recent_key\": most_recent_key\n",
        "            }\n",
        "        return analysis\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage:\n",
        "pdf_folders = [\n",
        "    \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "    \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "]\n",
        "persist_directory = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\n",
        "chatbot = BankEarningsChatbot(pdf_folders, persist_directory=persist_directory, test_mode=False, rebuild_index=True, verbose=False)\n",
        "\n",
        "analysis = chatbot.analyze_and_forecast_sentiment_by_bank()\n",
        "\n",
        "print(\"Yearly and Quarterly Sentiment Analysis by Bank:\")\n",
        "for bank, data in analysis.items():\n",
        "    print(f\"{bank.upper()} Analysis:\")\n",
        "    print(f\"Most Recent Quarter Key: {data.get('most_recent_key','N/A')}\")\n",
        "    print(\"Previous Year Sentiment Summary:\")\n",
        "    print(data[\"previous_year_summary\"])\n",
        "    print(\"Current Quarter Sentiment Summary:\")\n",
        "    print(data[\"current_quarter_summary\"])\n",
        "    print(\"Forecast for Next Quarter:\")\n",
        "    print(data[\"forecast_next_quarter\"])\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "sUdI42wVCFY5",
        "outputId": "ab5e57b0-1e09-49b4-a2ae-2fb99fcaeaa0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not connect to tenant default_tenant. Are you sure it exists?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36m_validate_tenant_database\u001b[0;34m(self, tenant, database)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_admin_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tenant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36mget_tenant\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_tenant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTenant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tenant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36mget_tenant\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_tenant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTenant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sysdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tenant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/db/mixins/sysdb.py\u001b[0m in \u001b[0;36mget_tenant\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: disk I/O error",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4654f212caab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m ]\n\u001b[1;32m    371\u001b[0m \u001b[0mpersist_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m \u001b[0mchatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBankEarningsChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebuild_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_and_forecast_sentiment_by_bank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4654f212caab>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pdf_folders, persist_directory, max_length, test_mode, rebuild_index, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrebuild_index\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_vector_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4654f212caab>\u001b[0m in \u001b[0;36m_build_vector_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_duplicate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    888\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m         chroma_collection = cls(\n\u001b[0m\u001b[1;32m    818\u001b[0m             \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0m_client_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_client_settings\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_client_settings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             self._persist_directory = (  # type: ignore[has-type]\n\u001b[1;32m    124\u001b[0m                 \u001b[0m_client_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist_directory\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpersist_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/__init__.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(settings, tenant, database)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mClientCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Create an admin client for verifying that databases and tenants exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_admin_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdminClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tenant_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit_client_start_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36m_validate_tenant_database\u001b[0;34m(self, tenant, database)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0;34mf\"Could not connect to tenant {tenant}. Are you sure it exists?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not connect to tenant default_tenant. Are you sure it exists?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Questions:\n",
        "\n",
        "\"What is the overall sentiment of the latest earnings call for JPMorgan?\"\n",
        "\"Summarize the key takeaways from UBS's most recent earnings call.\"\n",
        "Comparative Questions:\n",
        "\n",
        "\"How does the sentiment in Q4 2023 compare to Q4 2022 for JPMorgan?\"\n",
        "\"What are the major changes in sentiment between last year and this year for UBS?\"\n",
        "Forecasting and Trend Analysis:\n",
        "\n",
        "\"Based on historical transcripts, what is the forecast sentiment for the next quarter for JPMorgan?\"\n",
        "\"What trends in tone and risk are emerging from UBS's quarterly transcripts over the past two years?\"\n",
        "Detailed Breakdown:\n",
        "\n",
        "\"What are the key revenue trends and expense drivers mentioned in the transcripts for UBS?\"\n",
        "\"Provide a bullet-point summary of managementâ€™s outlook from the latest earnings calls for JPMorgan.\""
      ],
      "metadata": {
        "id": "CRqNRKPrHPqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interactive Chatbot Session:\n",
        "By calling chatbot.run_chatbot(), you launch an interactive loop. In this mode, the program continuously waits for user input from the command line. As the user types questions, the chatbot processes each one in real time and prints the response. This mode is ideal for a live, conversational experience where the operator manually drives the dialogue.\n",
        "\n",
        "Programmatic Prompt Processing:\n",
        "Instead of an interactive loop, you can supply a list of predefined prompts (as shown in the example). The code then iterates over this list, calling chatbot.answer_question(prompt) for each query. It prints both the prompt and the corresponding answer. This approach is useful for batch testing, automated evaluations, or when you want to process a fixed set of queries without manual intervention."
      ],
      "metadata": {
        "id": "2FQb2lbQV136"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the chatbot object"
      ],
      "metadata": {
        "id": "jaKJRslPWoj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch an interactive  Chatbot session"
      ],
      "metadata": {
        "id": "CLH9TcRtWL85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.run_chatbot()"
      ],
      "metadata": {
        "id": "g5TI2IMJWLQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}