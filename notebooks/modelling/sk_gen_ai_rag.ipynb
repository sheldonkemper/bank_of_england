{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcCHgfmdAvE9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_gen_ai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.2\n",
        "\n",
        "Description:\n",
        "    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\n",
        "    The code performs the following tasks:\n",
        "\n",
        "    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\n",
        "    2. Sets up sentence-transformer based embeddings for document vectorization.\n",
        "    3. Loads and splits PDF documents from one or more specified directories.\n",
        "    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\n",
        "    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\n",
        "    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\n",
        "    7. Maintains conversation memory for interactive sessions.\n",
        "    8. Supports both interactive and programmatic prompt-based querying.\n",
        "    9. Includes a 'test_mode' option for quick testing with a single PDF.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "i9Yx7FQ9BUYc",
        "outputId": "ab101214-3ad2-4801-f9ec-8b2440819bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.2\\n\\nDescription:\\n    This notebook contains a class-based implementation of a Retrieval Augmented Generation (RAG) engine\\n    designed to analyze bank quarterly earnings call transcripts (in PDF format) stored on Google Drive.\\n    The code performs the following tasks:\\n\\n    1. Configures an LLM pipeline using a Flan-T5-based model for text summarization.\\n    2. Sets up sentence-transformer based embeddings for document vectorization.\\n    3. Loads and splits PDF documents from one or more specified directories.\\n    4. Chunks the documents and builds a vector index using Chroma, persisting the index to Google Drive.\\n    5. Optionally loads an existing persisted vector index to avoid re-indexing, via the 'rebuild_index' parameter.\\n    6. Retrieves context relevant to user queries from the vector index with token truncation to enforce input limits.\\n    7. Maintains conversation memory for interactive sessions.\\n    8. Supports both interactive and programmatic prompt-based querying.\\n    9. Includes a 'test_mode' option for quick testing with a single PDF.\\n===================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install langchain-community\n",
        "!pip install -q langchain-community pypdf tiktoken chromadb sentence-transformers > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "R723tJpzCLou"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "b3cgTJG8XNh2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")\n"
      ],
      "metadata": {
        "id": "r-kX_dVDek9n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive to the root location with force_remount\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAkI_0h4A9Zj",
        "outputId": "ca6a7183-1447-45c8-e6ca-b3025581e66a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine"
      ],
      "metadata": {
        "id": "H4WpERwsXCLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JL-O9CQQAvFB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class BankEarningsChatbot:\n",
        "    \"\"\"\n",
        "    A class-based implementation of an LLM Retrieval Augmented Generation (RAG) engine\n",
        "    designed to analyze bank quarterly earnings call transcripts. It loads PDF documents\n",
        "    from one or more specified folders, builds a Chroma vector index, and sets up an interactive\n",
        "    conversational chain for prompt-based queries.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_folders (list or str): A list of folder paths containing PDF files, or a single folder path as a string.\n",
        "        persist_directory (str): Directory path to persist the vector index and model outputs.\n",
        "        max_length (int): Maximum output length for the T5 model.\n",
        "        test_mode (bool): If True, only loads one PDF (from the first folder) for quick testing.\n",
        "        rebuild_index (bool): If True, reprocess all PDFs and rebuild the index even if a persisted index exists.\n",
        "    \"\"\"\n",
        "    def __init__(self, pdf_folders,\n",
        "                 persist_directory=\"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\",\n",
        "                 max_length=256, test_mode=False, rebuild_index=False):\n",
        "        # Allow a single folder (string) or a list of folders.\n",
        "        if isinstance(pdf_folders, str):\n",
        "            self.pdf_folders = [pdf_folders]\n",
        "        else:\n",
        "            self.pdf_folders = pdf_folders\n",
        "\n",
        "        self.persist_directory = persist_directory\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "        # Set up the LLM pipeline using the Flan-T5 model.\n",
        "        self._setup_llm(max_length)\n",
        "\n",
        "        # Configure embeddings.\n",
        "        self._setup_embeddings()\n",
        "\n",
        "        # Check if we need to rebuild the vector index.\n",
        "        if rebuild_index or (not os.path.exists(self.persist_directory)) or (not os.listdir(self.persist_directory)):\n",
        "            # Load documents and build the vector index.\n",
        "            self._load_documents()\n",
        "            self._build_vector_index()\n",
        "        else:\n",
        "            print(\"Loading existing vector index from persistence directory.\")\n",
        "            self.db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "            # Note: If you need to update the in-memory index from the persisted data, this method should suffice.\n",
        "\n",
        "        # Configure retriever from the persisted vector database.\n",
        "        self._setup_retriever()\n",
        "\n",
        "        # Initialize conversation memory.\n",
        "        self.memory = ConversationBufferWindowMemory(k=3, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "        # Set up the conversational retrieval chain.\n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.retriever,\n",
        "            memory=self.memory,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    def _setup_llm(self, max_length):\n",
        "        \"\"\"\n",
        "        Configures the language model pipeline using a T5 model.\n",
        "        \"\"\"\n",
        "        self.model_name = \"google/flan-t5-large\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        # Using the text2text-generation pipeline for T5.\n",
        "        self.pipe = pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=max_length,\n",
        "            temperature=0.5,\n",
        "            top_p=0.8,\n",
        "            do_sample=True\n",
        "        )\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "    def _setup_embeddings(self):\n",
        "        \"\"\"\n",
        "        Initializes the sentence-transformer based embeddings.\n",
        "        \"\"\"\n",
        "        self.embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "    def _load_documents(self):\n",
        "        \"\"\"\n",
        "        Loads and splits PDF documents from the specified folders.\n",
        "        In test_mode, only the first PDF (from the first folder) is loaded.\n",
        "        \"\"\"\n",
        "        self.documents = []\n",
        "        for folder in self.pdf_folders:\n",
        "            file_names = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "            if not file_names:\n",
        "                continue\n",
        "            # If in test_mode, load only the first PDF file from this folder.\n",
        "            if self.test_mode:\n",
        "                file_names = file_names[:1]\n",
        "            for pdf_file in file_names:\n",
        "                pdf_path = os.path.join(folder, pdf_file)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(pdf_path, extract_images=False)\n",
        "                    self.documents.extend(loader.load_and_split())\n",
        "                    print(f\"Loaded: {pdf_file} from {folder}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {pdf_file} from {folder}: {e}\")\n",
        "            # In test mode, break after processing the first folder.\n",
        "            if self.test_mode:\n",
        "                break\n",
        "\n",
        "    def _build_vector_index(self):\n",
        "        \"\"\"\n",
        "        Chunks documents and builds a Chroma vector index.\n",
        "        \"\"\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        chunks = text_splitter.split_documents(self.documents)\n",
        "        # Remove duplicate chunks.\n",
        "        chunks = self.remove_duplicate_chunks(chunks)\n",
        "        self.chunks = chunks\n",
        "        self.db = Chroma.from_documents(chunks, embedding=self.embeddings, persist_directory=self.persist_directory)\n",
        "        self.db.persist()\n",
        "\n",
        "    def _setup_retriever(self):\n",
        "        \"\"\"\n",
        "        Initializes the retriever from the persisted vector database.\n",
        "        \"\"\"\n",
        "        self.vectordb = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)\n",
        "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_duplicate_chunks(chunks):\n",
        "        \"\"\"\n",
        "        Eliminates duplicate document chunks based on their content.\n",
        "        \"\"\"\n",
        "        seen = set()\n",
        "        unique_chunks = []\n",
        "        for chunk in chunks:\n",
        "            chunk_text = chunk.page_content.strip()\n",
        "            if chunk_text not in seen:\n",
        "                seen.add(chunk_text)\n",
        "                unique_chunks.append(chunk)\n",
        "        return unique_chunks\n",
        "\n",
        "    def truncate_context(self, context_list, max_tokens=800):\n",
        "        \"\"\"\n",
        "        Truncates the retrieved context to avoid overloading the model's input.\n",
        "        \"\"\"\n",
        "        truncated_docs = []\n",
        "        current_tokens = 0\n",
        "        for doc in context_list:\n",
        "            doc_tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "            if current_tokens + doc_tokens <= max_tokens:\n",
        "                truncated_docs.append(doc)\n",
        "                current_tokens += doc_tokens\n",
        "            else:\n",
        "                break\n",
        "        return truncated_docs\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_user_input(user_input):\n",
        "        \"\"\"\n",
        "        Cleans and standardizes user input.\n",
        "        \"\"\"\n",
        "        return user_input.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "    def reset_memory_if_needed(self):\n",
        "        \"\"\"\n",
        "        Clears conversation history if the number of exchanges exceeds a threshold.\n",
        "        \"\"\"\n",
        "        if len(self.memory.chat_memory.messages) > 6:\n",
        "            print(\"\\nMemory Full: Resetting Conversation History...\\n\")\n",
        "            self.memory.clear()\n",
        "\n",
        "    def format_response(self, question, response):\n",
        "        \"\"\"\n",
        "        Formats the output to clearly present both the question and the answer.\n",
        "        \"\"\"\n",
        "        response_text = response.strip()\n",
        "        unwanted_phrases = [\n",
        "            \"Use the following pieces of context\",\n",
        "            \"If you don't know the answer, just say that you don't know\",\n",
        "            \"Don't try to make up an answer.\"\n",
        "        ]\n",
        "        for phrase in unwanted_phrases:\n",
        "            if phrase in response_text:\n",
        "                response_text = response_text.split(phrase)[-1].strip()\n",
        "        return f\"Question: {question}\\nHelpful Answer: {response_text}\"\n",
        "\n",
        "    def trim_final_input(self, question, context, max_tokens=512):\n",
        "        \"\"\"\n",
        "        Truncates the final input to meet the token limit, preserving document metadata.\n",
        "        \"\"\"\n",
        "        system_message = (\n",
        "            \"You are analyzing a bank's quarterly earnings call transcript.\\n\"\n",
        "            \"Extract and summarize key financial insights, avoiding unnecessary details.\\n\"\n",
        "            \"If the answer isn't found, respond with 'I don't know.'\\n\"\n",
        "            \"Provide sources for your answers at the end.\"\n",
        "        )\n",
        "        # Join the context documents into one coherent string.\n",
        "        context_str = \"\\n\".join([doc.page_content for doc in context])\n",
        "        input_text = f\"{system_message}\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "        tokens = self.tokenizer.encode(input_text, truncation=True, max_length=max_tokens)\n",
        "        return self.tokenizer.decode(tokens)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"\n",
        "        Processes the user query: retrieves context, prepares the prompt,\n",
        "        and returns a formatted answer. If no relevant documents are retrieved,\n",
        "        a fallback message is returned.\n",
        "        \"\"\"\n",
        "        question = self.clean_user_input(question)\n",
        "        self.reset_memory_if_needed()\n",
        "\n",
        "        # Retrieve and process context.\n",
        "        context = self.retriever.get_relevant_documents(question)\n",
        "        context = self.remove_duplicate_chunks(context)\n",
        "        context = self.truncate_context(context, max_tokens=800)\n",
        "\n",
        "        # Fallback: if no relevant context is found.\n",
        "        if not context:\n",
        "            return f\"Question: {question}\\nHelpful Answer: I don't have information regarding that query.\"\n",
        "\n",
        "        print(\"\\nRetrieved Context:\")\n",
        "        for doc in context:\n",
        "            source = doc.metadata.get('source', 'Unknown Source')\n",
        "            page = doc.metadata.get('page', 'Unknown Page')\n",
        "            print(f\"- Source: {source}, Page: {page}\")\n",
        "\n",
        "        # Enforce a 512-token limit for the final prompt.\n",
        "        formatted_input = self.trim_final_input(question, context, max_tokens=512)\n",
        "        response = self.qa_chain({\"question\": formatted_input})\n",
        "        return self.format_response(question, response['answer'])\n",
        "\n",
        "    def run_chatbot(self):\n",
        "        \"\"\"\n",
        "        Initiates an interactive loop for prompt-based queries.\n",
        "        \"\"\"\n",
        "        print(\"\\nüí¨ Bank Earnings Chatbot (Type 'exit' to stop)\")\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \")\n",
        "            if user_input.lower() == \"exit\":\n",
        "                print(\"\\nExiting Chatbot. Have a great day!\")\n",
        "                break\n",
        "            answer = self.answer_question(user_input)\n",
        "            print(\"\\n\" + answer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the transcripts and retrieval‚Äêaugmented setup, here are some recommendations for crafting prompts that are likely to yield the most accurate and domain‚Äêspecific responses:\n",
        "\n",
        "- **Be Specific About the Timeframe:**  \n",
        "  Instead of asking ‚ÄúWhat were the key insights?‚Äù specify the quarter or transcript you‚Äôre interested in. For example:  \n",
        "  - \"What were the key financial insights from the Q4 2023 earnings call?\"  \n",
        "  - \"Summarize the main drivers of revenue in the Q1 2023 transcript.\"\n",
        "\n",
        "- **Target Specific Financial Metrics or Themes:**  \n",
        "  Focus on particular areas the transcripts cover, such as revenue trends, expense drivers, or capital performance. For example:  \n",
        "  - \"How did revenue change compared to the previous quarter in the Q4 2023 earnings call?\"  \n",
        "  - \"What were the primary expense drivers discussed in the Q4 2023 transcript?\"\n",
        "\n",
        "- **Incorporate Domain-Specific Language:**  \n",
        "  Use terminology that reflects the financial domain to guide the model. For example:  \n",
        "  - \"What risk factors and forward-looking statements were highlighted in the Q3 2023 transcript?\"  \n",
        "  - \"Outline the key operational challenges and strategic responses mentioned in the earnings call.\"\n",
        "\n",
        "- **Prompt for Summaries and Insights:**  \n",
        "  Asking for summaries can help the model focus on extracting concise information from large volumes of text. For example:  \n",
        "  - \"Provide a concise summary of the key financial insights from the Q4 2023 earnings transcript, including revenue, expenses, and capital allocation.\"  \n",
        "  - \"What are the overall sentiments and key management strategies discussed in the transcript?\"\n",
        "\n",
        "By tailoring your queries with specific quarters, financial metrics, and industry language, you guide the retrieval and summarization process more effectively. This structured approach should lead to more precise and contextually relevant responses from your system."
      ],
      "metadata": {
        "id": "CRqNRKPrHPqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interactive Chatbot Session:\n",
        "By calling chatbot.run_chatbot(), you launch an interactive loop. In this mode, the program continuously waits for user input from the command line. As the user types questions, the chatbot processes each one in real time and prints the response. This mode is ideal for a live, conversational experience where the operator manually drives the dialogue.\n",
        "\n",
        "Programmatic Prompt Processing:\n",
        "Instead of an interactive loop, you can supply a list of predefined prompts (as shown in the example). The code then iterates over this list, calling chatbot.answer_question(prompt) for each query. It prints both the prompt and the corresponding answer. This approach is useful for batch testing, automated evaluations, or when you want to process a fixed set of queries without manual intervention."
      ],
      "metadata": {
        "id": "2FQb2lbQV136"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the chatbot object"
      ],
      "metadata": {
        "id": "jaKJRslPWoj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Example usage of the BankEarningsChatbot class with T5, multiple data sources, and test mode enabled\n",
        "# ----------------------------\n",
        "\n",
        "# Define your PDF folder paths (ensure these paths contain your earnings transcripts in PDF format).\n",
        "pdf_folders = [\n",
        "    \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\",\n",
        "    \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\"\n",
        "]\n",
        "\n",
        "# Define the persistence directory for model outputs and the vector index.\n",
        "persist_directory = \"/content/drive/MyDrive/BOE/bank_of_england/data/model_outputs\"\n",
        "\n",
        "# Instantiate the chatbot object.\n",
        "# Set rebuild_index=True if you want to force re-indexing, otherwise it will load the persisted index if it exists.\n",
        "chatbot = BankEarningsChatbot(pdf_folders, persist_directory=persist_directory, test_mode=False, rebuild_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr-5lzazWgqc",
        "outputId": "a13ca9c3-be43-4014-d1bd-7d4d669d42b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 1q23-earnings-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: 2q23-earnings-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: 4q24-earnings-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-1q24-earnings-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-2q24-earnings-call-transcript-final.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-3q23-earnings-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpm-4q23-earnings-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\n",
            "Loaded: 1q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 1q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 2q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 2q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 3q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 3q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 4q23-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n",
            "Loaded: 4q24-earnings-call-remarks.pdf from /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For debugging, process a list of prompts programmatically"
      ],
      "metadata": {
        "id": "f8u8NRRIWxz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--vLwn2sAvFE",
        "outputId": "b28cf3a4-9b46-492e-bf1d-2c1d17ff4d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-20538e9b015c>:216: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  context = self.retriever.get_relevant_documents(question)\n",
            "<ipython-input-9-20538e9b015c>:232: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = self.qa_chain({\"question\": formatted_input})\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (992 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 1\n",
            "Question: What were the key insights from the latest earnings call for JP Morgan?\n",
            "Response: Question: What were the key insights from the latest earnings call for JP Morgan?\n",
            "Helpful Answer: Firmwide IB fees were up 18% year-on-year, reflecting particular strength in underwriting fees, and we've seen strong net inflows across AWM as well as in the CCB Wealth Management business.\n",
            "------------------------------------------------------------\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-3q23-earnings-call-transcript.pdf, Page: 3\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 1\n",
            "Question: How did revenue change in Q4 2024 compared to the previous Q3 2024 for JP Morgan?\n",
            "Response: Question: How did revenue change in Q4 2024 compared to the previous Q3 2024 for JP Morgan?\n",
            "Helpful Answer: down 3% year-on-year\n",
            "------------------------------------------------------------\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf, Page: 18\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-3q23-earnings-call-transcript.pdf, Page: 17\n",
            "Question: What risk factors were identified in the transcript for JP Morgan in Q4 2024?\n",
            "Response: Question: What risk factors were identified in the transcript for JP Morgan in Q4 2024?\n",
            "Helpful Answer: Factors that could cause JPMorgan Chase & Co.‚Äôs actual results to differ materially from those described in the forward-looking statements\n",
            "------------------------------------------------------------\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/4q24-earnings-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf, Page: 1\n",
            "Question: What is the overall sentiment of the earnings call for JP Morgan Q4 2024?\n",
            "Response: Question: What is the overall sentiment of the earnings call for JP Morgan Q4 2024?\n",
            "Helpful Answer: positive\n",
            "------------------------------------------------------------\n",
            "\n",
            "Memory Full: Resetting Conversation History...\n",
            "\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/1q23-earnings-call-remarks.pdf, Page: 0\n",
            "Question: Provide a table of the sentiment for each bank (JP Morgan, UBS), year 2023 and 2024 and quarter 1 to 4\n",
            "Response: Question: Provide a table of the sentiment for each bank (JP Morgan, UBS), year 2023 and 2024 and quarter 1 to 4\n",
            "Helpful Answer: I don't know\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Process a list of prompts programmatically.\n",
        "prompts = [\n",
        "    \"What were the key insights from the latest earnings call for JP Morgan?\",\n",
        "    \"How did revenue change in Q4 2024 compared to the previous Q3 2024 for JP Morgan?\",\n",
        "    \"What risk factors were identified in the transcript for JP Morgan in Q4 2024?\",\n",
        "    \"What is the overall sentiment of the earnings call for JP Morgan Q4 2024?\",\n",
        "    \"Provide a table of the sentiment for each bank (JP Morgan, UBS), year 2023 and 2024 and quarter 1 to 4\"# This might trigger the fallback if off-topic.\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    response = chatbot.answer_question(prompt)\n",
        "    print(\"Question:\", prompt)\n",
        "    print(\"Response:\", response)\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1nHYviQLaMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch an interactive  Chatbot session"
      ],
      "metadata": {
        "id": "CLH9TcRtWL85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.run_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g5TI2IMJWLQX",
        "outputId": "a40a1e1f-dd1d-4402-c35d-db08b03294f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üí¨ Bank Earnings Chatbot (Type 'exit' to stop)\n",
            "\n",
            "Memory Full: Resetting Conversation History...\n",
            "\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-4q23-earnings-call-transcript.pdf, Page: 0\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 0\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/4q24-earnings-transcript.pdf, Page: 0\n",
            "\n",
            "Question: What were the key financial insights from the Q4 2023 earnings call?\n",
            "Helpful Answer: I don't know\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/1q23-earnings-call-remarks.pdf, Page: 0\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/4q24-earnings-call-remarks.pdf, Page: 0\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/2q24-earnings-call-remarks.pdf, Page: 0\n",
            "\n",
            "Question: What were the key financial insights from the Q4 2023 earnings call for UBS bank?\n",
            "Helpful Answer: not enough information\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/2q23-earnings-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf, Page: 1\n",
            "\n",
            "Question: how much revenue did JP Morgan make in 2023?\n",
            "Helpful Answer: $725 million increase\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 17\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-3q23-earnings-call-transcript.pdf, Page: 17\n",
            "\n",
            "Question: What is the sentiment for JP Morgan in the most recent quarter of 2024?\n",
            "Helpful Answer: $725 million\n",
            "\n",
            "Memory Full: Resetting Conversation History...\n",
            "\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpmc-third-quarter-2024-earnings-conference-call-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/4q24-earnings-transcript.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-1q24-earnings-call-transcript.pdf, Page: 9\n",
            "\n",
            "Question: in jpm-1q24-earnings-call-transcript what is the sentiment?\n",
            "Helpful Answer: I don't know\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-2q24-earnings-call-transcript-final.pdf, Page: 11\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/jpm-2q24-earnings-call-transcript-final.pdf, Page: 8\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan/4q24-earnings-transcript.pdf, Page: 7\n",
            "\n",
            "Question: show me all the transcripts\n",
            "Helpful Answer: January 12, 2024\n",
            "\n",
            "Retrieved Context:\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/4q24-earnings-call-remarks.pdf, Page: 1\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/2q24-earnings-call-remarks.pdf, Page: 11\n",
            "- Source: /content/drive/MyDrive/BOE/bank_of_england/data/raw/ubs/3q24-earnings-call-remarks.pdf, Page: 0\n",
            "\n",
            "Question: how much revenue did UBS make in 2023?\n",
            "Helpful Answer: $1.15\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6c22d766e62b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-d767584868f3>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüí¨ Bank Earnings Chatbot (Type 'exit' to stop)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExiting Chatbot. Have a great day!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v65X7FRWZV0h"
      ],
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}