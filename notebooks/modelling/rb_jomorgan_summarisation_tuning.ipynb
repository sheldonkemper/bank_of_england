{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/rb_jomorgan_summarisation_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FLAN-T5-Large** is tested for text summarisation using JPMorgan Financial transcripts.\n",
        "\n",
        "The model is applied to three different parts of the transcripts:\n",
        "\n",
        "**1)** individual financial analyst questions\n",
        "\n",
        "**2)** responses to these questions.\n",
        "\n",
        "**Different prompts** are applied on the three points above to allow the extraction of tailored outputs that serve different purposes:\n",
        "\n",
        "**1)** *prompt = f\"Extract and summarize the key questions asked by analysts in the following text: {text}\"*\n",
        "\n",
        "**2)** *prompt = f\"Summarize by extracting different statements the following text: {text}\"*\n",
        "\n",
        "**3)** *prompt = f\"Rewrite the following text into a concise and original summary while maintaining its key ideas: {text}\"*\n",
        "\n",
        "**ROUGE scores**, which measure alignment with reference texts through precision, recall, and F-measure, are used, helping assess models performance.\n"
      ],
      "metadata": {
        "id": "L5J2VwEbPrGV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "4_UYXE_38U87"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic umap-learn hdbscan sentence-transformers > /dev/null 2>&1\n",
        "!pip install transformers torch > /dev/null 2>&1\n",
        "!pip install rouge_score > /dev/null 2>&1\n",
        "!pip install evaluate > /dev/null 2>&1\n",
        "!pip install --upgrade protobuf > /dev/null 2>&1\n",
        "!pip install tensorboard > /dev/null 2>&1\n",
        "!pip install tensorflow > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siaJxgIwEUqh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List, Union, Optional\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_session():\n",
        "    tf.keras.backend.clear_session()\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "VnoqUmd7GT93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data (questions and answers for JPM and UBS)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "kCGH8oLgAg3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path1 = \"/content/drive/MyDrive/BOE/bank_of_england/data/preprocessed_data/jp_morgan_qna.csv\"\n",
        "\n",
        "path2 = \"/content/drive/MyDrive/BOE/bank_of_england/data/preprocessed_data/ubs_qa_df_preprocessed_ver2.csv\"\n",
        "\n",
        "JP_qna = pd.read_csv(path1)\n",
        "UBS_qna = pd.read_csv(path2)"
      ],
      "metadata": {
        "id": "rxsT7z3lAjRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JP_qna = JP_qna[JP_qna[\"Quarter\"] != \"1Q23\"]\n",
        "UBS_qna = UBS_qna[UBS_qna[\"Quarter\"] != \"1Q23\"]"
      ],
      "metadata": {
        "id": "YWOWI6tAApUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q&A summarisation**"
      ],
      "metadata": {
        "id": "dJaMrOcRoRpO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGlJb6t_8vgr"
      },
      "source": [
        "### **Analysing Jim Mitchell data Q2-2024 data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "NUrX9T1cX6_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwiX70Vl8vBU",
        "outputId": "e2b9ee4f-4b3d-4475-a2c4-a29b9a13f2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Quarter       Analyst Analyst_Role  \\\n",
            "2  2024-Q4  Jim Mitchell      Analyst   \n",
            "\n",
            "                                            Question      Executive  \\\n",
            "2  Hey. Good morning. Maybe just on regulation, w...  Jeremy Barnum   \n",
            "\n",
            "  Executive_Role                                           Response Type  \\\n",
            "2            CFO  Hey, Jim. I mean, it's obviously something we'...  Q&A   \n",
            "\n",
            "                                      Response_clean  \\\n",
            "2  hey, jim. i mean, it's obviously something we'...   \n",
            "\n",
            "                                      Question_clean  \\\n",
            "2  hey. good morning. maybe just on regulation, w...   \n",
            "\n",
            "                                       Answer_tokens  \\\n",
            "2  [hey, jim, i, mean, its, obviously, something,...   \n",
            "\n",
            "                                     Question_tokens  \n",
            "2  [Hey, Good, morning, Maybe, just, on, regulati...  \n"
          ]
        }
      ],
      "source": [
        "filtered_df = df_qna[(df_qna[\"Analyst\"] == 'Jim Mitchell')& (df_qna[\"Quarter\"] == \"2024-Q4\")]\n",
        "\n",
        "# Display results\n",
        "print(filtered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarising questions**"
      ],
      "metadata": {
        "id": "80C-0Ka-YLaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_q = filtered_df[\"Question\"].tolist()  #### genertaing list for modeling"
      ],
      "metadata": {
        "id": "s--a-OqOYUlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating code to reflect new modeling parameters, i.e. prompt and output size, for questions summarisation"
      ],
      "metadata": {
        "id": "6RjE_RxTrcqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text_q(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 50,\n",
        "                      max_new_tokens: int = 250) -> str:\n",
        "        \"\"\"Summarize a single piece of text focusing on analyst questions.\"\"\"\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            # Specialized prompt for analyst questions\n",
        "            prompt = f\"Extract and summarize the key questions asked by analysts in the following text: {text}\"\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text_q(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 400,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization focusing on analyst questions.\"\"\"\n",
        "        try:\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text_q(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text_q(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=300\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer_q = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of analyst questions\")\n",
        "    question_summary = summarizer_q.summarize_long_text_q(analyst_q)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Analyst Questions:\")\n",
        "    print(question_summary)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfNIn_NCxY6A",
        "outputId": "d428fe19-0db1-4221-ec0d-f0d83b536af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Analyst Questions:\n",
            "Q&A: What areas of the regulatory structure would be most impactful if it were to change? Is there any area where capital requirements could actually go down? Are there any areas where requirements just simply stop going up? Are you starting to see any improvement in demand on lending?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9njaxz-P9D"
      },
      "source": [
        "**Adding ROUGE score for valuation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWP8NOv0EFDv"
      },
      "outputs": [],
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_q_str = \" \".join(analyst_q)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores_q = scorer.score(analyst_q_str, question_summary)\n",
        "for key in scores_q:\n",
        "    print(f'{key}: {scores_q[key]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwGBpr2TYl5p",
        "outputId": "3b8d4194-de81-4832-f5ca-0091d8584cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.92, recall=0.34074074074074073, fmeasure=0.49729729729729727)\n",
            "rouge2: Score(precision=0.7346938775510204, recall=0.26865671641791045, fmeasure=0.39344262295081966)\n",
            "rougeL: Score(precision=0.8, recall=0.2962962962962963, fmeasure=0.43243243243243246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE results reveal good model's ability to capture both the relevance and completeness of the content. The higher recall values, combined with solid precision, have resulted in strong F1 scores across all ROUGE metrics, demonstrating that the model is  producing comprehensive and accurate summaries."
      ],
      "metadata": {
        "id": "zqZ5ruMxvYsz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-ecGSIGC_fN"
      },
      "source": [
        "**Summarising answers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTWq_ibBJYL6"
      },
      "outputs": [],
      "source": [
        "analyst_a = filtered_df[\"Response\"].tolist()  #### genertaing list for modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "MX0t7QPSuDDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating code to reflect new modeling parameters, i.e. prompt and output size, for questions summarisation"
      ],
      "metadata": {
        "id": "g4gLfnzUrqDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 100,\n",
        "                      max_new_tokens: int = 500) -> str:\n",
        "        \"\"\"Summarize a single piece of text focusing on analyst questions.\"\"\"\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            # Specialized prompt for answer to analyst\n",
        "            prompt = f\"Summarize by extracting different statements the following text: {text}\"\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 400,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization focusing on analyst questions.\"\"\"\n",
        "        try:\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=500\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of answer\")\n",
        "    answer_summary = summarizer.summarize_long_text(analyst_a)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Answer:\")\n",
        "    print(answer_summary)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNstfsxC4rqe",
        "outputId": "34d5ab56-dff2-4a3f-fa97-fb66e8bdc062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Answer:\n",
            "Jamie Dimon: What do you think about the regulatory framework for banks? Jim: We've been saying for a long time that we want a coherent, rational, holistically-assessed regulatory framework that allows banks to do their job, supporting the economy, that isn't reflexively anti-bank, that doesn't default to the answer every question being more of everything, more capital, more liquidity, that uses data and that balances the obvious goal that we all share of a safe and sound banking system with actually recognizing that banks play a critical role in supporting growth, and the hope is that we get some of that There's a bit of caution in some areas, but we'll see what the new year brings as the current optimism starts getting tested with reality, one way or the other, and you'll actually see that come through c&i loan growth in particular.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6lTt-6SOh1z",
        "outputId": "ce834bf4-c87e-4cbc-d068-ad790c3457a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.9671052631578947, recall=0.31343283582089554, fmeasure=0.47342995169082125)\n",
            "rouge2: Score(precision=0.8278145695364238, recall=0.2670940170940171, fmeasure=0.4038772213247173)\n",
            "rougeL: Score(precision=0.9210526315789473, recall=0.29850746268656714, fmeasure=0.45088566827697263)\n"
          ]
        }
      ],
      "source": [
        "analyst_a_str = \" \".join(analyst_a)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores_a = scorer.score(analyst_a_str, answer_summary)\n",
        "for key in scores_a:\n",
        "    print(f'{key}: {scores_a[key]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model demonstrates strong precision across all ROUGE metrics, with a particularly high ROUGE-1 precision of 96.7%. However, recall values are relatively lower, indicating that while the model excels at accurately identifying relevant content, it may miss some aspects of the full text. Overall, the F1 scores suggest a moderate balance between precision and recall, with ROUGE-2 showing a solid performance in capturing key bigrams and ROUGE-L reflecting good overall summary structure."
      ],
      "metadata": {
        "id": "MmkyrvGcvywr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvL9xQreg7c4"
      },
      "source": [
        "### **Analysing John McDonald data Q2-2024 data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoFuXAvvb-o6",
        "outputId": "c8403f4d-3ee2-4587-85bb-acdf6ddd8096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Quarter        Analyst                      Analyst_Role  \\\n",
            "0  2024-Q4  John McDonald  Analyst, Truist Securities, Inc.   \n",
            "\n",
            "                                            Question      Executive  \\\n",
            "0  Hi. Good morning. Jeremy, I wanted to ask abou...  Jeremy Barnum   \n",
            "\n",
            "  Executive_Role                                           Response Type  \\\n",
            "0            CFO  Yeah. Good question, John, and welcome back, b...  Q&A   \n",
            "\n",
            "                                      Response_clean  \\\n",
            "0  yeah. good question, john, and welcome back, b...   \n",
            "\n",
            "                                      Question_clean  \\\n",
            "0  hi. good morning. jeremy, i wanted to ask abou...   \n",
            "\n",
            "                                       Answer_tokens  \\\n",
            "0  [yeah, good, question, john, and, welcome, bac...   \n",
            "\n",
            "                                     Question_tokens  \n",
            "0  [Hi, Good, morning, Jeremy, I, wanted, to, ask...  \n"
          ]
        }
      ],
      "source": [
        "filtered_df2 = df_qna[(df_qna[\"Analyst\"] == 'John McDonald')& (df_qna[\"Quarter\"] == \"2024-Q4\")]\n",
        "\n",
        "print(filtered_df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarising questions**"
      ],
      "metadata": {
        "id": "e7FNcq_LHR8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_q2 = filtered_df2[\"Question\"].tolist()  #### genertaing list for modeling"
      ],
      "metadata": {
        "id": "sF0rPZvU1DNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "qhc16VfuuGyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "\n",
        "    summarizer_q = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of analyst questions\")\n",
        "    question_summary2 = summarizer_q.summarize_long_text_q(analyst_q2)\n",
        "\n",
        "    print(\"\\nSummary of Analyst Questions:\")\n",
        "    print(question_summary2)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RvWX6QF1N2j",
        "outputId": "482062a5-5897-4ad9-8d8a-2a4611c25f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Analyst Questions:\n",
            "Key questions asked by analysts: What's the framework for thinking about the opportunity cost of sitting on the growing base of capital and how high you might let that go versus your patience in waiting for more attractive deployment opportunities? When we think about the investment spend agenda this year, how does it differ from last year or last couple of years across lines of business?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_q2_str = \" \".join(analyst_q2)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores_q2 = scorer.score(analyst_q2_str, question_summary2)\n",
        "for key in scores_a:\n",
        "    print(f'{key}: {scores_a[key]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgEiTR8e1WUq",
        "outputId": "86e032b7-54e5-4b48-a273-d7a1cd6af553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.5294117647058824, recall=0.24161073825503357, fmeasure=0.33179723502304154)\n",
            "rouge2: Score(precision=0.16417910447761194, recall=0.07432432432432433, fmeasure=0.10232558139534885)\n",
            "rougeL: Score(precision=0.35294117647058826, recall=0.1610738255033557, fmeasure=0.2211981566820276)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarinsing answsers**"
      ],
      "metadata": {
        "id": "5kCDVRno0wkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGoKXjL0KRnD"
      },
      "outputs": [],
      "source": [
        "analyst_text2 = filtered_df2[\"Response\"].tolist()  #### genertaing list for modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "IS-kzTozL0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of answer\")\n",
        "    answer_summary2 = summarizer.summarize_long_text(analyst_text2)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Answer:\")\n",
        "    print(answer_summary2)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGzDw4Rk0dyz",
        "outputId": "e5fa03a2-4fa4-4f86-e0f1-274e8fc99331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Answer:\n",
            "\"We're going to try to run things, with some important exceptions that I'll highlight in a second, on roughly flat head count and have that lead to people generating internal efficiencies as they get creative with their teams,\" he said. \"The obvious exceptions are the ongoing areas of high certainty investment and growth, so, obviously, branches and bankers, so on. and also, critical non-negotiable areas of risk and control like cyber or whatever independent risk management needs to ensure that we're running the company safely.\" He added that the company has been able to generate a little bit of efficiency over the last few years, and that's a testament to the bottoms-up culture that they've developed at the company.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02OB-4osPhmt",
        "outputId": "9985bdb1-9c60-4cbb-a914-a938e251004d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.9603174603174603, recall=0.12683438155136267, fmeasure=0.22407407407407406)\n",
            "rouge2: Score(precision=0.832, recall=0.10912906610703044, fmeasure=0.19294990723562153)\n",
            "rougeL: Score(precision=0.7142857142857143, recall=0.09433962264150944, fmeasure=0.16666666666666669)\n"
          ]
        }
      ],
      "source": [
        "analyst_text2_str = \" \".join(analyst_text2)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores2 = scorer.score(analyst_text2_str, answer_summary2)\n",
        "for key in scores2:\n",
        "    print(f'{key}: {scores2[key]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tuning parameters and changing prompt to summarise Answers text**"
      ],
      "metadata": {
        "id": "cDflN-GIBTvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "v921chQnBc0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changing prompt and updating chunks size.**"
      ],
      "metadata": {
        "id": "e431c-_CDOZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 100,\n",
        "                      max_new_tokens: int = 400) -> str:\n",
        "        \"\"\"Summarize a single piece of text.\"\"\"\n",
        "\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            prompt = f\"Rewrite the following text into a concise and original summary while maintaining its key ideas: {text}\"\n",
        "\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text_2(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 300,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization.\"\"\"\n",
        "        try:\n",
        "            # Get chunks\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            # Summarize chunks\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            # Summarize the combined summaries\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=300\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "pMNbxjpdBQGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of answer\")\n",
        "    answer_summary3 = summarizer.summarize_long_text_2(analyst_text2)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Answer:\")\n",
        "    print(answer_summary2)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "nq8f_0U2Bejp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_text3_str = \" \".join(analyst_text2)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores2 = scorer.score(analyst_text3_str, answer_summary3)\n",
        "for key in scores2:\n",
        "    print(f'{key}: {scores2[key]}')"
      ],
      "metadata": {
        "id": "C83gD2IdBie_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9vgYh7tW/ff55I6Ou/ZjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}