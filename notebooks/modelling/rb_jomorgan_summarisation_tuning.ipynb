{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/rb_jomorgan_summarisation_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FLAN-T5-Large** is tested for text summarisation using JPMorgan Financial transcripts.\n",
        "\n",
        "The model is applied to two different parts of the transcripts:\n",
        "\n",
        "**1)** individual financial analyst questions\n",
        "\n",
        "**2)** responses to these questions.\n",
        "\n",
        "**Different prompts** are applied on the two points above to allow the extraction of tailored outputs that serve different purposes:\n",
        "\n",
        "**1)** *prompt = f\"Extract and summarize the key questions asked by analysts in the following text: {text}\"*\n",
        "\n",
        "**2)** *prompt = f\"Generate a detailed and comprehensive summary that captures all key points: {text}\"*\n",
        "\n",
        "**3)** *prompt = f\"Rewrite the following text into a concise and original summary while maintaining its key ideas: {text}\"*\n",
        "\n",
        "**ROUGE scores**, which measure alignment with reference texts through precision, recall, and F-measure, are used, helping assess models performance.\n"
      ],
      "metadata": {
        "id": "L5J2VwEbPrGV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4_UYXE_38U87"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic umap-learn hdbscan sentence-transformers > /dev/null 2>&1\n",
        "!pip install transformers torch > /dev/null 2>&1\n",
        "!pip install rouge_score > /dev/null 2>&1\n",
        "!pip install evaluate > /dev/null 2>&1\n",
        "!pip install --upgrade protobuf > /dev/null 2>&1\n",
        "!pip install tensorboard > /dev/null 2>&1\n",
        "!pip install tensorflow > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siaJxgIwEUqh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List, Union, Optional\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_session():\n",
        "    tf.keras.backend.clear_session()\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "VnoqUmd7GT93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data (questions and answers for JPM and UBS)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "kCGH8oLgAg3h",
        "outputId": "7fe5a3fd-6e76-4a47-c4de-c42b524b94b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path1 = \"/content/drive/MyDrive/BOE/bank_of_england/data/preprocessed_data/jp_morgan_qna.csv\"\n",
        "\n",
        "path2 = \"/content/drive/MyDrive/BOE/bank_of_england/data/preprocessed_data/ubs_qa_df_preprocessed_ver2.csv\"\n",
        "\n",
        "JP_qna = pd.read_csv(path1)\n",
        "UBS_qna = pd.read_csv(path2)"
      ],
      "metadata": {
        "id": "rxsT7z3lAjRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JP_qna = JP_qna[JP_qna[\"Quarter\"] != \"1Q23\"]\n",
        "UBS_qna = UBS_qna[UBS_qna[\"Quarter\"] != \"1Q23\"]"
      ],
      "metadata": {
        "id": "YWOWI6tAApUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q&A summarisation**"
      ],
      "metadata": {
        "id": "dJaMrOcRoRpO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGlJb6t_8vgr"
      },
      "source": [
        "### **Analysing Jim Mitchell data Q2-2024 data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "NUrX9T1cX6_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwiX70Vl8vBU"
      },
      "outputs": [],
      "source": [
        "filtered_df = JP_qna[(JP_qna[\"Analyst\"] == 'Jim Mitchell')& (JP_qna[\"Quarter\"] == \"4Q24\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarising questions**"
      ],
      "metadata": {
        "id": "80C-0Ka-YLaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_q = filtered_df[\"Question\"].tolist()  #### genertaing list for modeling"
      ],
      "metadata": {
        "id": "s--a-OqOYUlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text_q(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 50,\n",
        "                      max_new_tokens: int = 250) -> str:\n",
        "        \"\"\"Summarize a single piece of text focusing on analyst questions.\"\"\"\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            # Specialized prompt for analyst questions\n",
        "            prompt = f\"Extract and summarize the key questions asked by analysts in the following text: {text}\"\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text_q(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 400,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization focusing on analyst questions.\"\"\"\n",
        "        try:\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text_q(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text_q(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=300\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer_q = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of analyst questions\")\n",
        "    question_summary = summarizer_q.summarize_long_text_q(analyst_q)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Analyst Questions:\")\n",
        "    print(question_summary)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfNIn_NCxY6A",
        "outputId": "a1c75021-4fd3-4996-d2f9-7d9df4565787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Analyst Questions:\n",
            "Key questions: What areas of the regulatory structure, if it were to change, would be most impactful for you? And is there any areas where you think capital requirements could actually go down? Or is this more of a story of requirements just simply stop going up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9njaxz-P9D"
      },
      "source": [
        "**Adding ROUGE score for valuation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWP8NOv0EFDv"
      },
      "outputs": [],
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_q_str = \" \".join(analyst_q)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores_q = scorer.score(analyst_q_str, question_summary)\n",
        "for key in scores_q:\n",
        "    print(f'{key}: {scores_q[key]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwGBpr2TYl5p",
        "outputId": "6d7f04f9-1b5b-47f1-d29f-03f01cccb1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.9574468085106383, recall=0.3333333333333333, fmeasure=0.4945054945054945)\n",
            "rouge2: Score(precision=0.9565217391304348, recall=0.3283582089552239, fmeasure=0.48888888888888893)\n",
            "rougeL: Score(precision=0.9574468085106383, recall=0.3333333333333333, fmeasure=0.4945054945054945)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE results reveal good model's ability to capture both the relevance and completeness of the content. The higher recall values, combined with solid precision, have resulted in strong F1 scores across all ROUGE metrics, demonstrating that the model is  producing comprehensive and accurate summaries."
      ],
      "metadata": {
        "id": "zqZ5ruMxvYsz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvL9xQreg7c4"
      },
      "source": [
        "### **Analysing John McDonald data Q2-2024 data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoFuXAvvb-o6"
      },
      "outputs": [],
      "source": [
        "filtered_df3 = JP_qna[(JP_qna[\"Analyst\"] == 'John McDonald')& (JP_qna[\"Quarter\"] == \"4Q24\")]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarinsing answsers**"
      ],
      "metadata": {
        "id": "5kCDVRno0wkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGoKXjL0KRnD"
      },
      "outputs": [],
      "source": [
        "analyst_text2 = filtered_df2[\"Response\"].tolist()  #### genertaing list for modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "IS-kzTozL0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 100,\n",
        "                      max_new_tokens: int = 500) -> str:\n",
        "        \"\"\"Summarize a single piece of text focusing on analyst questions.\"\"\"\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            # Specialized prompt for answer to analyst\n",
        "            prompt = f\"Generate a detailed and comprehensive summary that captures all key points: {text}\"\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 400,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization focusing on analyst questions.\"\"\"\n",
        "        try:\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=500\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "yk2hJJ1uXJqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of answer\")\n",
        "    answer_summary2 = summarizer.summarize_long_text(analyst_text2)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Answer:\")\n",
        "    print(answer_summary2)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGzDw4Rk0dyz",
        "outputId": "decdd1a2-4547-432b-e3d1-41c44d7846c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Answer:\n",
            "\"We would like to not have the excess grow from here,\" CEO Jeff Bezos tells CNBC in an interview with CNBC's Jim Cramer. \"We've grown a lot over the last few years, and that's a form of efficiency. But what I would say is that if you look at the head count trajectory of the company, we have grown very much and it's been a very strong growth story over the past few years. But, at the margin, that means that inside the tech teams, there's got a little bit of capacity that gets freed up to focus on features and new product development and so on, which is also in some sense a kind of efficiency.\" The obvious exceptions are the ongoing areas of high certainty investment and growth, so, obviously, branches and bankers, and also, critical non-negotiable areas of risk and control like cyber.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02OB-4osPhmt",
        "outputId": "84298dfe-b6e8-4a9e-97ae-ba74353c3996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.9078947368421053, recall=0.14465408805031446, fmeasure=0.24954792043399637)\n",
            "rouge2: Score(precision=0.7682119205298014, recall=0.12172088142707241, fmeasure=0.2101449275362319)\n",
            "rougeL: Score(precision=0.6973684210526315, recall=0.1111111111111111, fmeasure=0.19168173598553348)\n"
          ]
        }
      ],
      "source": [
        "analyst_text2_str = \" \".join(analyst_text2)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores2 = scorer.score(analyst_text2_str, answer_summary2)\n",
        "for key in scores2:\n",
        "    print(f'{key}: {scores2[key]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These ROUGE scores suggest that the generated summary captures key phrases with high precision but has low recall, meaning it includes mostly relevant words but misses many important details from the original text. The F1-scores indicate an overall weak balance, showing that the summary is quite selective and may need improvements to cover more content."
      ],
      "metadata": {
        "id": "wCnmZA8QXqnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tuning parameters and changing prompt to summarise Answers text**"
      ],
      "metadata": {
        "id": "cDflN-GIBTvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "v921chQnBc0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changing prompt and updating chunks sizes to capture better context from the text.**"
      ],
      "metadata": {
        "id": "e431c-_CDOZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 100,\n",
        "                      max_new_tokens: int = 400) -> str:\n",
        "        \"\"\"Summarize a single piece of text.\"\"\"\n",
        "\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            prompt = f\"Rewrite the following text into a concise and original summary while maintaining its key ideas: {text}\"\n",
        "\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text_2(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 300,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization.\"\"\"\n",
        "        try:\n",
        "            # Get chunks\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            # Summarize chunks\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            # Summarize the combined summaries\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=300\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "pMNbxjpdBQGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of answer\")\n",
        "    answer_summary3 = summarizer.summarize_long_text_2(analyst_text2)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Answer:\")\n",
        "    print(answer_summary3)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "nq8f_0U2Bejp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6189747-7d9e-4c49-b751-35cd77ccd788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Answer:\n",
            "I think the way we're thinking about it right now is that we feel very comfortable with the notion that it makes sense for us to have a nice store of extra capital in light of the current environment. We believe there's a good chance that we get to deploy it at better levels essentially than the current opportunities would suggest. And so that feels like a correct kind of strategic and financial decision for us. Having said that, having studied it quite extensively over the last six months and have all the debates that you would expect, we've concluded that we do have enough. We have enough excess. And given that, we would like not have the excess grow from here. So, when you think about the implications of that, given the amount of organic capital generation that we’re producing, it means that unless we find in the near term opportunities for organic deployment or otherwise, it mean more capital return through buyback, all else being equal, in order to arrest the growth of the excess. We're going to try to run things on roughly flat head count and have that lead to people generating internal efficiencies as they get creative with their teams, and we consider more efficient ways of doing things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_text3_str = \" \".join(analyst_text2)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores2 = scorer.score(analyst_text3_str, answer_summary3)\n",
        "for key in scores2:\n",
        "    print(f'{key}: {scores2[key]}')"
      ],
      "metadata": {
        "id": "C83gD2IdBie_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e8010d-c817-4725-f1b4-c2435f2cee09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=0.9948979591836735, recall=0.20440251572327045, fmeasure=0.33913043478260874)\n",
            "rouge2: Score(precision=0.958974358974359, recall=0.19622245540398742, fmeasure=0.3257839721254356)\n",
            "rougeL: Score(precision=0.5408163265306123, recall=0.1111111111111111, fmeasure=0.1843478260869565)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall results have now improved, although some of the key context is still missing, possibly due to chunking the data.\n",
        "\n",
        "ROUGE-1 and ROUGE-2 show higher precision, recall, and F1-score, meaning the summary captures more relevant words and bigrams while maintaining good accuracy.\n",
        "\n",
        "ROUGE-L recall remains the same, but ROUGE-L precision dropped, indicating some loss in capturing longer phrase structures.\n",
        "\n",
        "Overall, the improved ROUGE-1 and ROUGE-2 scores suggest the summary is now more aligned with the original text while retaining conciseness."
      ],
      "metadata": {
        "id": "5OoGV2kBX9Sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding longer chunks and re-running the model**"
      ],
      "metadata": {
        "id": "MVCAXU25a0rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "PGcMJaNFa9fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 500,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 100,\n",
        "                      max_new_tokens: int = 500) -> str:\n",
        "        \"\"\"Summarize a single piece of text.\"\"\"\n",
        "\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            prompt = f\"Rewrite the following text into a concise and original summary while maintaining its key ideas: {text}\"\n",
        "\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text_2(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 500,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization.\"\"\"\n",
        "        try:\n",
        "            # Get chunks\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            # Summarize chunks\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            # Summarize the combined summaries\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=300\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "AeeAZvkvauE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model\n",
        "try:\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "\n",
        "    logger.info(\"Starting summarization of answer\")\n",
        "    answer_summary3 = summarizer.summarize_long_text_2(analyst_text2)\n",
        "\n",
        "\n",
        "    print(\"\\nSummary of Answer:\")\n",
        "    print(answer_summary3)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during summarization: {str(e)}\")\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "EGsy4CSQa7Gc",
        "outputId": "d617861a-6a22-4d85-f757-9191d621d48a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of Answer:\n",
            "Daniel: So, yeah, you've noted all the points that we always make so I won't repeat them. And I think the way we're thinking about it right now is that we feel very comfortable with the notion that it makes sense for us to have a nice store of extra capital in light of the current environment. We believe there's a good chance that there will be a moment where we get to deploy it at better levels than the current opportunities would suggest. And so that feels like a correct kind of strategic and financial decision for us. Having said that, having studied it quite extensively over the last six months and have all the debates that you would expect, we've concluded that we do have enough. We have enough excess. And given that, we would like not have the excess grow from here. And that is our current plan, although, I'll give the caveat that, as you know, is in our disclosure, which is we don't want to get in the business of guiding on buybacks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_text3_str = \" \".join(analyst_text2)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores2 = scorer.score(analyst_text3_str, answer_summary3)\n",
        "for key in scores2:\n",
        "    print(f'{key}: {scores2[key]}')"
      ],
      "metadata": {
        "id": "hCO5ULEqbAvL",
        "outputId": "e17d069d-552c-4652-8ceb-64108d544e82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: Score(precision=1.0, recall=0.19392033542976939, fmeasure=0.32484635645302895)\n",
            "rouge2: Score(precision=0.9782608695652174, recall=0.1888772298006296, fmeasure=0.31662269129287596)\n",
            "rougeL: Score(precision=0.9945945945945946, recall=0.1928721174004193, fmeasure=0.3230904302019315)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall ROUGE results have improved after tuning chunks sizes."
      ],
      "metadata": {
        "id": "sqaS6U3wcsNy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPinrOAGdkSYJ+BSGT8EF1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}