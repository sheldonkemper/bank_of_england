{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/ob_flan_t5_sentiment_jpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym8RIzXNhpX5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Oscar Bowden\n",
        "Role: Research Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://uk.linkedin.com/in/oscar-bowden-4b14711b7\n",
        "Date: 2025-03-05\n",
        "Version: 2.4\n",
        "\n",
        "Description:\n",
        "    This notebook contains an inference pipeline for a Flan-T5 (base)\n",
        "    model that has been fine-tuned for polar sentiment analysis\n",
        "    of financial sentences (using Financial Phrasebank:\n",
        "    https://huggingface.co/datasets/takala/financial_phrasebank).\n",
        "===================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhLPtVR3bVl9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mcN0DMpkEcn0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7SH5v9iViB1"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Data handling\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI665hStbfV8"
      },
      "source": [
        "# Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uDBFS9QLw1Sr",
        "outputId": "06e892df-b05d-4220-e41d-353a4f3d4527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load topic modelled data (questions and answers for JPM and UBS)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/Topic_Modelling_am/tqc_JPMorgan_answer_topic.csv\"\n",
        "path2 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/Topic_Modelling_am/tqc_UBS_answer_topic.csv\"\n",
        "path3 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/Topic_Modelling_am/tqc_JPMorgan_question_topic.csv\"\n",
        "path4 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/Topic_Modelling_am/tqc_UBS_question_topic.csv\"\n",
        "\n",
        "\n",
        "df_a_jpm = pd.read_csv(path1)\n",
        "df_a_ubs = pd.read_csv(path2)\n",
        "df_q_jpm = pd.read_csv(path3)\n",
        "df_q_ubs = pd.read_csv(path4)"
      ],
      "metadata": {
        "id": "EcpEY7LnJQ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning - remove quotes\n",
        "\n",
        "df_a_jpm['Snippet'] = df_a_jpm['Snippet'].str.replace('\"', '', regex=False)\n",
        "df_a_ubs['Snippet'] = df_a_ubs['Snippet'].str.replace('\"', '', regex=False)\n",
        "\n",
        "df_q_jpm['Snippet'] = df_q_jpm['Snippet'].str.replace('\"', '', regex=False)\n",
        "df_q_ubs['Snippet'] = df_q_ubs['Snippet'].str.replace('\"', '', regex=False)"
      ],
      "metadata": {
        "id": "w-fPjxVO5tzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare topics for input into fine-tuned flan-t5\n",
        "\n",
        "def prepare_text_for_inference(text):\n",
        "    text = str(text).strip()\n",
        "    return f\"Classify sentiment: {text}\"\n",
        "\n",
        "df_a_jpm[\"snippet_infer\"] = df_a_jpm[\"Snippet\"].apply(prepare_text_for_inference)\n",
        "df_a_ubs[\"snippet_infer\"] = df_a_ubs[\"Snippet\"].apply(prepare_text_for_inference)\n",
        "\n",
        "df_q_jpm[\"snippet_infer\"] = df_q_jpm[\"Snippet\"].apply(prepare_text_for_inference)\n",
        "df_q_ubs[\"snippet_infer\"] = df_q_ubs[\"Snippet\"].apply(prepare_text_for_inference)"
      ],
      "metadata": {
        "id": "EdRvZDsZ8Izi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P0GD-CgpZ46"
      },
      "source": [
        "# Inference using fine-tuned Flan-T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxWQPZoKs9yZ"
      },
      "outputs": [],
      "source": [
        "# Load fine-tuned model and tokeniser from the best checkpoint\n",
        "\n",
        "best_checkpoint = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/flan_t5_sent\"\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(best_checkpoint)\n",
        "tokenizer = T5Tokenizer.from_pretrained(best_checkpoint)\n",
        "\n",
        "# Define the prediction function using your fine-tuned model\n",
        "def predict_sentiment(prepared_text):\n",
        "    \"\"\"\n",
        "    Predicts sentiment using the fine-tuned Flan-T5 model.\n",
        "    Assumes the input text is already preprocessed (i.e., prompt prepended).\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prepared_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=2)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# Apply inference on your prepared quarterly data\n",
        "df_a_jpm[\"snippet_sent\"] = df_a_jpm[\"snippet_infer\"].apply(predict_sentiment)\n",
        "df_a_ubs[\"snippet_sent\"] = df_a_ubs[\"snippet_infer\"].apply(predict_sentiment)\n",
        "\n",
        "df_q_jpm[\"snippet_sent\"] = df_q_jpm[\"snippet_infer\"].apply(predict_sentiment)\n",
        "df_q_ubs[\"snippet_sent\"] = df_q_ubs[\"snippet_infer\"].apply(predict_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_a_jpm[\"snippet_sent\"] = df_a_jpm[\"snippet_sent\"].map({\"0\": \"Negative\", \"1\": \"Neutral\", \"2\": \"Positive\"})\n",
        "df_a_ubs[\"snippet_sent\"] = df_a_ubs[\"snippet_sent\"].map({\"0\": \"Negative\", \"1\": \"Neutral\", \"2\": \"Positive\"})\n",
        "\n",
        "df_a_jpm.drop(columns=['snippet_infer'], inplace=True)\n",
        "df_a_ubs.drop(columns=['snippet_infer'], inplace=True)\n",
        "\n",
        "df_q_jpm[\"snippet_sent\"] = df_q_jpm[\"snippet_sent\"].map({\"0\": \"Negative\", \"1\": \"Neutral\", \"2\": \"Positive\"})\n",
        "df_q_ubs[\"snippet_sent\"] = df_q_ubs[\"snippet_sent\"].map({\"0\": \"Negative\", \"1\": \"Neutral\", \"2\": \"Positive\"})\n",
        "\n",
        "df_q_jpm.drop(columns=['snippet_infer'], inplace=True)\n",
        "df_q_ubs.drop(columns=['snippet_infer'], inplace=True)"
      ],
      "metadata": {
        "id": "hb59khjaNVd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"JPM Answer Snippet Sentiment:\\n {df_a_jpm['snippet_sent'].value_counts()}\")\n",
        "print(f\"\\nUBS Answer Snippet Sentiment:\\n {df_a_ubs['snippet_sent'].value_counts()}\")\n",
        "\n",
        "print(f\"\\nJPM Question Snippet Sentiment:\\n {df_q_jpm['snippet_sent'].value_counts()}\")\n",
        "print(f\"\\nUBS Question Snippet Sentiment:\\n {df_q_ubs['snippet_sent'].value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2QYhtPfNqVx",
        "outputId": "9e903b6e-5fdd-4014-e566-03d0bebf8e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JPM Answer Snippet Sentiment:\n",
            " snippet_sent\n",
            "Neutral     335\n",
            "Positive     81\n",
            "Negative     18\n",
            "Name: count, dtype: int64\n",
            "\n",
            "UBS Answer Snippet Sentiment:\n",
            " snippet_sent\n",
            "Neutral     268\n",
            "Positive     69\n",
            "Negative     20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "JPM Question Snippet Sentiment:\n",
            " snippet_sent\n",
            "Neutral     311\n",
            "Positive     64\n",
            "Negative     12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "UBS Question Snippet Sentiment:\n",
            " snippet_sent\n",
            "Neutral     370\n",
            "Positive     93\n",
            "Negative     13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save CSVs\n",
        "\n",
        "file_path_1 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/sent_output/JPM_answers_sent_output_050325_v1.csv\"\n",
        "file_path_2 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/sent_output/UBS_answers_sent_output_050325_v1.csv\"\n",
        "file_path_3 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/sent_output/JPM_questions_sent_output_050325_v1.csv\"\n",
        "file_path_4 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/sent_output/UBS_questions_sent_output_050325_v1.csv\"\n",
        "\n",
        "df_a_jpm.to_csv(file_path_1, index=False)\n",
        "\n",
        "df_a_ubs.to_csv(file_path_2, index=False)\n",
        "\n",
        "df_q_jpm.to_csv(file_path_3, index=False)\n",
        "\n",
        "df_q_ubs.to_csv(file_path_4, index=False)"
      ],
      "metadata": {
        "id": "SvS3Ie5VRgYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}