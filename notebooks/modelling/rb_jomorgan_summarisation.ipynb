{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/rb_jomorgan_summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Rita Bini\n",
        "Role: Analytics and Data Science, Bank of England Employer Project (Quant Collective)\n",
        "Linkedin: https://www.linkedin.com/in/rita-bini/\n",
        "Date: 2025-03-08\n",
        "Version: 1.0\n",
        "\n",
        "Description:\n",
        "    This notebook contains a pipeline for a Flan-T5 (large)\n",
        "    model for text summarisation of financial text. The model will be part of our LLM\n",
        "    pipeline and linked to topic and sentiment modeling results for a final insights dashboard.\n",
        "===================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "djWaecAVYokX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "p6fVafQjbq5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4_UYXE_38U87"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn hdbscan sentence-transformers > /dev/null 2>&1\n",
        "!pip install transformers torch > /dev/null 2>&1\n",
        "!pip install tensorboard > /dev/null 2>&1\n",
        "!pip install tensorflow > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siaJxgIwEUqh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "from typing import List, Union, Optional\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_session():\n",
        "    tf.keras.backend.clear_session()\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "VnoqUmd7GT93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data loading**"
      ],
      "metadata": {
        "id": "WBj7oSqbbvQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data (questions and answers for JPM and UBS)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GdQJPXAdFUa",
        "outputId": "393e8d78-d44f-469f-eb95-44450938fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = \"/content/drive/MyDrive/bank_of_england/data/preprocessed_data/jp_morgan_qna.csv\"\n",
        "path2 = \"/content/drive/MyDrive/bank_of_england/data/preprocessed_data/ubs_qa_df_preprocessed_ver2.csv\"\n",
        "\n",
        "JP_qna = pd.read_csv(path1)\n",
        "UBS_qna = pd.read_csv(path2)"
      ],
      "metadata": {
        "id": "9PsezWfHdFzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JP_qna = JP_qna[JP_qna[\"Quarter\"] != \"1Q23\"]\n",
        "UBS_qna = UBS_qna[UBS_qna[\"Quarter\"] != \"1Q23\"]"
      ],
      "metadata": {
        "id": "wz6Pq_7nMu3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model pipeline**"
      ],
      "metadata": {
        "id": "2Gz6S_5Gnnlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-large\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize the summarizer with model and device.\"\"\"\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            logger.info(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chunk_text(self,\n",
        "                  text: Union[str, List[str]],\n",
        "                  chunk_size: int = 400,\n",
        "                  overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "\n",
        "        if chunk_size <= 0 or overlap < 0 or overlap >= chunk_size:\n",
        "            raise ValueError(\"Invalid chunk_size or overlap parameters\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(text)\n",
        "\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(words):\n",
        "                end = min(start + chunk_size, len(words))\n",
        "                chunk = \" \".join(words[start:end])\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_size - overlap\n",
        "\n",
        "            logger.debug(f\"Split text into {len(chunks)} chunks\")\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chunk_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_text(self,\n",
        "                      text: str,\n",
        "                      min_new_tokens: int = 100,\n",
        "                      max_new_tokens: int = 400) -> str:\n",
        "        \"\"\"Summarize a single piece of text.\"\"\"\n",
        "\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            logger.warning(\"Empty or NaN text provided\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            prompt = f\"Rewrite the following text into a concise and original summary while maintaining its key ideas: {text}\"\n",
        "\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_text: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def summarize_long_text(self,\n",
        "                          text: Union[str, List[str]],\n",
        "                          chunk_size: int = 300,\n",
        "                          overlap: int = 50) -> str:\n",
        "        \"\"\"Handle long text summarization.\"\"\"\n",
        "        try:\n",
        "            # Get chunks\n",
        "            chunks = self.chunk_text(text, chunk_size, overlap)\n",
        "            if not chunks:\n",
        "                logger.warning(\"No valid chunks to summarize\")\n",
        "                return \"\"\n",
        "\n",
        "            # Summarize chunks\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
        "                summary = self.summarize_text(chunk)\n",
        "                if summary.strip():\n",
        "                    chunk_summaries.append(summary)\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                logger.warning(\"No valid summaries generated\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "            if len(chunk_summaries) == 1:\n",
        "                return chunk_summaries[0]\n",
        "\n",
        "            # Summarize the combined summaries\n",
        "            logger.debug(\"Generating final summary\")\n",
        "            final_summary = self.summarize_text(\n",
        "                \" \".join(chunk_summaries),\n",
        "                min_new_tokens=150,\n",
        "                max_new_tokens=300\n",
        "            )\n",
        "\n",
        "            return final_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summarize_long_text: {str(e)}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "jhne_gln-eDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying model to all quarters and to analysts level data**"
      ],
      "metadata": {
        "id": "eEdXczDUb8pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**JPMorgan**"
      ],
      "metadata": {
        "id": "8_kDZIJteMSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = TextSummarizer()\n",
        "\n",
        "summary_data = []\n",
        "\n",
        "quarter_years = JP_qna[\"Quarter\"].unique()\n",
        "analysts = JP_qna[\"Analyst\"].unique()\n",
        "\n",
        "# Loop through each unique Quarter-Year and Analyst\n",
        "for quarter in quarter_years:\n",
        "    for analyst in analysts:\n",
        "        filtered_df = JP_qna[(JP_qna[\"Quarter\"] == quarter) & (JP_qna[\"Analyst\"] == analyst)]\n",
        "\n",
        "        snippets = filtered_df[\"Response\"].dropna().astype(str).tolist()\n",
        "\n",
        "        if not snippets:\n",
        "            logger.warning(f\"No valid snippets found for Quarter-Year: {quarter}, Analyst: {analyst}\")\n",
        "            continue\n",
        "\n",
        "        print(type(summarizer))\n",
        "        # Generate the summary for the text in this Quarter-Year and Analyst\n",
        "        try:\n",
        "            summary_text = summarizer.summarize_long_text(snippets)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error summarizing for Quarter {quarter}, Analyst {analyst}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        summary_data.append({\"Summary\": summary_text, \"Quarter\": quarter, \"Analyst\": analyst})\n",
        "\n",
        "df_summaryJPM = pd.DataFrame(summary_data)"
      ],
      "metadata": {
        "id": "YLMR0ogaJHnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**UBS**"
      ],
      "metadata": {
        "id": "sPNoYCUabR52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_session()"
      ],
      "metadata": {
        "id": "VLOIWedwbRXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = TextSummarizer()\n",
        "\n",
        "summary_dataUBS = []\n",
        "\n",
        "quarter_years = UBS_qna[\"Quarter\"].unique()\n",
        "analysts = UBS_qna[\"Analyst_Bank\"].unique()\n",
        "\n",
        "# Loop through each unique Quarter-Year and Analyst\n",
        "for quarter in quarter_years:\n",
        "    for analyst in analysts:\n",
        "        filtered_df = UBS_qna[(UBS_qna[\"Quarter\"] == quarter) & (UBS_qna[\"Analyst_Bank\"] == analyst)]\n",
        "\n",
        "        snippets = filtered_df[\"Response\"].dropna().astype(str).tolist()\n",
        "\n",
        "        if not snippets:\n",
        "            logger.warning(f\"No valid snippets found for Quarter-Year: {quarter}, Analyst_Bank: {analyst}\")\n",
        "            continue\n",
        "\n",
        "        print(type(summarizer))\n",
        "        # Generate the summary for the text in this Quarter-Year and Analyst\n",
        "        try:\n",
        "            summary_text = summarizer.summarize_long_text(snippets)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error summarizing for Quarter {quarter}, Analyst_Bank {analyst}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        summary_dataUBS.append({\"Summary\": summary_text, \"Quarter\": quarter, \"Analyst_Bank\": analyst})\n",
        "\n",
        "df_summaryUBS = pd.DataFrame(summary_dataUBS)"
      ],
      "metadata": {
        "id": "SfLSdR3LaqZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Export Output Datasets**"
      ],
      "metadata": {
        "id": "bNUEhN8rqnsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save CSVs\n",
        "file_path_1 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/sent_output/summary_output_JPM.csv\"\n",
        "file_path_2 = \"/content/drive/MyDrive/bank_of_england/data/model_outputs/sent_output/summary_output_UBS.csv\"\n",
        "\n",
        "df_summaryJPM.to_csv(file_path_1, index=False)\n",
        "df_summaryUBS.to_csv(file_path_2, index=False)"
      ],
      "metadata": {
        "id": "gZRSZceVcgQM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}