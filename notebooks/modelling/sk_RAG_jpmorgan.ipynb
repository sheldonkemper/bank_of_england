{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/bank_of_england/blob/main/notebooks/modelling/sk_RAG_jpmorgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "2C9SSNHREALD",
        "outputId": "7f68c4f9-423c-4783-a3e8-3811ba76b6ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n===================================================\\nAuthor: Sheldon Kemper\\nRole: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\\nLinkedIn: https://www.linkedin.com/in/sheldon-kemper\\nDate: 2025-02-04\\nVersion: 1.1\\n\\nDescription:\\n    This notebook implements a Retrieval-Augmented Generation (RAG) system using JP Morgan\\n    earnings transcripts as the source data. It builds on our existing data engineering pipeline\\n    by reading raw PDF files stored in Google Drive, extracting text using LangChain’s PyPDFLoader,\\n    and indexing the content with CHROMA and Sentence Transformer embeddings. A text generation model\\n    (Flan-T5) is then used to answer queries based on the retrieved context, and the functionality\\n    is wrapped as a tool for a LangChain agent to handle more complex interactions.\\n\\n===================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 1.1\n",
        "\n",
        "Description:\n",
        "    This notebook implements a Retrieval-Augmented Generation (RAG) system using JP Morgan\n",
        "    earnings transcripts as the source data. It builds on our existing data engineering pipeline\n",
        "    by reading raw PDF files stored in Google Drive, extracting text using LangChain’s PyPDFLoader,\n",
        "    and indexing the content with CHROMA and Sentence Transformer embeddings. A text generation model\n",
        "    (Flan-T5) is then used to answer queries based on the retrieved context, and the functionality\n",
        "    is wrapped as a tool for a LangChain agent to handle more complex interactions.\n",
        "\n",
        "===================================================\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up and Import Libraries"
      ],
      "metadata": {
        "id": "HsOlVscFP8bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n",
        "!pip install pypdf\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uTGf-avQdHP",
        "outputId": "834cbb4f-1e78-4d56-e3fb-61bfc7393a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.37 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.37-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.19 (from langchain-community)\n",
            "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.37-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.35\n",
            "    Uninstalling langchain-core-0.3.35:\n",
            "      Successfully uninstalled langchain-core-0.3.35\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.18\n",
            "    Uninstalling langchain-0.3.18:\n",
            "      Successfully uninstalled langchain-0.3.18\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.37 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "4dnA9VlNUcLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import Chroma\n",
        "from google.colab import drive\n",
        "from langchain.agents import initialize_agent, Tool"
      ],
      "metadata": {
        "id": "POMlE--4UgTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "6m9KlYOGUjfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the raw directory (adjust if necessary)\n",
        "raw_dir = \"/content/drive/MyDrive/BOE/bank_of_england/data/raw/jpmorgan\"\n",
        "\n",
        "# List all PDF files in the raw directory\n",
        "pdf_files = [os.path.join(raw_dir, file) for file in os.listdir(raw_dir) if file.endswith(\".pdf\")]\n",
        "print(f\"Found {len(pdf_files)} PDF files in {raw_dir}\")\n"
      ],
      "metadata": {
        "id": "9pgXUITkP4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6wyySTZLUsy"
      },
      "source": [
        "## Load and Process the PDFs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list for documents\n",
        "documents = []\n",
        "\n",
        "# Loop over each PDF file and load its content, skipping the first chunk which is likely a header.\n",
        "for pdf_file in pdf_files:\n",
        "    try:\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        docs = loader.load()\n",
        "        # Option 1: Skip the first chunk if it is a header page\n",
        "        if docs and \"JPMorgan Chase\" in docs[0].page_content and \"Earnings Call Transcript\" in docs[0].page_content:\n",
        "            docs = docs[1:]\n",
        "        documents.extend(docs)\n",
        "        print(f\"Loaded {len(docs)} chunks from {os.path.basename(pdf_file)} (header skipped if detected)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_file}: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8DZP3jyZQ4Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Sentence Transformer embeddings"
      ],
      "metadata": {
        "id": "usV40njRRQP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Sentence Transformer embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "y8hqC5d4Tft6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Chroma vector store from the loaded documents"
      ],
      "metadata": {
        "id": "JuHiJJN2ToJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Chroma vector store from the loaded documents.\n",
        "vectorstore = Chroma.from_documents(documents, embeddings, collection_name=\"jpm_transcripts\")\n",
        "print(\"Chroma vector store created from the JP Morgan transcripts.\")"
      ],
      "metadata": {
        "id": "z7Beeo_3RSVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Chroma vector store from the loaded documents"
      ],
      "metadata": {
        "id": "dzI4hLrMT19Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents, embeddings, collection_name=\"jpm_transcripts\")\n",
        "print(\"Chroma vector store created from the JP Morgan transcripts.\")\n"
      ],
      "metadata": {
        "id": "aJ_qrAkDT58Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Flan-T5 model for text generation using Hugging Face's transformers pipeline"
      ],
      "metadata": {
        "id": "LXq4rMD4T8Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0)"
      ],
      "metadata": {
        "id": "nY6FGvbuT_qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a Retrieval-Augmented Generation (RAG) function"
      ],
      "metadata": {
        "id": "P8SOnB-vUB7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "    # Retrieve more document chunks for a broader context\n",
        "    docs = vectorstore.similarity_search(query, k=6)\n",
        "    # Filter out chunks that are very short (e.g., less than 20 words)\n",
        "    informative_docs = [doc for doc in docs if len(doc.page_content.split()) > 20]\n",
        "    context = \" \".join([doc.page_content for doc in informative_docs])\n",
        "\n",
        "    # Create a detailed prompt instructing the model to synthesize insights on performance\n",
        "    prompt = (\n",
        "        \"Below are excerpts from JP Morgan earnings call transcripts. \"\n",
        "        \"Please analyze these excerpts and provide a detailed summary of the key insights regarding quarterly performance. \"\n",
        "        \"Include any performance metrics, trends, improvements or declines, and significant factors mentioned. \"\n",
        "        \"If available, mention any specific challenges or notable observations about quarterly performance.\\n\\n\"\n",
        "        \"Transcript Excerpts:\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "\n",
        "    # Optionally, increase max_length to allow a longer response\n",
        "    result = qa_model(prompt, max_length=512, temperature=0.7)\n",
        "    return result[0]['generated_text']"
      ],
      "metadata": {
        "id": "Cfl9tCORUHJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the RAG function with an example query"
      ],
      "metadata": {
        "id": "LNhhY2_SUJ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the updated RAG function with your query\n",
        "test_query = \"What insights about quarterly performance are highlighted in the transcripts?\"\n",
        "print(\"RAG Answer:\", generate_answer(test_query))"
      ],
      "metadata": {
        "id": "En_LiQneUNX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap the RAG function as a tool for a LangChain agent"
      ],
      "metadata": {
        "id": "eDYvVyS0URA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Replace 'YOUR_HF_API_TOKEN' with your actual Hugging Face API token or set it as an environment variable.\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_HF_API_TOKEN\"  # <-- Replace with your token\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "# Initialize the LLM endpoint using the updated HuggingFaceEndpoint.\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=\"https://api-inference.huggingface.co/models/google/flan-t5-base\",\n",
        "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        "    model_kwargs={\"temperature\": 0.7}\n",
        ")\n",
        "\n",
        "# Define a tool that wraps our RAG function.\n",
        "def rag_tool(query: str) -> str:\n",
        "    return generate_answer(query)\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"JP Morgan RAG\",\n",
        "        func=rag_tool,\n",
        "        description=\"Answers questions using JP Morgan earnings transcripts as context.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the agent using the defined tool.\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "# Use the agent to answer a sample query.\n",
        "agent_query = \"Summarize the key performance trends mentioned in the JP Morgan earnings transcripts.\"\n",
        "agent_answer = agent.run(agent_query)\n",
        "print(\"Agent Answer:\", agent_answer)"
      ],
      "metadata": {
        "id": "egfAcBmQTBEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===================================================\n",
        "Author: Sheldon Kemper\n",
        "Role: Data Engineering Lead, Bank of England Employer Project (Quant Collective)\n",
        "LinkedIn: https://www.linkedin.com/in/sheldon-kemper\n",
        "Date: 2025-02-04\n",
        "Version: 2.0\n",
        "\n",
        "Description:\n",
        "    This notebook implements a Retrieval-Augmented Generation (RAG) system using cleaned\n",
        "    data from Bank of England projects. The data is stored in two CSV files – one containing the management\n",
        "    discussion and one containing the questions and answers. The notebook loads these CSV files from Google Drive,\n",
        "    converts each row into a Document object, and indexes them using a Chroma vector store with Sentence Transformer embeddings.\n",
        "    A text generation model (Flan-T5) is then used to answer queries based on the retrieved context.\n",
        "===================================================\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Mount Google Drive (if running in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 3: Define paths to your cleaned CSV files\n",
        "management_path = \"/content/drive/MyDrive/BOE/bank_of_england/data/processed/management_discussion.csv\"\n",
        "qa_path = \"/content/drive/MyDrive/BOE/bank_of_england/data/processed/qa_section.csv\"\n",
        "\n",
        "# Load the CSV files\n",
        "management_df = pd.read_csv(management_path)\n",
        "qa_df = pd.read_csv(qa_path)\n",
        "\n",
        "print(\"Management Discussion CSV shape:\", management_df.shape)\n",
        "print(\"Q&A CSV shape:\", qa_df.shape)\n",
        "\n",
        "# Step 4: Create Document objects from each CSV row.\n",
        "# Assumption: Each CSV has a column named \"content\" that holds the cleaned text.\n",
        "docs = []\n",
        "\n",
        "# Process management discussion data\n",
        "for index, row in management_df.iterrows():\n",
        "    content = row[\"content\"]  # adjust this if your column name is different\n",
        "    docs.append(Document(page_content=content, metadata={\"source\": \"management_discussion\"}))\n",
        "\n",
        "# Process Q&A data\n",
        "for index, row in qa_df.iterrows():\n",
        "    content = row[\"content\"]\n",
        "    docs.append(Document(page_content=content, metadata={\"source\": \"qa_section\"}))\n",
        "\n",
        "print(\"Total documents created:\", len(docs))\n",
        "\n",
        "# Step 5: Initialize Sentence Transformer embeddings and create a Chroma vector store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(docs, embeddings, collection_name=\"boe_cleaned_data\")\n",
        "print(\"Chroma vector store created from cleaned BOE data.\")\n",
        "\n",
        "# Step 6: Load the text generation model (Flan-T5) using Hugging Face's transformers pipeline.\n",
        "# Note: Ensure that your runtime is set to GPU for faster inference.\n",
        "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0)\n",
        "\n",
        "# Step 7: Define the Retrieval-Augmented Generation (RAG) function\n",
        "def generate_answer(query):\n",
        "    # Retrieve the top 6 most similar documents from the vector store\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=6)\n",
        "    # Combine the content from the retrieved documents\n",
        "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Construct a detailed prompt instructing the model to summarize key performance insights\n",
        "    prompt = (\n",
        "        \"Below are excerpts from the Bank of England management discussion and Q&A sections. \"\n",
        "        \"Please analyze these excerpts and provide a detailed summary of the key insights regarding quarterly performance. \"\n",
        "        \"Include performance metrics, trends, improvements or declines, and any notable observations.\\n\\n\"\n",
        "        \"Excerpts:\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "\n",
        "    result = qa_model(prompt, max_length=512, temperature=0.7)\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Step 8: Test the RAG function with a sample query\n",
        "test_query = \"What insights about quarterly performance are highlighted in the processed data?\"\n",
        "print(\"RAG Answer:\", generate_answer(test_query))\n"
      ],
      "metadata": {
        "id": "0hcXcmLff_zX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}